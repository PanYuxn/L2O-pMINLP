{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "be61b914-e1af-43b9-9766-5092593f92dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x12e2c209330>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch import nn\n",
    "from tqdm import tqdm\n",
    "\n",
    "# random seed\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "35fe4527-9dbf-48f0-bfb3-62cda59092aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# turn off warning\n",
    "import logging\n",
    "logging.getLogger('pyomo.core').setLevel(logging.ERROR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5560447-da9d-4cbe-aab4-538aa11c8ee8",
   "metadata": {},
   "source": [
    "## Problem Setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "52363c72-0d20-4c08-bd00-0f62aa73686c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# init\n",
    "num_data = 5000   # number of data\n",
    "num_vars = 5      # number of decision variables\n",
    "num_ints = 5      # number of integer decision variables\n",
    "test_size = 1000  # number of test size\n",
    "val_size = 1000   # number of validation size\n",
    "train_size = num_data - test_size - val_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "21938c00-d73f-4255-bc7c-bce6e8034cf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters as input data\n",
    "p_train = np.random.uniform(1, 11, (train_size, num_vars)).astype(np.float32)\n",
    "p_test = np.random.uniform(1, 11, (test_size, num_vars)).astype(np.float32)\n",
    "p_dev = np.random.uniform(1, 11, (val_size, num_vars)).astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ab7678b7-f3c9-4066-816b-a13131bcd650",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nm datasets\n",
    "from neuromancer.dataset import DictDataset\n",
    "data_train = DictDataset({\"p\":p_train}, name=\"train\")\n",
    "data_test = DictDataset({\"p\":p_test}, name=\"test\")\n",
    "data_dev = DictDataset({\"p\":p_dev}, name=\"dev\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c91eaf9b-55bd-417f-a544-84fde74c6dce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch dataloaders\n",
    "from torch.utils.data import DataLoader\n",
    "loader_train = DataLoader(data_train, batch_size=32, num_workers=0, collate_fn=data_train.collate_fn, shuffle=True)\n",
    "loader_test = DataLoader(data_test, batch_size=32, num_workers=0, collate_fn=data_test.collate_fn, shuffle=False)\n",
    "loader_dev = DataLoader(data_dev, batch_size=32, num_workers=0, collate_fn=data_dev.collate_fn, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "137f16db-28e1-4c9e-bdd3-b747f670c04d",
   "metadata": {},
   "source": [
    "## NM Problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "38c1c5a4-038b-4776-8a44-8ebb5a351e6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import neuromancer as nm\n",
    "from problem.neural import probQuadratic\n",
    "\n",
    "def getNMProb(round_module):\n",
    "    # parameters\n",
    "    p = nm.constraint.variable(\"p\")\n",
    "    # variables\n",
    "    x_bar = nm.constraint.variable(\"x_bar\")\n",
    "    x_rnd = nm.constraint.variable(\"x_rnd\")\n",
    "\n",
    "    # model\n",
    "    obj_bar, constrs_bar = probQuadratic(x_bar, p, num_vars=num_vars, alpha=100)\n",
    "    obj_rnd, constrs_rnd = probQuadratic(x_rnd, p, num_vars=num_vars, alpha=100)\n",
    "\n",
    "    # define neural architecture for the solution mapping\n",
    "    func = nm.modules.blocks.MLP(insize=num_vars, outsize=num_vars, bias=True,\n",
    "                                 linear_map=nm.slim.maps[\"linear\"], nonlin=nn.ReLU, hsizes=[80]*4)\n",
    "    # solution map from model parameters: sol_map(p) -> x\n",
    "    sol_map = nm.system.Node(func, [\"p\"], [\"x_bar\"], name=\"smap\")\n",
    "\n",
    "    # penalty loss for mapping\n",
    "    components = [sol_map]\n",
    "    loss = nm.loss.PenaltyLoss(obj_bar, constrs_bar)\n",
    "    problem = nm.problem.Problem(components, loss)\n",
    "\n",
    "    # penalty loss for rounding\n",
    "    components = [sol_map, round_module]\n",
    "    loss = nm.loss.PenaltyLoss(obj_rnd, constrs_rnd)\n",
    "    problem_rnd = nm.problem.Problem(components, loss)\n",
    "\n",
    "    return problem, problem_rnd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2b99751-3a6e-46dc-9e33-fdf68d813034",
   "metadata": {},
   "source": [
    "## Exact Solver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f2e7f07c-6a97-44d6-aca7-56f395e0c964",
   "metadata": {},
   "outputs": [],
   "source": [
    "from problem.solver import exactQuadratic\n",
    "model = exactQuadratic(n_vars=num_vars, n_integers=num_ints)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "77e326fa-de2f-415e-a4d4-1f28eef6511a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 1000/1000 [01:24<00:00, 11.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           Obj Val  Constraints Viol  Elapsed Time\n",
      "count  1000.000000       1000.000000   1000.000000\n",
      "mean    200.432000          3.835546      0.083735\n",
      "std      78.224654          5.574005      0.034054\n",
      "min      36.000000          0.000000      0.057559\n",
      "25%     149.000000          0.000000      0.061650\n",
      "50%     190.000000          0.000000      0.075394\n",
      "75%     247.000000          8.126751      0.076709\n",
      "max     511.000000         25.352531      0.228122\n"
     ]
    }
   ],
   "source": [
    "sols, objvals, conviols, elapseds = [], [], [], []\n",
    "for p in tqdm(p_test):\n",
    "    model.setParamValue(*p)\n",
    "    tick = time.time()\n",
    "    xval, objval = model.solve(\"scip\")\n",
    "    tock = time.time()\n",
    "    sols.append(list(xval.values()))\n",
    "    objvals.append(objval)\n",
    "    conviols.append(sum(model.calViolation()))\n",
    "    elapseds.append(tock - tick)\n",
    "df = pd.DataFrame({\"Sol\":sols, \"Obj Val\": objvals, \"Constraints Viol\": conviols, \"Elapsed Time\": elapseds})\n",
    "time.sleep(1)\n",
    "print(df.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9ef5a5b1-15c8-477a-9cf0-9c12b29f1d9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sol</th>\n",
       "      <th>Obj Val</th>\n",
       "      <th>Constraints Viol</th>\n",
       "      <th>Elapsed Time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[-1.0, 8.0, -1.0, 8.0, 2.0]</td>\n",
       "      <td>134.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.117057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[-3.0, 7.0, 0.0, 8.0, 1.0]</td>\n",
       "      <td>123.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.074816</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[-6.0, 8.0, 1.0, 10.0, 4.0]</td>\n",
       "      <td>217.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.076735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[-2.0, 12.0, 1.0, 9.0, 6.0]</td>\n",
       "      <td>266.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.075107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[-3.0, 7.0, -3.0, 8.0, -1.0]</td>\n",
       "      <td>132.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.075651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>[-6.0, 11.0, -2.0, 13.0, 3.0]</td>\n",
       "      <td>339.0</td>\n",
       "      <td>17.149001</td>\n",
       "      <td>0.060144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>[-1.0, 11.0, 1.0, 9.0, 4.0]</td>\n",
       "      <td>220.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.077659</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>[-1.0, 11.0, 1.0, 9.0, 4.0]</td>\n",
       "      <td>220.0</td>\n",
       "      <td>5.488008</td>\n",
       "      <td>0.074160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>[-1.0, 11.0, 1.0, 9.0, 4.0]</td>\n",
       "      <td>220.0</td>\n",
       "      <td>9.041870</td>\n",
       "      <td>0.062052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>[-1.0, 11.0, 1.0, 9.0, 4.0]</td>\n",
       "      <td>220.0</td>\n",
       "      <td>4.928075</td>\n",
       "      <td>0.181166</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                               Sol  Obj Val  Constraints Viol  Elapsed Time\n",
       "0      [-1.0, 8.0, -1.0, 8.0, 2.0]    134.0          0.000000      0.117057\n",
       "1       [-3.0, 7.0, 0.0, 8.0, 1.0]    123.0          0.000000      0.074816\n",
       "2      [-6.0, 8.0, 1.0, 10.0, 4.0]    217.0          0.000000      0.076735\n",
       "3      [-2.0, 12.0, 1.0, 9.0, 6.0]    266.0          0.000000      0.075107\n",
       "4     [-3.0, 7.0, -3.0, 8.0, -1.0]    132.0          0.000000      0.075651\n",
       "..                             ...      ...               ...           ...\n",
       "995  [-6.0, 11.0, -2.0, 13.0, 3.0]    339.0         17.149001      0.060144\n",
       "996    [-1.0, 11.0, 1.0, 9.0, 4.0]    220.0          0.000000      0.077659\n",
       "997    [-1.0, 11.0, 1.0, 9.0, 4.0]    220.0          5.488008      0.074160\n",
       "998    [-1.0, 11.0, 1.0, 9.0, 4.0]    220.0          9.041870      0.062052\n",
       "999    [-1.0, 11.0, 1.0, 9.0, 4.0]    220.0          4.928075      0.181166\n",
       "\n",
       "[1000 rows x 4 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b3e1f73-eaa5-40d9-97fb-3b70009df41b",
   "metadata": {},
   "source": [
    "## Heuristic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "987f3b11-c41c-4012-b2ef-935f62099616",
   "metadata": {},
   "outputs": [],
   "source": [
    "from heuristic import naive_round"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "161556c9-67af-4686-bfec-d0fdea488bad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# relaxed model\n",
    "model_rel = model.relax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f0a6d751-9afa-4b4c-abbb-1b2c9ba6ad08",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 1000/1000 [02:02<00:00,  8.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          Obj Val  Constraints Viol  Elapsed Time\n",
      "count  1000.00000       1000.000000   1000.000000\n",
      "mean    179.96800         28.608444      0.121861\n",
      "std      76.37923          7.851483      0.044695\n",
      "min      26.00000         10.634741      0.058464\n",
      "25%     130.00000         21.912093      0.091372\n",
      "50%     170.00000         27.912093      0.121078\n",
      "75%     222.00000         33.912093      0.135634\n",
      "max     471.00000         53.912093      0.290110\n"
     ]
    }
   ],
   "source": [
    "sols, objvals, conviols, elapseds = [], [], [], []\n",
    "for p in tqdm(p_test):\n",
    "    model_rel.setParamValue(*p)\n",
    "    tick = time.time()\n",
    "    xval_init, _ = model_rel.solve(\"scip\", max_iter=100)\n",
    "    naive_round(xval_init, model)\n",
    "    tock = time.time()\n",
    "    xval, objval = model.getVal()\n",
    "    sols.append(list(xval.values()))\n",
    "    objvals.append(objval)\n",
    "    conviols.append(sum(model.calViolation()))\n",
    "    elapseds.append(tock - tick)\n",
    "df = pd.DataFrame({\"Sol\":sols, \"Obj Val\": objvals, \"Constraints Viol\": conviols, \"Elapsed Time\": elapseds})\n",
    "time.sleep(1)\n",
    "print(df.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d961673a-7420-4260-838a-1ef2fb765ceb",
   "metadata": {},
   "source": [
    "## Learning to Round"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2ec0072d-675a-4430-ac06-07936115a28d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from model.layer import netFC\n",
    "from model.round import roundModel\n",
    "# round x\n",
    "layers_rnd = netFC(input_dim=num_vars*2, hidden_dims=[80]*4, output_dim=num_vars)\n",
    "round_func = roundModel(layers=layers_rnd, param_keys=[\"p\"], var_keys=[\"x_bar\"], output_keys=[\"x_rnd\"],\n",
    "                        int_ind={\"x_bar\":model.intInd}, name=\"round\")\n",
    "_, problem = getNMProb(round_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ea14d3a4-62f7-409b-874e-c9c58e0322a8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0  train_loss: 937.0948486328125\n",
      "epoch: 1  train_loss: 402.47930908203125\n",
      "epoch: 2  train_loss: 381.07696533203125\n",
      "epoch: 3  train_loss: 372.87158203125\n",
      "epoch: 4  train_loss: 380.60809326171875\n",
      "epoch: 5  train_loss: 380.5409240722656\n",
      "epoch: 6  train_loss: 382.86431884765625\n",
      "epoch: 7  train_loss: 377.9921875\n",
      "epoch: 8  train_loss: 372.2610168457031\n",
      "epoch: 9  train_loss: 370.8432922363281\n",
      "epoch: 10  train_loss: 370.6485595703125\n",
      "epoch: 11  train_loss: 361.46246337890625\n",
      "epoch: 12  train_loss: 358.6501770019531\n",
      "epoch: 13  train_loss: 364.6545104980469\n",
      "epoch: 14  train_loss: 356.6761474609375\n",
      "epoch: 15  train_loss: 355.5833740234375\n",
      "epoch: 16  train_loss: 351.9570007324219\n",
      "epoch: 17  train_loss: 357.2872009277344\n",
      "epoch: 18  train_loss: 349.68865966796875\n",
      "epoch: 19  train_loss: 350.5256042480469\n",
      "epoch: 20  train_loss: 351.1717529296875\n",
      "epoch: 21  train_loss: 346.0594482421875\n",
      "epoch: 22  train_loss: 348.74774169921875\n",
      "epoch: 23  train_loss: 348.5304260253906\n",
      "epoch: 24  train_loss: 346.1385803222656\n",
      "epoch: 25  train_loss: 348.56634521484375\n",
      "epoch: 26  train_loss: 345.4257507324219\n",
      "epoch: 27  train_loss: 349.6109619140625\n",
      "epoch: 28  train_loss: 345.4755554199219\n",
      "epoch: 29  train_loss: 344.5577697753906\n",
      "epoch: 30  train_loss: 346.12579345703125\n",
      "epoch: 31  train_loss: 345.2369384765625\n",
      "epoch: 32  train_loss: 343.48748779296875\n",
      "epoch: 33  train_loss: 340.8851623535156\n",
      "epoch: 34  train_loss: 346.5123596191406\n",
      "epoch: 35  train_loss: 344.0036926269531\n",
      "epoch: 36  train_loss: 343.6815185546875\n",
      "epoch: 37  train_loss: 339.1451416015625\n",
      "epoch: 38  train_loss: 342.4012756347656\n",
      "epoch: 39  train_loss: 342.23773193359375\n",
      "epoch: 40  train_loss: 349.11602783203125\n",
      "epoch: 41  train_loss: 341.599853515625\n",
      "epoch: 42  train_loss: 344.2157897949219\n",
      "epoch: 43  train_loss: 344.37347412109375\n",
      "epoch: 44  train_loss: 344.5758056640625\n",
      "epoch: 45  train_loss: 343.1227111816406\n",
      "epoch: 46  train_loss: 344.8054504394531\n",
      "epoch: 47  train_loss: 345.7437744140625\n",
      "epoch: 48  train_loss: 344.3121337890625\n",
      "epoch: 49  train_loss: 345.09783935546875\n",
      "epoch: 50  train_loss: 342.8112487792969\n",
      "epoch: 51  train_loss: 342.642578125\n",
      "epoch: 52  train_loss: 342.78570556640625\n",
      "epoch: 53  train_loss: 342.61566162109375\n",
      "epoch: 54  train_loss: 344.02227783203125\n",
      "epoch: 55  train_loss: 343.5342102050781\n",
      "epoch: 56  train_loss: 339.5896301269531\n",
      "epoch: 57  train_loss: 342.10382080078125\n",
      "epoch: 58  train_loss: 342.2771301269531\n",
      "epoch: 59  train_loss: 343.5820617675781\n",
      "epoch: 60  train_loss: 342.0441589355469\n",
      "epoch: 61  train_loss: 342.9319763183594\n",
      "epoch: 62  train_loss: 343.1004333496094\n",
      "epoch: 63  train_loss: 343.64404296875\n",
      "epoch: 64  train_loss: 342.9412536621094\n",
      "epoch: 65  train_loss: 343.3995056152344\n",
      "epoch: 66  train_loss: 345.9265441894531\n",
      "epoch: 67  train_loss: 339.08013916015625\n",
      "epoch: 68  train_loss: 342.318603515625\n",
      "epoch: 69  train_loss: 342.6793212890625\n",
      "epoch: 70  train_loss: 341.3650817871094\n",
      "epoch: 71  train_loss: 341.89410400390625\n",
      "epoch: 72  train_loss: 337.07269287109375\n",
      "epoch: 73  train_loss: 341.3712463378906\n",
      "epoch: 74  train_loss: 342.62347412109375\n",
      "epoch: 75  train_loss: 344.1130676269531\n",
      "epoch: 76  train_loss: 339.58416748046875\n",
      "epoch: 77  train_loss: 341.9734802246094\n",
      "epoch: 78  train_loss: 342.3959045410156\n",
      "epoch: 79  train_loss: 340.07769775390625\n",
      "epoch: 80  train_loss: 340.6936950683594\n",
      "epoch: 81  train_loss: 344.1036376953125\n",
      "epoch: 82  train_loss: 337.8146057128906\n",
      "epoch: 83  train_loss: 343.1639099121094\n",
      "epoch: 84  train_loss: 342.5691223144531\n",
      "epoch: 85  train_loss: 343.2979431152344\n",
      "epoch: 86  train_loss: 341.31036376953125\n",
      "epoch: 87  train_loss: 341.00244140625\n",
      "epoch: 88  train_loss: 342.48876953125\n",
      "epoch: 89  train_loss: 343.4381103515625\n",
      "epoch: 90  train_loss: 340.9928894042969\n",
      "epoch: 91  train_loss: 343.81939697265625\n",
      "epoch: 92  train_loss: 340.5504150390625\n",
      "epoch: 93  train_loss: 344.0555114746094\n",
      "epoch: 94  train_loss: 338.6162414550781\n",
      "epoch: 95  train_loss: 341.23541259765625\n",
      "epoch: 96  train_loss: 344.4356994628906\n",
      "epoch: 97  train_loss: 337.2750244140625\n",
      "epoch: 98  train_loss: 343.7645568847656\n",
      "epoch: 99  train_loss: 339.0968322753906\n",
      "epoch: 100  train_loss: 340.2704162597656\n",
      "epoch: 101  train_loss: 340.4642333984375\n",
      "Early stopping!!!\n"
     ]
    }
   ],
   "source": [
    "# training\n",
    "lr = 0.001    # step size for gradient descent\n",
    "epochs = 400  # number of training epochs\n",
    "warmup = 50   # number of epochs to wait before enacting early stopping policy\n",
    "patience = 50 # number of epochs with no improvement in eval metric to allow before early stopping\n",
    "# set adamW as optimizer\n",
    "optimizer = torch.optim.AdamW(problem.parameters(), lr=lr)\n",
    "# define trainer\n",
    "trainer = nm.trainer.Trainer(problem, loader_train, loader_dev, loader_test,\n",
    "                             optimizer, epochs=epochs, patience=patience, warmup=warmup)\n",
    "best_model = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "04b034b7-d404-4e6d-aa6f-11ae008ef1d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:04<00:00, 234.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           Obj Val  Constraints Viol  Elapsed Time\n",
      "count  1000.000000       1000.000000   1000.000000\n",
      "mean    201.908000          8.299482      0.003886\n",
      "std      68.289846          2.787089      0.000768\n",
      "min      47.000000          2.687708      0.002000\n",
      "25%     153.000000          6.265262      0.003503\n",
      "50%     194.000000          8.012799      0.003974\n",
      "75%     247.000000         10.265262      0.004235\n",
      "max     471.000000         18.542614      0.007208\n"
     ]
    }
   ],
   "source": [
    "sols, objvals, conviols, elapseds = [], [], [], []\n",
    "for p in tqdm(p_test):\n",
    "    datapoints = {\"p\": torch.tensor(np.array([p]), dtype=torch.float32), \"name\": \"test\"}\n",
    "    tick = time.time()\n",
    "    output = problem(datapoints)\n",
    "    tock = time.time()\n",
    "    x = output[\"test_x_rnd\"]\n",
    "    # get values\n",
    "    for ind in model.x:\n",
    "        model.x[ind].value = x[0, ind].item()\n",
    "    xval, objval = model.getVal()\n",
    "    sols.append(xval.values())\n",
    "    objvals.append(objval)\n",
    "    conviols.append(sum(model.calViolation()))\n",
    "    elapseds.append(tock - tick)\n",
    "df = pd.DataFrame({\"Sol\":sols, \"Obj Val\": objvals, \"Constraints Viol\": conviols, \"Elapsed Time\": elapseds})\n",
    "time.sleep(1)\n",
    "print(df.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5ae6e5c-983f-4689-88e2-4ef26fe8e910",
   "metadata": {},
   "source": [
    "## Learnable Threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "545cae68-65b7-4d0d-b87d-f021dfb0a2a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from model.layer import netFC\n",
    "from model.threshold import roundThresholdModel\n",
    "# round x\n",
    "layers_rnd = netFC(input_dim=num_vars*2, hidden_dims=[80]*4, output_dim=num_vars)\n",
    "round_func = roundThresholdModel(layers=layers_rnd, param_keys=[\"p\"], var_keys=[\"x_bar\"], output_keys=[\"x_rnd\"],\n",
    "                                 int_ind={\"x_bar\":model.intInd}, name=\"round\")\n",
    "_, problem = getNMProb(round_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "18744735-a740-489c-bd05-49988544dfac",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0  train_loss: 975.09033203125\n",
      "epoch: 1  train_loss: 392.9432067871094\n",
      "epoch: 2  train_loss: 355.19049072265625\n",
      "epoch: 3  train_loss: 352.5472106933594\n",
      "epoch: 4  train_loss: 347.5971984863281\n",
      "epoch: 5  train_loss: 347.3907165527344\n",
      "epoch: 6  train_loss: 346.7637634277344\n",
      "epoch: 7  train_loss: 340.3681945800781\n",
      "epoch: 8  train_loss: 342.6441955566406\n",
      "epoch: 9  train_loss: 342.302978515625\n",
      "epoch: 10  train_loss: 339.9844055175781\n",
      "epoch: 11  train_loss: 340.7442321777344\n",
      "epoch: 12  train_loss: 339.2669677734375\n",
      "epoch: 13  train_loss: 339.19537353515625\n",
      "epoch: 14  train_loss: 338.64849853515625\n",
      "epoch: 15  train_loss: 337.8860168457031\n",
      "epoch: 16  train_loss: 338.0227355957031\n",
      "epoch: 17  train_loss: 337.99822998046875\n",
      "epoch: 18  train_loss: 340.48162841796875\n",
      "epoch: 19  train_loss: 337.3781433105469\n",
      "epoch: 20  train_loss: 337.25927734375\n",
      "epoch: 21  train_loss: 335.99505615234375\n",
      "epoch: 22  train_loss: 337.12945556640625\n",
      "epoch: 23  train_loss: 338.3099670410156\n",
      "epoch: 24  train_loss: 336.0382385253906\n",
      "epoch: 25  train_loss: 336.6238098144531\n",
      "epoch: 26  train_loss: 337.50390625\n",
      "epoch: 27  train_loss: 335.7518005371094\n",
      "epoch: 28  train_loss: 335.37066650390625\n",
      "epoch: 29  train_loss: 334.4630432128906\n",
      "epoch: 30  train_loss: 333.77569580078125\n",
      "epoch: 31  train_loss: 335.72943115234375\n",
      "epoch: 32  train_loss: 334.8046875\n",
      "epoch: 33  train_loss: 335.7342224121094\n",
      "epoch: 34  train_loss: 335.9500732421875\n",
      "epoch: 35  train_loss: 335.4145812988281\n",
      "epoch: 36  train_loss: 335.3478698730469\n",
      "epoch: 37  train_loss: 335.395751953125\n",
      "epoch: 38  train_loss: 335.5956115722656\n",
      "epoch: 39  train_loss: 337.029052734375\n",
      "epoch: 40  train_loss: 335.19464111328125\n",
      "epoch: 41  train_loss: 334.5083923339844\n",
      "epoch: 42  train_loss: 335.8339538574219\n",
      "epoch: 43  train_loss: 334.5362854003906\n",
      "epoch: 44  train_loss: 335.12457275390625\n",
      "epoch: 45  train_loss: 335.5028381347656\n",
      "epoch: 46  train_loss: 334.52777099609375\n",
      "epoch: 47  train_loss: 334.3180236816406\n",
      "epoch: 48  train_loss: 334.3423767089844\n",
      "epoch: 49  train_loss: 334.0496520996094\n",
      "epoch: 50  train_loss: 334.9891052246094\n",
      "epoch: 51  train_loss: 336.52667236328125\n",
      "epoch: 52  train_loss: 335.6070251464844\n",
      "epoch: 53  train_loss: 334.0945739746094\n",
      "epoch: 54  train_loss: 334.95654296875\n",
      "epoch: 55  train_loss: 335.4512939453125\n",
      "epoch: 56  train_loss: 334.32000732421875\n",
      "epoch: 57  train_loss: 334.166259765625\n",
      "epoch: 58  train_loss: 334.8055114746094\n",
      "epoch: 59  train_loss: 336.072265625\n",
      "epoch: 60  train_loss: 332.4083251953125\n",
      "epoch: 61  train_loss: 333.00274658203125\n",
      "epoch: 62  train_loss: 333.48834228515625\n",
      "epoch: 63  train_loss: 333.8514709472656\n",
      "epoch: 64  train_loss: 334.4170837402344\n",
      "epoch: 65  train_loss: 334.1372375488281\n",
      "epoch: 66  train_loss: 333.1219787597656\n",
      "epoch: 67  train_loss: 334.3206787109375\n",
      "epoch: 68  train_loss: 334.1453857421875\n",
      "epoch: 69  train_loss: 333.2682800292969\n",
      "epoch: 70  train_loss: 333.1277160644531\n",
      "epoch: 71  train_loss: 333.04638671875\n",
      "epoch: 72  train_loss: 332.74188232421875\n",
      "epoch: 73  train_loss: 333.92877197265625\n",
      "epoch: 74  train_loss: 332.6134948730469\n",
      "epoch: 75  train_loss: 333.97552490234375\n",
      "epoch: 76  train_loss: 334.1870422363281\n",
      "epoch: 77  train_loss: 332.3443298339844\n",
      "epoch: 78  train_loss: 334.287841796875\n",
      "epoch: 79  train_loss: 333.4957275390625\n",
      "epoch: 80  train_loss: 332.1923828125\n",
      "epoch: 81  train_loss: 332.8411865234375\n",
      "epoch: 82  train_loss: 332.08544921875\n",
      "epoch: 83  train_loss: 333.5491638183594\n",
      "epoch: 84  train_loss: 332.00103759765625\n",
      "epoch: 85  train_loss: 332.85809326171875\n",
      "epoch: 86  train_loss: 333.11090087890625\n",
      "epoch: 87  train_loss: 333.26025390625\n",
      "epoch: 88  train_loss: 334.4580993652344\n",
      "epoch: 89  train_loss: 332.5275573730469\n",
      "epoch: 90  train_loss: 333.3076171875\n",
      "epoch: 91  train_loss: 334.5198974609375\n",
      "epoch: 92  train_loss: 332.701171875\n",
      "epoch: 93  train_loss: 332.1152038574219\n",
      "epoch: 94  train_loss: 333.7929382324219\n",
      "epoch: 95  train_loss: 332.8848876953125\n",
      "epoch: 96  train_loss: 334.4632568359375\n",
      "epoch: 97  train_loss: 335.3966979980469\n",
      "epoch: 98  train_loss: 332.7335510253906\n",
      "epoch: 99  train_loss: 332.8635559082031\n",
      "epoch: 100  train_loss: 332.5722351074219\n",
      "epoch: 101  train_loss: 335.04046630859375\n",
      "epoch: 102  train_loss: 333.1237487792969\n",
      "epoch: 103  train_loss: 332.4314270019531\n",
      "epoch: 104  train_loss: 332.6451416015625\n",
      "epoch: 105  train_loss: 331.7073669433594\n",
      "epoch: 106  train_loss: 332.7414245605469\n",
      "epoch: 107  train_loss: 331.5762023925781\n",
      "epoch: 108  train_loss: 332.14129638671875\n",
      "epoch: 109  train_loss: 331.6987609863281\n",
      "epoch: 110  train_loss: 332.37847900390625\n",
      "epoch: 111  train_loss: 333.7030334472656\n",
      "epoch: 112  train_loss: 333.0173645019531\n",
      "epoch: 113  train_loss: 333.4499816894531\n",
      "epoch: 114  train_loss: 331.5030212402344\n",
      "epoch: 115  train_loss: 332.1604309082031\n",
      "epoch: 116  train_loss: 332.9388427734375\n",
      "epoch: 117  train_loss: 332.2803649902344\n",
      "epoch: 118  train_loss: 333.03961181640625\n",
      "epoch: 119  train_loss: 332.3888854980469\n",
      "epoch: 120  train_loss: 332.6114501953125\n",
      "epoch: 121  train_loss: 333.01068115234375\n",
      "epoch: 122  train_loss: 332.9798583984375\n",
      "epoch: 123  train_loss: 332.6604309082031\n",
      "epoch: 124  train_loss: 332.9775695800781\n",
      "epoch: 125  train_loss: 333.4144287109375\n",
      "epoch: 126  train_loss: 331.6448974609375\n",
      "epoch: 127  train_loss: 332.1566467285156\n",
      "epoch: 128  train_loss: 333.0797119140625\n",
      "epoch: 129  train_loss: 332.2394104003906\n",
      "epoch: 130  train_loss: 332.7012634277344\n",
      "epoch: 131  train_loss: 332.19464111328125\n",
      "epoch: 132  train_loss: 332.4805603027344\n",
      "epoch: 133  train_loss: 333.1549987792969\n",
      "epoch: 134  train_loss: 333.4311828613281\n",
      "epoch: 135  train_loss: 332.081787109375\n",
      "epoch: 136  train_loss: 333.17535400390625\n",
      "epoch: 137  train_loss: 332.08831787109375\n",
      "epoch: 138  train_loss: 332.4025573730469\n",
      "epoch: 139  train_loss: 332.59130859375\n",
      "epoch: 140  train_loss: 332.45941162109375\n",
      "epoch: 141  train_loss: 331.1842346191406\n",
      "epoch: 142  train_loss: 333.7587585449219\n",
      "epoch: 143  train_loss: 332.45306396484375\n",
      "epoch: 144  train_loss: 333.4420471191406\n",
      "epoch: 145  train_loss: 332.23114013671875\n",
      "epoch: 146  train_loss: 332.185302734375\n",
      "epoch: 147  train_loss: 331.50897216796875\n",
      "epoch: 148  train_loss: 331.9956359863281\n",
      "epoch: 149  train_loss: 331.951416015625\n",
      "epoch: 150  train_loss: 332.2748718261719\n",
      "epoch: 151  train_loss: 331.8161926269531\n",
      "epoch: 152  train_loss: 332.89495849609375\n",
      "epoch: 153  train_loss: 333.60693359375\n",
      "epoch: 154  train_loss: 331.765869140625\n",
      "epoch: 155  train_loss: 333.3535461425781\n",
      "epoch: 156  train_loss: 331.78875732421875\n",
      "epoch: 157  train_loss: 333.0465393066406\n",
      "epoch: 158  train_loss: 332.23980712890625\n",
      "epoch: 159  train_loss: 333.0803527832031\n",
      "epoch: 160  train_loss: 331.82891845703125\n",
      "epoch: 161  train_loss: 332.6889343261719\n",
      "epoch: 162  train_loss: 332.2104797363281\n",
      "epoch: 163  train_loss: 332.1327209472656\n",
      "epoch: 164  train_loss: 331.1974792480469\n",
      "epoch: 165  train_loss: 331.8128967285156\n",
      "epoch: 166  train_loss: 332.7345886230469\n",
      "epoch: 167  train_loss: 332.79052734375\n",
      "epoch: 168  train_loss: 333.4740295410156\n",
      "epoch: 169  train_loss: 332.1455383300781\n",
      "epoch: 170  train_loss: 331.1836853027344\n",
      "epoch: 171  train_loss: 332.00640869140625\n",
      "Early stopping!!!\n"
     ]
    }
   ],
   "source": [
    "# training\n",
    "lr = 0.001    # step size for gradient descent\n",
    "epochs = 400  # number of training epochs\n",
    "warmup = 50   # number of epochs to wait before enacting early stopping policy\n",
    "patience = 50 # number of epochs with no improvement in eval metric to allow before early stopping\n",
    "# set adamW as optimizer\n",
    "optimizer = torch.optim.AdamW(problem.parameters(), lr=lr)\n",
    "# define trainer\n",
    "trainer = nm.trainer.Trainer(problem, loader_train, loader_dev, loader_test,\n",
    "                             optimizer, epochs=epochs, patience=patience, warmup=warmup)\n",
    "best_model = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "234958b5-a9bd-4a1a-b56a-edd8f2cbd6f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:04<00:00, 213.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           Obj Val  Constraints Viol  Elapsed Time\n",
      "count  1000.000000       1000.000000   1000.000000\n",
      "mean    207.073000          8.717267      0.004213\n",
      "std      73.458327          3.018748      0.001014\n",
      "min      40.000000          2.650724      0.002000\n",
      "25%     156.000000          6.302247      0.003510\n",
      "50%     198.000000          8.302247      0.004001\n",
      "75%     255.000000         10.542614      0.004998\n",
      "max     500.000000         20.109414      0.007771\n"
     ]
    }
   ],
   "source": [
    "sols, objvals, conviols, elapseds = [], [], [], []\n",
    "for p in tqdm(p_test):\n",
    "    datapoints = {\"p\": torch.tensor(np.array([p]), dtype=torch.float32), \"name\": \"test\"}\n",
    "    tick = time.time()\n",
    "    output = problem(datapoints)\n",
    "    tock = time.time()\n",
    "    x = output[\"test_x_rnd\"]\n",
    "    # get values\n",
    "    for ind in model.x:\n",
    "        model.x[ind].value = x[0, ind].item()\n",
    "    xval, objval = model.getVal()\n",
    "    sols.append(xval.values())\n",
    "    objvals.append(objval)\n",
    "    conviols.append(sum(model.calViolation()))\n",
    "    elapseds.append(tock - tick)\n",
    "df = pd.DataFrame({\"Sol\":sols, \"Obj Val\": objvals, \"Constraints Viol\": conviols, \"Elapsed Time\": elapseds})\n",
    "time.sleep(1)\n",
    "print(df.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6c777ad-d5a3-4065-a137-123aa84c0df5",
   "metadata": {},
   "source": [
    "## Learning to Round with Fixed Solution Mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "64737e09-1735-4893-a0d6-c8ca73a91fc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from model.layer import netFC\n",
    "from model.round import roundModel\n",
    "# round x\n",
    "layers_rnd = netFC(input_dim=num_vars*2, hidden_dims=[80]*4, output_dim=num_vars)\n",
    "round_func = roundModel(layers=layers_rnd, param_keys=[\"p\"], var_keys=[\"x_bar\"], output_keys=[\"x_rnd\"],\n",
    "                        int_ind={\"x_bar\":model.intInd}, name=\"round\")\n",
    "problem_rel, problem_rnd = getNMProb(round_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8b39c4a7-2b21-4384-9d85-2e479ce85965",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0  train_loss: 932.9334106445312\n",
      "epoch: 1  train_loss: 356.8347473144531\n",
      "epoch: 2  train_loss: 336.45758056640625\n",
      "epoch: 3  train_loss: 331.31048583984375\n",
      "epoch: 4  train_loss: 324.2931213378906\n",
      "epoch: 5  train_loss: 318.75872802734375\n",
      "epoch: 6  train_loss: 316.02508544921875\n",
      "epoch: 7  train_loss: 313.67919921875\n",
      "epoch: 8  train_loss: 313.3650817871094\n",
      "epoch: 9  train_loss: 312.2769775390625\n",
      "epoch: 10  train_loss: 312.3884582519531\n",
      "epoch: 11  train_loss: 311.3890686035156\n",
      "epoch: 12  train_loss: 310.96063232421875\n",
      "epoch: 13  train_loss: 312.8399353027344\n",
      "epoch: 14  train_loss: 309.7454528808594\n",
      "epoch: 15  train_loss: 312.87237548828125\n",
      "epoch: 16  train_loss: 309.1521301269531\n",
      "epoch: 17  train_loss: 309.9900817871094\n",
      "epoch: 18  train_loss: 310.71148681640625\n",
      "epoch: 19  train_loss: 306.4747619628906\n",
      "epoch: 20  train_loss: 307.1275634765625\n",
      "epoch: 21  train_loss: 307.9350891113281\n",
      "epoch: 22  train_loss: 306.3865661621094\n",
      "epoch: 23  train_loss: 307.3204345703125\n",
      "epoch: 24  train_loss: 306.8388366699219\n",
      "epoch: 25  train_loss: 306.1849060058594\n",
      "epoch: 26  train_loss: 306.3838195800781\n",
      "epoch: 27  train_loss: 304.94610595703125\n",
      "epoch: 28  train_loss: 307.21844482421875\n",
      "epoch: 29  train_loss: 305.7421569824219\n",
      "epoch: 30  train_loss: 305.9813232421875\n",
      "epoch: 31  train_loss: 303.99774169921875\n",
      "epoch: 32  train_loss: 303.9259338378906\n",
      "epoch: 33  train_loss: 304.2318115234375\n",
      "epoch: 34  train_loss: 302.8601989746094\n",
      "epoch: 35  train_loss: 303.8232727050781\n",
      "epoch: 36  train_loss: 303.9900817871094\n",
      "epoch: 37  train_loss: 304.0069885253906\n",
      "epoch: 38  train_loss: 303.51641845703125\n",
      "epoch: 39  train_loss: 302.1554870605469\n",
      "epoch: 40  train_loss: 304.0890808105469\n",
      "epoch: 41  train_loss: 302.10302734375\n",
      "epoch: 42  train_loss: 303.93896484375\n",
      "epoch: 43  train_loss: 302.732421875\n",
      "epoch: 44  train_loss: 302.2016296386719\n",
      "epoch: 45  train_loss: 301.51434326171875\n",
      "epoch: 46  train_loss: 301.8230895996094\n",
      "epoch: 47  train_loss: 300.3242492675781\n",
      "epoch: 48  train_loss: 301.9022521972656\n",
      "epoch: 49  train_loss: 300.3905334472656\n",
      "epoch: 50  train_loss: 300.6494140625\n",
      "epoch: 51  train_loss: 299.95855712890625\n",
      "epoch: 52  train_loss: 301.4244079589844\n",
      "epoch: 53  train_loss: 300.6818542480469\n",
      "epoch: 54  train_loss: 299.61083984375\n",
      "epoch: 55  train_loss: 300.70196533203125\n",
      "epoch: 56  train_loss: 301.37451171875\n",
      "epoch: 57  train_loss: 300.5324401855469\n",
      "epoch: 58  train_loss: 301.1631774902344\n",
      "epoch: 59  train_loss: 300.5215759277344\n",
      "epoch: 60  train_loss: 299.3537292480469\n",
      "epoch: 61  train_loss: 300.66650390625\n",
      "epoch: 62  train_loss: 299.9519348144531\n",
      "epoch: 63  train_loss: 299.8477478027344\n",
      "epoch: 64  train_loss: 298.37939453125\n",
      "epoch: 65  train_loss: 299.37310791015625\n",
      "epoch: 66  train_loss: 301.4763488769531\n",
      "epoch: 67  train_loss: 299.8488464355469\n",
      "epoch: 68  train_loss: 299.48699951171875\n",
      "epoch: 69  train_loss: 300.7736511230469\n",
      "epoch: 70  train_loss: 300.50518798828125\n",
      "epoch: 71  train_loss: 300.04248046875\n",
      "epoch: 72  train_loss: 299.1159973144531\n",
      "epoch: 73  train_loss: 300.24407958984375\n",
      "epoch: 74  train_loss: 298.8691101074219\n",
      "epoch: 75  train_loss: 300.50830078125\n",
      "epoch: 76  train_loss: 301.0060119628906\n",
      "epoch: 77  train_loss: 299.89971923828125\n",
      "epoch: 78  train_loss: 299.10113525390625\n",
      "epoch: 79  train_loss: 299.0198669433594\n",
      "epoch: 80  train_loss: 299.6593933105469\n",
      "epoch: 81  train_loss: 299.2630615234375\n",
      "epoch: 82  train_loss: 299.8840026855469\n",
      "epoch: 83  train_loss: 299.5733947753906\n",
      "epoch: 84  train_loss: 299.61065673828125\n",
      "epoch: 85  train_loss: 299.2296447753906\n",
      "epoch: 86  train_loss: 299.0251770019531\n",
      "epoch: 87  train_loss: 299.2239074707031\n",
      "epoch: 88  train_loss: 297.30426025390625\n",
      "epoch: 89  train_loss: 298.5527648925781\n",
      "epoch: 90  train_loss: 298.5356140136719\n",
      "epoch: 91  train_loss: 299.39105224609375\n",
      "epoch: 92  train_loss: 298.94744873046875\n",
      "epoch: 93  train_loss: 298.0205383300781\n",
      "epoch: 94  train_loss: 300.1726379394531\n",
      "epoch: 95  train_loss: 298.6230773925781\n",
      "epoch: 96  train_loss: 299.8541259765625\n",
      "epoch: 97  train_loss: 297.6995849609375\n",
      "epoch: 98  train_loss: 299.09552001953125\n",
      "epoch: 99  train_loss: 298.3890380859375\n",
      "epoch: 100  train_loss: 299.0435485839844\n",
      "epoch: 101  train_loss: 297.7906799316406\n",
      "Early stopping!!!\n"
     ]
    }
   ],
   "source": [
    "# training for mapping\n",
    "lr = 0.001    # step size for gradient descent\n",
    "epochs = 400  # number of training epochs\n",
    "warmup = 50   # number of epochs to wait before enacting early stopping policy\n",
    "patience = 50 # number of epochs with no improvement in eval metric to allow before early stopping\n",
    "# set adamW as optimizer\n",
    "optimizer = torch.optim.AdamW(problem_rel.parameters(), lr=lr)\n",
    "# define trainer\n",
    "trainer = nm.trainer.Trainer(problem_rel, loader_train, loader_dev, loader_test,\n",
    "                             optimizer, epochs=epochs, patience=patience, warmup=warmup)\n",
    "best_model = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a1efad3d-dd09-4aef-81dd-3ed3c31ffb31",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0  train_loss: 395.63311767578125\n",
      "epoch: 1  train_loss: 395.4594421386719\n",
      "epoch: 2  train_loss: 395.5500793457031\n",
      "epoch: 3  train_loss: 394.0802307128906\n",
      "epoch: 4  train_loss: 395.0129699707031\n",
      "epoch: 5  train_loss: 396.0818176269531\n",
      "epoch: 6  train_loss: 395.1732482910156\n",
      "epoch: 7  train_loss: 395.8547668457031\n",
      "epoch: 8  train_loss: 395.42840576171875\n",
      "epoch: 9  train_loss: 395.6715087890625\n",
      "epoch: 10  train_loss: 395.98187255859375\n",
      "epoch: 11  train_loss: 395.4700927734375\n",
      "epoch: 12  train_loss: 395.11761474609375\n",
      "epoch: 13  train_loss: 397.1585388183594\n",
      "epoch: 14  train_loss: 396.5810852050781\n",
      "epoch: 15  train_loss: 394.6093444824219\n",
      "epoch: 16  train_loss: 394.5400695800781\n",
      "epoch: 17  train_loss: 394.8847961425781\n",
      "epoch: 18  train_loss: 395.0920104980469\n",
      "epoch: 19  train_loss: 396.547119140625\n",
      "epoch: 20  train_loss: 395.2701110839844\n",
      "epoch: 21  train_loss: 395.3181457519531\n",
      "epoch: 22  train_loss: 396.2032165527344\n",
      "epoch: 23  train_loss: 397.4877014160156\n",
      "epoch: 24  train_loss: 397.0378112792969\n",
      "epoch: 25  train_loss: 396.4247741699219\n",
      "epoch: 26  train_loss: 396.0035705566406\n",
      "epoch: 27  train_loss: 395.73016357421875\n",
      "epoch: 28  train_loss: 396.3167419433594\n",
      "epoch: 29  train_loss: 395.5481262207031\n",
      "epoch: 30  train_loss: 396.46258544921875\n",
      "epoch: 31  train_loss: 395.6125183105469\n",
      "epoch: 32  train_loss: 396.1362609863281\n",
      "epoch: 33  train_loss: 395.77508544921875\n",
      "epoch: 34  train_loss: 393.94696044921875\n",
      "epoch: 35  train_loss: 394.72955322265625\n",
      "epoch: 36  train_loss: 395.5747985839844\n",
      "epoch: 37  train_loss: 396.3241882324219\n",
      "epoch: 38  train_loss: 394.1087951660156\n",
      "epoch: 39  train_loss: 394.8992004394531\n",
      "epoch: 40  train_loss: 396.1855773925781\n",
      "epoch: 41  train_loss: 397.0758361816406\n",
      "epoch: 42  train_loss: 397.5042419433594\n",
      "epoch: 43  train_loss: 394.5845642089844\n",
      "epoch: 44  train_loss: 397.4742431640625\n",
      "epoch: 45  train_loss: 395.59490966796875\n",
      "epoch: 46  train_loss: 396.41656494140625\n",
      "epoch: 47  train_loss: 394.57135009765625\n",
      "epoch: 48  train_loss: 396.2687683105469\n",
      "epoch: 49  train_loss: 395.48675537109375\n",
      "epoch: 50  train_loss: 396.7042541503906\n",
      "epoch: 51  train_loss: 395.9444885253906\n",
      "epoch: 52  train_loss: 394.1003723144531\n",
      "epoch: 53  train_loss: 394.9784851074219\n",
      "epoch: 54  train_loss: 395.7887268066406\n",
      "epoch: 55  train_loss: 395.2519226074219\n",
      "epoch: 56  train_loss: 395.2813720703125\n",
      "epoch: 57  train_loss: 394.87200927734375\n",
      "epoch: 58  train_loss: 395.60791015625\n",
      "epoch: 59  train_loss: 395.2654113769531\n",
      "epoch: 60  train_loss: 395.5648193359375\n",
      "epoch: 61  train_loss: 394.7696838378906\n",
      "epoch: 62  train_loss: 396.603271484375\n",
      "epoch: 63  train_loss: 395.77606201171875\n",
      "epoch: 64  train_loss: 396.9659729003906\n",
      "epoch: 65  train_loss: 397.1878662109375\n",
      "epoch: 66  train_loss: 396.42864990234375\n",
      "epoch: 67  train_loss: 395.8580017089844\n",
      "epoch: 68  train_loss: 395.3812255859375\n",
      "epoch: 69  train_loss: 396.36248779296875\n",
      "epoch: 70  train_loss: 396.0224914550781\n",
      "epoch: 71  train_loss: 395.1571960449219\n",
      "epoch: 72  train_loss: 397.123046875\n",
      "epoch: 73  train_loss: 395.7800598144531\n",
      "epoch: 74  train_loss: 395.8747863769531\n",
      "epoch: 75  train_loss: 396.4942321777344\n",
      "epoch: 76  train_loss: 396.4701232910156\n",
      "epoch: 77  train_loss: 396.57940673828125\n",
      "epoch: 78  train_loss: 395.0525817871094\n",
      "epoch: 79  train_loss: 395.47491455078125\n",
      "epoch: 80  train_loss: 394.74908447265625\n",
      "epoch: 81  train_loss: 396.23358154296875\n",
      "epoch: 82  train_loss: 397.52593994140625\n",
      "epoch: 83  train_loss: 396.6314697265625\n",
      "epoch: 84  train_loss: 395.5997314453125\n",
      "epoch: 85  train_loss: 394.4164733886719\n",
      "epoch: 86  train_loss: 397.017333984375\n",
      "epoch: 87  train_loss: 395.6921081542969\n",
      "epoch: 88  train_loss: 394.3535461425781\n",
      "epoch: 89  train_loss: 396.1600036621094\n",
      "epoch: 90  train_loss: 395.01519775390625\n",
      "epoch: 91  train_loss: 396.3479919433594\n",
      "epoch: 92  train_loss: 395.6261291503906\n",
      "epoch: 93  train_loss: 393.72149658203125\n",
      "epoch: 94  train_loss: 395.1937255859375\n",
      "epoch: 95  train_loss: 395.53192138671875\n",
      "epoch: 96  train_loss: 396.2447509765625\n",
      "epoch: 97  train_loss: 395.8615417480469\n",
      "epoch: 98  train_loss: 396.9178466796875\n",
      "epoch: 99  train_loss: 397.8160400390625\n",
      "epoch: 100  train_loss: 394.5138854980469\n",
      "epoch: 101  train_loss: 395.6388244628906\n",
      "epoch: 102  train_loss: 396.6272888183594\n",
      "epoch: 103  train_loss: 396.7292175292969\n",
      "epoch: 104  train_loss: 395.09893798828125\n",
      "epoch: 105  train_loss: 394.6555480957031\n",
      "epoch: 106  train_loss: 396.09344482421875\n",
      "epoch: 107  train_loss: 393.8418884277344\n",
      "epoch: 108  train_loss: 396.5713806152344\n",
      "epoch: 109  train_loss: 394.79443359375\n",
      "epoch: 110  train_loss: 393.37249755859375\n",
      "epoch: 111  train_loss: 395.5074462890625\n",
      "epoch: 112  train_loss: 394.441162109375\n",
      "epoch: 113  train_loss: 396.3274230957031\n",
      "epoch: 114  train_loss: 396.6763610839844\n",
      "epoch: 115  train_loss: 395.6793518066406\n",
      "epoch: 116  train_loss: 396.1435241699219\n",
      "epoch: 117  train_loss: 395.4418640136719\n",
      "epoch: 118  train_loss: 396.04144287109375\n",
      "epoch: 119  train_loss: 395.76904296875\n",
      "epoch: 120  train_loss: 397.50262451171875\n",
      "epoch: 121  train_loss: 395.9267272949219\n",
      "epoch: 122  train_loss: 395.5883483886719\n",
      "epoch: 123  train_loss: 395.1786804199219\n",
      "epoch: 124  train_loss: 396.4266662597656\n",
      "epoch: 125  train_loss: 395.1224670410156\n",
      "epoch: 126  train_loss: 396.9217834472656\n",
      "epoch: 127  train_loss: 394.71319580078125\n",
      "epoch: 128  train_loss: 394.9990539550781\n",
      "epoch: 129  train_loss: 396.4139099121094\n",
      "epoch: 130  train_loss: 397.171875\n",
      "epoch: 131  train_loss: 396.70989990234375\n",
      "epoch: 132  train_loss: 395.35595703125\n",
      "epoch: 133  train_loss: 394.8621520996094\n",
      "epoch: 134  train_loss: 395.02618408203125\n",
      "epoch: 135  train_loss: 394.30206298828125\n",
      "epoch: 136  train_loss: 396.0282287597656\n",
      "epoch: 137  train_loss: 396.6228942871094\n",
      "epoch: 138  train_loss: 394.91485595703125\n",
      "epoch: 139  train_loss: 395.2638244628906\n",
      "epoch: 140  train_loss: 395.00885009765625\n",
      "epoch: 141  train_loss: 396.9116516113281\n",
      "epoch: 142  train_loss: 395.0617370605469\n",
      "epoch: 143  train_loss: 394.41644287109375\n",
      "epoch: 144  train_loss: 396.43841552734375\n",
      "epoch: 145  train_loss: 396.4962463378906\n",
      "epoch: 146  train_loss: 394.4974670410156\n",
      "epoch: 147  train_loss: 395.58636474609375\n",
      "epoch: 148  train_loss: 395.4612731933594\n",
      "epoch: 149  train_loss: 394.9853820800781\n",
      "epoch: 150  train_loss: 396.9251708984375\n",
      "epoch: 151  train_loss: 395.0850524902344\n",
      "epoch: 152  train_loss: 394.5914611816406\n",
      "epoch: 153  train_loss: 397.2919006347656\n",
      "epoch: 154  train_loss: 396.1033020019531\n",
      "epoch: 155  train_loss: 394.8102111816406\n",
      "epoch: 156  train_loss: 394.95257568359375\n",
      "epoch: 157  train_loss: 397.337646484375\n",
      "epoch: 158  train_loss: 396.66278076171875\n",
      "epoch: 159  train_loss: 396.2958679199219\n",
      "epoch: 160  train_loss: 395.12786865234375\n",
      "epoch: 161  train_loss: 394.8097839355469\n",
      "epoch: 162  train_loss: 392.63629150390625\n",
      "epoch: 163  train_loss: 396.23101806640625\n",
      "epoch: 164  train_loss: 393.8737487792969\n",
      "epoch: 165  train_loss: 395.9606018066406\n",
      "epoch: 166  train_loss: 396.80596923828125\n",
      "epoch: 167  train_loss: 397.09765625\n",
      "epoch: 168  train_loss: 397.0871276855469\n",
      "epoch: 169  train_loss: 395.79913330078125\n",
      "epoch: 170  train_loss: 393.3597106933594\n",
      "epoch: 171  train_loss: 394.8829040527344\n",
      "epoch: 172  train_loss: 396.36663818359375\n",
      "epoch: 173  train_loss: 396.5857849121094\n",
      "epoch: 174  train_loss: 397.0946960449219\n",
      "epoch: 175  train_loss: 396.0230712890625\n",
      "epoch: 176  train_loss: 396.7877197265625\n",
      "epoch: 177  train_loss: 395.0599365234375\n",
      "epoch: 178  train_loss: 396.39385986328125\n",
      "epoch: 179  train_loss: 395.35296630859375\n",
      "epoch: 180  train_loss: 395.3674011230469\n",
      "epoch: 181  train_loss: 395.88140869140625\n",
      "Early stopping!!!\n"
     ]
    }
   ],
   "source": [
    "# freeze sol mapping\n",
    "#problem_rel.freeze()\n",
    "# training for rounding\n",
    "lr = 0.001    # step size for gradient descent\n",
    "epochs = 400  # number of training epochs\n",
    "warmup = 50   # number of epochs to wait before enacting early stopping policy\n",
    "patience = 50 # number of epochs with no improvement in eval metric to allow before early stopping\n",
    "# set adamW as optimizer\n",
    "optimizer = torch.optim.AdamW(problem.parameters(), lr=lr)\n",
    "# define trainer\n",
    "trainer = nm.trainer.Trainer(problem_rnd, loader_train, loader_dev, loader_test,\n",
    "                             optimizer, epochs=epochs, patience=patience, warmup=warmup)\n",
    "best_model = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7858c207-11d3-4cfc-8e7a-13d831ee7c1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:04<00:00, 229.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           Obj Val  Constraints Viol  Elapsed Time\n",
      "count  1000.000000       1000.000000   1000.000000\n",
      "mean    189.095000          9.041808      0.003920\n",
      "std      72.065911          2.930551      0.000862\n",
      "min      27.000000          2.735447      0.001999\n",
      "25%     136.000000          6.887273      0.003317\n",
      "50%     183.500000          8.585647      0.003999\n",
      "75%     234.000000         10.841309      0.004416\n",
      "max     443.000000         18.579599      0.008525\n"
     ]
    }
   ],
   "source": [
    "sols, objvals, conviols, elapseds = [], [], [], []\n",
    "for p in tqdm(p_test):\n",
    "    datapoints = {\"p\": torch.tensor(np.array([p]), dtype=torch.float32), \"name\": \"test\"}\n",
    "    tick = time.time()\n",
    "    output = problem_rnd(datapoints)\n",
    "    tock = time.time()\n",
    "    x = output[\"test_x_rnd\"]\n",
    "    # get values\n",
    "    for ind in model.x:\n",
    "        model.x[ind].value = x[0, ind].item()\n",
    "    xval, objval = model.getVal()\n",
    "    sols.append(xval.values())\n",
    "    objvals.append(objval)\n",
    "    conviols.append(sum(model.calViolation()))\n",
    "    elapseds.append(tock - tick)\n",
    "df = pd.DataFrame({\"Sol\":sols, \"Obj Val\": objvals, \"Constraints Viol\": conviols, \"Elapsed Time\": elapseds})\n",
    "time.sleep(1)\n",
    "print(df.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f140ecf-274f-4dcb-b26e-e76c46ca53e7",
   "metadata": {},
   "source": [
    "### Learning to Feasibility Pump"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ddfd3db5-b61c-46dc-92ae-138ce002fc11",
   "metadata": {},
   "outputs": [],
   "source": [
    "from model.layer import netFC\n",
    "# define neural architecture for the solution mapping\n",
    "sol_func = nm.modules.blocks.MLP(insize=num_vars, outsize=num_vars, bias=True,\n",
    "                                 linear_map=nm.slim.maps[\"linear\"], nonlin=nn.ReLU,\n",
    "                                 hsizes=[80]*4)\n",
    "# define neural architecture for rounding\n",
    "rnd_layer = netFC(input_dim=num_vars*2, hidden_dims=[80]*4, output_dim=num_vars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e9397410-cf75-46de-83b9-d975605b2022",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to get nm optimization model\n",
    "from problem.neural import probQuadratic\n",
    "def getProb(x, p):\n",
    "    return probQuadratic(x, p, num_vars=num_vars, alpha=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2903e087-27af-4738-b1f5-3c30d717ba17",
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import feasibilityPumpModel\n",
    "# feasibility pump model\n",
    "problem_rel, problem_fp = feasibilityPumpModel([\"p\"], getProb, sol_func, rnd_layer, int_ind=model.intInd, num_iters=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "57c24ef8-218a-40f0-a448-f0b38851c3f9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0  train_loss: 1055.1697998046875\n",
      "epoch: 1  train_loss: 375.1524963378906\n",
      "epoch: 2  train_loss: 328.26104736328125\n",
      "epoch: 3  train_loss: 321.15948486328125\n",
      "epoch: 4  train_loss: 317.2909240722656\n",
      "epoch: 5  train_loss: 315.58453369140625\n",
      "epoch: 6  train_loss: 314.23565673828125\n",
      "epoch: 7  train_loss: 312.3757019042969\n",
      "epoch: 8  train_loss: 311.7950744628906\n",
      "epoch: 9  train_loss: 311.66888427734375\n",
      "epoch: 10  train_loss: 310.39910888671875\n",
      "epoch: 11  train_loss: 310.2560729980469\n",
      "epoch: 12  train_loss: 310.1974182128906\n",
      "epoch: 13  train_loss: 309.4531555175781\n",
      "epoch: 14  train_loss: 309.32568359375\n",
      "epoch: 15  train_loss: 308.2574462890625\n",
      "epoch: 16  train_loss: 309.0048522949219\n",
      "epoch: 17  train_loss: 308.4734802246094\n",
      "epoch: 18  train_loss: 308.40618896484375\n",
      "epoch: 19  train_loss: 307.931640625\n",
      "epoch: 20  train_loss: 306.3957214355469\n",
      "epoch: 21  train_loss: 305.8752136230469\n",
      "epoch: 22  train_loss: 305.4753723144531\n",
      "epoch: 23  train_loss: 306.5816955566406\n",
      "epoch: 24  train_loss: 306.013916015625\n",
      "epoch: 25  train_loss: 306.8361511230469\n",
      "epoch: 26  train_loss: 307.9098205566406\n",
      "epoch: 27  train_loss: 305.62420654296875\n",
      "epoch: 28  train_loss: 306.21270751953125\n",
      "epoch: 29  train_loss: 306.2701416015625\n",
      "epoch: 30  train_loss: 304.10382080078125\n",
      "epoch: 31  train_loss: 302.57843017578125\n",
      "epoch: 32  train_loss: 302.5232238769531\n",
      "epoch: 33  train_loss: 303.6941223144531\n",
      "epoch: 34  train_loss: 302.6715393066406\n",
      "epoch: 35  train_loss: 301.9190368652344\n",
      "epoch: 36  train_loss: 301.3351745605469\n",
      "epoch: 37  train_loss: 302.275146484375\n",
      "epoch: 38  train_loss: 301.7905578613281\n",
      "epoch: 39  train_loss: 303.0672912597656\n",
      "epoch: 40  train_loss: 300.7250671386719\n",
      "epoch: 41  train_loss: 301.4158020019531\n",
      "epoch: 42  train_loss: 300.92132568359375\n",
      "epoch: 43  train_loss: 300.0269775390625\n",
      "epoch: 44  train_loss: 302.6532287597656\n",
      "epoch: 45  train_loss: 300.40460205078125\n",
      "epoch: 46  train_loss: 299.76983642578125\n",
      "epoch: 47  train_loss: 301.6759338378906\n",
      "epoch: 48  train_loss: 300.35113525390625\n",
      "epoch: 49  train_loss: 301.2142333984375\n",
      "epoch: 50  train_loss: 300.17498779296875\n",
      "epoch: 51  train_loss: 301.4576721191406\n",
      "epoch: 52  train_loss: 301.8084411621094\n",
      "epoch: 53  train_loss: 300.0635681152344\n",
      "epoch: 54  train_loss: 298.2673645019531\n",
      "epoch: 55  train_loss: 299.5665283203125\n",
      "epoch: 56  train_loss: 300.8074035644531\n",
      "epoch: 57  train_loss: 299.81689453125\n",
      "epoch: 58  train_loss: 300.0939636230469\n",
      "epoch: 59  train_loss: 300.66534423828125\n",
      "epoch: 60  train_loss: 299.9278259277344\n",
      "epoch: 61  train_loss: 299.9797058105469\n",
      "epoch: 62  train_loss: 299.8789367675781\n",
      "epoch: 63  train_loss: 299.43206787109375\n",
      "epoch: 64  train_loss: 299.18865966796875\n",
      "epoch: 65  train_loss: 301.6394348144531\n",
      "epoch: 66  train_loss: 300.3534851074219\n",
      "epoch: 67  train_loss: 299.6451110839844\n",
      "epoch: 68  train_loss: 299.6800537109375\n",
      "epoch: 69  train_loss: 301.08380126953125\n",
      "epoch: 70  train_loss: 299.62225341796875\n",
      "epoch: 71  train_loss: 300.6560974121094\n",
      "epoch: 72  train_loss: 299.38763427734375\n",
      "epoch: 73  train_loss: 299.8450622558594\n",
      "epoch: 74  train_loss: 299.49700927734375\n",
      "epoch: 75  train_loss: 300.1517333984375\n",
      "epoch: 76  train_loss: 298.5995788574219\n",
      "epoch: 77  train_loss: 299.2025146484375\n",
      "epoch: 78  train_loss: 298.5672302246094\n",
      "epoch: 79  train_loss: 301.36322021484375\n",
      "epoch: 80  train_loss: 299.0489501953125\n",
      "epoch: 81  train_loss: 298.59405517578125\n",
      "epoch: 82  train_loss: 298.86181640625\n",
      "epoch: 83  train_loss: 299.0169677734375\n",
      "epoch: 84  train_loss: 298.93402099609375\n",
      "epoch: 85  train_loss: 299.6287841796875\n",
      "epoch: 86  train_loss: 299.23834228515625\n",
      "epoch: 87  train_loss: 298.5078125\n",
      "epoch: 88  train_loss: 299.38055419921875\n",
      "epoch: 89  train_loss: 298.2830810546875\n",
      "epoch: 90  train_loss: 297.36376953125\n",
      "epoch: 91  train_loss: 297.24285888671875\n",
      "epoch: 92  train_loss: 300.3037414550781\n",
      "epoch: 93  train_loss: 298.5455017089844\n",
      "epoch: 94  train_loss: 299.61077880859375\n",
      "epoch: 95  train_loss: 299.53546142578125\n",
      "epoch: 96  train_loss: 298.068115234375\n",
      "epoch: 97  train_loss: 299.6092529296875\n",
      "epoch: 98  train_loss: 297.6314697265625\n",
      "epoch: 99  train_loss: 300.6896057128906\n",
      "epoch: 100  train_loss: 298.9239501953125\n",
      "epoch: 101  train_loss: 299.3637390136719\n",
      "epoch: 102  train_loss: 298.16619873046875\n",
      "epoch: 103  train_loss: 298.02886962890625\n",
      "epoch: 104  train_loss: 298.77984619140625\n",
      "epoch: 105  train_loss: 299.2132873535156\n",
      "epoch: 106  train_loss: 298.1405944824219\n",
      "epoch: 107  train_loss: 298.0431823730469\n",
      "epoch: 108  train_loss: 298.3252868652344\n",
      "epoch: 109  train_loss: 297.02862548828125\n",
      "epoch: 110  train_loss: 298.60662841796875\n",
      "epoch: 111  train_loss: 297.47705078125\n",
      "epoch: 112  train_loss: 297.49078369140625\n",
      "epoch: 113  train_loss: 298.7126159667969\n",
      "epoch: 114  train_loss: 297.67474365234375\n",
      "epoch: 115  train_loss: 299.3438415527344\n",
      "Early stopping!!!\n"
     ]
    }
   ],
   "source": [
    "# training for mapping\n",
    "lr = 0.001    # step size for gradient descent\n",
    "epochs = 400  # number of training epochs\n",
    "warmup = 50   # number of epochs to wait before enacting early stopping policy\n",
    "patience = 50 # number of epochs with no improvement in eval metric to allow before early stopping\n",
    "# set adamW as optimizer\n",
    "optimizer = torch.optim.AdamW(problem_rel.parameters(), lr=lr)\n",
    "# define trainer\n",
    "trainer = nm.trainer.Trainer(problem_rel, loader_train, loader_dev, loader_test,\n",
    "                             optimizer, epochs=epochs, patience=patience, warmup=warmup)\n",
    "best_model = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "917856c4-f7db-4537-a59c-1499d0d27557",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0  train_loss: 2151.41552734375\n",
      "epoch: 1  train_loss: 2151.20361328125\n",
      "epoch: 2  train_loss: 2150.4990234375\n",
      "epoch: 3  train_loss: 2155.776611328125\n",
      "epoch: 4  train_loss: 2151.611328125\n",
      "epoch: 5  train_loss: 2152.010009765625\n",
      "epoch: 6  train_loss: 2148.83935546875\n",
      "epoch: 7  train_loss: 2147.4033203125\n",
      "epoch: 8  train_loss: 2152.97021484375\n",
      "epoch: 9  train_loss: 2155.150146484375\n",
      "epoch: 10  train_loss: 2145.474853515625\n",
      "epoch: 11  train_loss: 2153.859619140625\n",
      "epoch: 12  train_loss: 2154.91357421875\n",
      "epoch: 13  train_loss: 2149.227783203125\n",
      "epoch: 14  train_loss: 2153.484130859375\n",
      "epoch: 15  train_loss: 2148.620849609375\n",
      "epoch: 16  train_loss: 2149.673095703125\n",
      "epoch: 17  train_loss: 2153.195068359375\n",
      "epoch: 18  train_loss: 2150.64501953125\n",
      "epoch: 19  train_loss: 2147.54931640625\n",
      "epoch: 20  train_loss: 2146.578857421875\n",
      "epoch: 21  train_loss: 2148.906005859375\n",
      "epoch: 22  train_loss: 2148.907470703125\n",
      "epoch: 23  train_loss: 2152.666015625\n",
      "epoch: 24  train_loss: 2149.102294921875\n",
      "epoch: 25  train_loss: 2152.13623046875\n",
      "epoch: 26  train_loss: 2151.83544921875\n",
      "epoch: 27  train_loss: 2150.3759765625\n",
      "epoch: 28  train_loss: 2152.892578125\n",
      "epoch: 29  train_loss: 2148.211181640625\n",
      "epoch: 30  train_loss: 2152.866455078125\n",
      "epoch: 31  train_loss: 2148.762451171875\n",
      "epoch: 32  train_loss: 2149.28759765625\n",
      "epoch: 33  train_loss: 2152.724853515625\n",
      "epoch: 34  train_loss: 2155.165771484375\n",
      "epoch: 35  train_loss: 2152.3447265625\n",
      "epoch: 36  train_loss: 2149.43603515625\n",
      "epoch: 37  train_loss: 2154.317626953125\n",
      "epoch: 38  train_loss: 2150.398681640625\n",
      "epoch: 39  train_loss: 2150.994140625\n",
      "epoch: 40  train_loss: 2150.081787109375\n",
      "epoch: 41  train_loss: 2150.579833984375\n",
      "epoch: 42  train_loss: 2153.39697265625\n",
      "epoch: 43  train_loss: 2150.1044921875\n",
      "epoch: 44  train_loss: 2151.8388671875\n",
      "epoch: 45  train_loss: 2149.84716796875\n",
      "epoch: 46  train_loss: 2144.834716796875\n",
      "epoch: 47  train_loss: 2149.36474609375\n",
      "epoch: 48  train_loss: 2149.016357421875\n",
      "epoch: 49  train_loss: 2151.35205078125\n",
      "epoch: 50  train_loss: 2151.818603515625\n",
      "epoch: 51  train_loss: 2147.615966796875\n",
      "epoch: 52  train_loss: 2151.066162109375\n",
      "epoch: 53  train_loss: 2151.93603515625\n",
      "epoch: 54  train_loss: 2152.059326171875\n",
      "epoch: 55  train_loss: 2151.80029296875\n",
      "epoch: 56  train_loss: 2152.239990234375\n",
      "epoch: 57  train_loss: 2153.177490234375\n",
      "epoch: 58  train_loss: 2151.399169921875\n",
      "epoch: 59  train_loss: 2151.31640625\n",
      "epoch: 60  train_loss: 2151.950439453125\n",
      "epoch: 61  train_loss: 2151.71044921875\n",
      "epoch: 62  train_loss: 2154.63232421875\n",
      "epoch: 63  train_loss: 2148.479736328125\n",
      "epoch: 64  train_loss: 2150.672119140625\n",
      "epoch: 65  train_loss: 2153.793212890625\n",
      "epoch: 66  train_loss: 2152.51171875\n",
      "epoch: 67  train_loss: 2152.156005859375\n",
      "epoch: 68  train_loss: 2152.84912109375\n",
      "epoch: 69  train_loss: 2150.3994140625\n",
      "epoch: 70  train_loss: 2152.835205078125\n",
      "epoch: 71  train_loss: 2150.840576171875\n",
      "epoch: 72  train_loss: 2151.97119140625\n",
      "epoch: 73  train_loss: 2152.545654296875\n",
      "epoch: 74  train_loss: 2152.05712890625\n",
      "epoch: 75  train_loss: 2148.547119140625\n",
      "epoch: 76  train_loss: 2148.32373046875\n",
      "epoch: 77  train_loss: 2151.475830078125\n",
      "epoch: 78  train_loss: 2150.365478515625\n",
      "epoch: 79  train_loss: 2156.243408203125\n",
      "epoch: 80  train_loss: 2150.966796875\n",
      "epoch: 81  train_loss: 2153.061279296875\n",
      "epoch: 82  train_loss: 2150.52099609375\n",
      "epoch: 83  train_loss: 2151.805908203125\n",
      "epoch: 84  train_loss: 2152.961669921875\n",
      "epoch: 85  train_loss: 2155.23828125\n",
      "epoch: 86  train_loss: 2148.844482421875\n",
      "epoch: 87  train_loss: 2150.00927734375\n",
      "epoch: 88  train_loss: 2149.75048828125\n",
      "epoch: 89  train_loss: 2151.344482421875\n",
      "epoch: 90  train_loss: 2153.3232421875\n",
      "epoch: 91  train_loss: 2155.02392578125\n",
      "epoch: 92  train_loss: 2154.991455078125\n",
      "epoch: 93  train_loss: 2148.250732421875\n",
      "epoch: 94  train_loss: 2149.55322265625\n",
      "epoch: 95  train_loss: 2150.64501953125\n",
      "epoch: 96  train_loss: 2145.4013671875\n",
      "epoch: 97  train_loss: 2151.46435546875\n",
      "epoch: 98  train_loss: 2154.1845703125\n",
      "epoch: 99  train_loss: 2151.76025390625\n",
      "epoch: 100  train_loss: 2151.821533203125\n",
      "epoch: 101  train_loss: 2146.77587890625\n",
      "Early stopping!!!\n"
     ]
    }
   ],
   "source": [
    "# freeze sol mapping\n",
    "#problem_rel.freeze()\n",
    "# training for feasibility pump\n",
    "lr = 0.001    # step size for gradient descent\n",
    "epochs = 400  # number of training epochs\n",
    "warmup = 50   # number of epochs to wait before enacting early stopping policy\n",
    "patience = 50 # number of epochs with no improvement in eval metric to allow before early stopping\n",
    "# set adamW as optimizer\n",
    "optimizer = torch.optim.AdamW(problem.parameters(), lr=lr)\n",
    "# define trainer\n",
    "trainer = nm.trainer.Trainer(problem_fp, loader_train, loader_dev, loader_test,\n",
    "                             optimizer, epochs=epochs, patience=patience, warmup=warmup)\n",
    "best_model = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "cb8b7b8b-96db-47cc-8140-91f75aa2f717",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:04<00:00, 218.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           Obj Val  Constraints Viol  Elapsed Time\n",
      "count  1000.000000       1000.000000   1000.000000\n",
      "mean    207.073000          8.717267      0.004109\n",
      "std      73.458327          3.018748      0.000835\n",
      "min      40.000000          2.650724      0.002000\n",
      "25%     156.000000          6.302247      0.003512\n",
      "50%     198.000000          8.302247      0.004004\n",
      "75%     255.000000         10.542614      0.004564\n",
      "max     500.000000         20.109414      0.007529\n"
     ]
    }
   ],
   "source": [
    "sols, objvals, conviols, elapseds = [], [], [], []\n",
    "for p in tqdm(p_test):\n",
    "    datapoints = {\"p\": torch.tensor(np.array([p]), dtype=torch.float32), \"name\": \"test\"}\n",
    "    tick = time.time()\n",
    "    output = problem(datapoints)\n",
    "    tock = time.time()\n",
    "    x = output[\"test_x_rnd\"]\n",
    "    # get values\n",
    "    for ind in model.x:\n",
    "        model.x[ind].value = x[0, ind].item()\n",
    "    xval, objval = model.getVal()\n",
    "    sols.append(xval.values())\n",
    "    objvals.append(objval)\n",
    "    conviols.append(sum(model.calViolation()))\n",
    "    elapseds.append(tock - tick)\n",
    "df = pd.DataFrame({\"Sol\":sols, \"Obj Val\": objvals, \"Constraints Viol\": conviols, \"Elapsed Time\": elapseds})\n",
    "time.sleep(1)\n",
    "print(df.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d761fd9-ab80-4f5a-80ec-cfafb9539b39",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
