{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "be61b914-e1af-43b9-9766-5092593f92dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch import nn\n",
    "from tqdm import tqdm\n",
    "\n",
    "# random seed\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "torch.cuda.manual_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "35fe4527-9dbf-48f0-bfb3-62cda59092aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# turn off warning\n",
    "import logging\n",
    "logging.getLogger('pyomo.core').setLevel(logging.ERROR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5560447-da9d-4cbe-aab4-538aa11c8ee8",
   "metadata": {},
   "source": [
    "## Problem Setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "52363c72-0d20-4c08-bd00-0f62aa73686c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# init\n",
    "num_var = 200     # number of variables\n",
    "num_ineq = 200    # number of constraints\n",
    "num_data = 10000  # number of data\n",
    "test_size = 1000  # number of test size\n",
    "val_size = 1000   # number of validation size\n",
    "train_size = num_data - test_size - val_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "21938c00-d73f-4255-bc7c-bce6e8034cf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data sample from uniform distribution\n",
    "b_samples = torch.from_numpy(np.random.uniform(-1, 1, size=(num_data, num_ineq))).float()\n",
    "data = {\"b\":b_samples}\n",
    "# data split\n",
    "from src.utlis import data_split\n",
    "data_train, data_test, data_dev = data_split(data, test_size=test_size, val_size=val_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5ecf81ff-627d-444d-8d90-121c7e20c217",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch dataloaders\n",
    "from torch.utils.data import DataLoader\n",
    "batch_size = 64\n",
    "loader_train = DataLoader(data_train, batch_size, num_workers=0, collate_fn=data_train.collate_fn, shuffle=True)\n",
    "loader_test = DataLoader(data_test, batch_size, num_workers=0, collate_fn=data_test.collate_fn, shuffle=False)\n",
    "loader_dev = DataLoader(data_dev, batch_size, num_workers=0, collate_fn=data_dev.collate_fn, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16ec0a67-54ea-4a9d-b037-0c7c9390e3f9",
   "metadata": {},
   "source": [
    "## Exact Solver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "50aaabd7-90ed-4628-a1da-c98777868d92",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.problem import msLinear\n",
    "model = msLinear(num_var, num_ineq, timelimit=60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1c8d0f11-7ad1-4886-a76d-430189d06c33",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 100/100 [1:40:48<00:00, 60.48s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       Elapsed Time\n",
      "count    100.000000\n",
      "mean      60.481189\n",
      "std        0.032173\n",
      "min       60.412278\n",
      "25%       60.458226\n",
      "50%       60.482873\n",
      "75%       60.505656\n",
      "max       60.573851\n",
      "Number of infeasible solution: 0\n",
      "Number of None values:  100\n"
     ]
    }
   ],
   "source": [
    "params, sols, objvals, conviols, elapseds = [], [], [], [], []\n",
    "for b in tqdm(data_test.datadict[\"b\"][:100]):\n",
    "    # set params\n",
    "    model.set_param_val({\"b\":b.cpu().numpy()})\n",
    "    # solve\n",
    "    tick = time.time()\n",
    "    try:\n",
    "        xval, objval = model.solve(\"gurobi\")\n",
    "        # eval\n",
    "        params.append(list(b.cpu().numpy()))\n",
    "        sols.append(list(list(xval.values())[0].values()))\n",
    "        objvals.append(objval)\n",
    "        conviols.append(sum(model.cal_violation()))\n",
    "    except:\n",
    "        params.append(list(b.cpu().numpy()))\n",
    "        sols.append(None)\n",
    "        objvals.append(None)\n",
    "        conviols.append(None)\n",
    "    tock = time.time()\n",
    "    elapseds.append(tock - tick)\n",
    "df = pd.DataFrame({\"Param\":params, \"Sol\":sols, \"Obj Val\": objvals, \"Constraints Viol\": conviols, \"Elapsed Time\": elapseds})\n",
    "time.sleep(1)\n",
    "print(df.describe())\n",
    "print(\"Number of infeasible solution: {}\".format(np.sum(df[\"Constraints Viol\"] > 0)))\n",
    "print(\"Number of None values: \", df[\"Sol\"].isna().sum())\n",
    "df.to_csv(\"result/ln_exact_200-200.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cad38e16-1a87-4f11-92b4-b608f7f3ea11",
   "metadata": {},
   "source": [
    "## Heuristic - Round"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "03328361-3f39-461d-ac81-40c8c14717bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.heuristic import naive_round"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2e57b62f-dfb4-4c87-b42e-e43b202ef96c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 100/100 [02:05<00:00,  1.26s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          Obj Val  Constraints Viol  Elapsed Time\n",
      "count  100.000000        100.000000    100.000000\n",
      "mean  -172.915954         11.712249      0.414948\n",
      "std      0.441110          1.520185      0.073842\n",
      "min   -173.875061          7.938176      0.355120\n",
      "25%   -173.234474         10.614469      0.368889\n",
      "50%   -172.926655         11.520263      0.376018\n",
      "75%   -172.585474         12.504157      0.430602\n",
      "max   -171.680405         16.651590      0.660999\n",
      "Number of infeasible solution: 100\n",
      "Number of None values:  0\n"
     ]
    }
   ],
   "source": [
    "params, sols, objvals, conviols, elapseds = [], [], [], [], []\n",
    "for b in tqdm(data_test.datadict[\"b\"][:100]):\n",
    "    # set params\n",
    "    model.set_param_val({\"b\":b.cpu().numpy()})\n",
    "    # relax\n",
    "    model_rel = model.relax()\n",
    "    # solve\n",
    "    tick = time.time()\n",
    "    try:\n",
    "        xval_rel, _ = model_rel.solve(\"gurobi\")\n",
    "        xval, objval = naive_round(xval_rel, model)\n",
    "        # eval\n",
    "        params.append(list(b.cpu().numpy()))\n",
    "        sols.append(list(list(xval.values())[0].values()))\n",
    "        objvals.append(objval)\n",
    "        conviols.append(sum(model.cal_violation()))\n",
    "    except:\n",
    "        params.append(list(b.cpu().numpy()))\n",
    "        sols.append(None)\n",
    "        objvals.append(None)\n",
    "        conviols.append(None)\n",
    "    tock = time.time()\n",
    "    elapseds.append(tock - tick)\n",
    "df = pd.DataFrame({\"Param\":params, \"Sol\":sols, \"Obj Val\": objvals, \"Constraints Viol\": conviols, \"Elapsed Time\": elapseds})\n",
    "time.sleep(1)\n",
    "print(df.describe())\n",
    "print(\"Number of infeasible solution: {}\".format(np.sum(df[\"Constraints Viol\"] > 0)))\n",
    "print(\"Number of None values: \", df[\"Sol\"].isna().sum())\n",
    "df.to_csv(\"result/ln_heur_rnd_200-200.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96304fbd-08f5-4be7-a81e-83c85332029c",
   "metadata": {},
   "source": [
    "## Heuristic - N1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "356187af-e02d-4478-bf78-ada6a2e1b94b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_heur = model.first_solution_heuristic(nodes_limit=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f4c7fc52-cbbc-4414-bf1d-316d65c8db5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 100/100 [15:37<00:00,  9.38s/it]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "All arrays must be of the same length",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 20\u001b[0m\n\u001b[0;32m     18\u001b[0m         conviols\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m     19\u001b[0m     tock \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m---> 20\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDataFrame\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mParam\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mSol\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43msols\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mObj Val\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mobjvals\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mConstraints Viol\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mconviols\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mElapsed Time\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43melapseds\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     21\u001b[0m time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28mprint\u001b[39m(df\u001b[38;5;241m.\u001b[39mdescribe())\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\pandas\\core\\frame.py:778\u001b[0m, in \u001b[0;36mDataFrame.__init__\u001b[1;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[0;32m    772\u001b[0m     mgr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_mgr(\n\u001b[0;32m    773\u001b[0m         data, axes\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mindex\u001b[39m\u001b[38;5;124m\"\u001b[39m: index, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumns\u001b[39m\u001b[38;5;124m\"\u001b[39m: columns}, dtype\u001b[38;5;241m=\u001b[39mdtype, copy\u001b[38;5;241m=\u001b[39mcopy\n\u001b[0;32m    774\u001b[0m     )\n\u001b[0;32m    776\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, \u001b[38;5;28mdict\u001b[39m):\n\u001b[0;32m    777\u001b[0m     \u001b[38;5;66;03m# GH#38939 de facto copy defaults to False only in non-dict cases\u001b[39;00m\n\u001b[1;32m--> 778\u001b[0m     mgr \u001b[38;5;241m=\u001b[39m \u001b[43mdict_to_mgr\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtyp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmanager\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    779\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, ma\u001b[38;5;241m.\u001b[39mMaskedArray):\n\u001b[0;32m    780\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mma\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m mrecords\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\pandas\\core\\internals\\construction.py:503\u001b[0m, in \u001b[0;36mdict_to_mgr\u001b[1;34m(data, index, columns, dtype, typ, copy)\u001b[0m\n\u001b[0;32m    499\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    500\u001b[0m         \u001b[38;5;66;03m# dtype check to exclude e.g. range objects, scalars\u001b[39;00m\n\u001b[0;32m    501\u001b[0m         arrays \u001b[38;5;241m=\u001b[39m [x\u001b[38;5;241m.\u001b[39mcopy() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(x, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m x \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m arrays]\n\u001b[1;32m--> 503\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43marrays_to_mgr\u001b[49m\u001b[43m(\u001b[49m\u001b[43marrays\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtyp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtyp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconsolidate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\pandas\\core\\internals\\construction.py:114\u001b[0m, in \u001b[0;36marrays_to_mgr\u001b[1;34m(arrays, columns, index, dtype, verify_integrity, typ, consolidate)\u001b[0m\n\u001b[0;32m    111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m verify_integrity:\n\u001b[0;32m    112\u001b[0m     \u001b[38;5;66;03m# figure out the index, if necessary\u001b[39;00m\n\u001b[0;32m    113\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m index \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 114\u001b[0m         index \u001b[38;5;241m=\u001b[39m \u001b[43m_extract_index\u001b[49m\u001b[43m(\u001b[49m\u001b[43marrays\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    115\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    116\u001b[0m         index \u001b[38;5;241m=\u001b[39m ensure_index(index)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\pandas\\core\\internals\\construction.py:677\u001b[0m, in \u001b[0;36m_extract_index\u001b[1;34m(data)\u001b[0m\n\u001b[0;32m    675\u001b[0m lengths \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mset\u001b[39m(raw_lengths))\n\u001b[0;32m    676\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(lengths) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m--> 677\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAll arrays must be of the same length\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    679\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m have_dicts:\n\u001b[0;32m    680\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    681\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMixing dicts with non-Series may lead to ambiguous ordering.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    682\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: All arrays must be of the same length"
     ]
    }
   ],
   "source": [
    "params, sols, objvals, conviols, elapseds = [], [], [], [], []\n",
    "for b in tqdm(data_test.datadict[\"b\"][:100]):\n",
    "    # set params\n",
    "    model_heur.set_param_val({\"b\":b.cpu().numpy()})\n",
    "    # solve\n",
    "    tick = time.time()\n",
    "    try:\n",
    "        xval, objval = model_heur.solve(\"gurobi\")\n",
    "        # eval\n",
    "        params.append(list(b))\n",
    "        sols.append(list(list(xval.values())[0].values()))\n",
    "        objvals.append(objval)\n",
    "        conviols.append(sum(model_heur.cal_violation()))\n",
    "    except:\n",
    "        params.append(list(b))\n",
    "        sols.append(None)\n",
    "        objvals.append(None)\n",
    "        conviols.append(None)\n",
    "    tock = time.time()\n",
    "df = pd.DataFrame({\"Param\":params, \"Sol\":sols, \"Obj Val\": objvals, \"Constraints Viol\": conviols, \"Elapsed Time\": elapseds})\n",
    "time.sleep(1)\n",
    "print(df.describe())\n",
    "print(\"Number of infeasible solution: {}\".format(np.sum(df[\"Constraints Viol\"] > 0)))\n",
    "print(\"Number of None values: \", df[\"Sol\"].isna().sum())\n",
    "df.to_csv(\"result/ln_heur_n1_200-200.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9819c60c-df5f-41c6-ae83-888c75aa78d7",
   "metadata": {},
   "source": [
    "## Learnable Rounding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6a4fb3e2-9c47-4c10-8116-e94be18ea59c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# random seed\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "torch.cuda.manual_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "536d2d58-67ba-408a-b72a-7c5bd54a5ff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "penalty_weight = 100  # weight of constraint violation penealty\n",
    "hlayers_sol = 5       # number of hidden layers for solution mapping\n",
    "hlayers_rnd = 4       # number of hidden layers for solution mapping\n",
    "hsize = 512           # width of hidden layers for solution mapping\n",
    "lr = 1e-3             # learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fee1241e-e53a-4cbb-927a-edcdb3537303",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set problem\n",
    "import neuromancer as nm\n",
    "from src.problem import nmLinear\n",
    "from src.func.layer import netFC\n",
    "from src.func import roundGumbelModel\n",
    "# build neural architecture for the solution map\n",
    "func = nm.modules.blocks.MLP(insize=num_ineq, outsize=num_var, bias=True,\n",
    "                             linear_map=nm.slim.maps[\"linear\"],\n",
    "                             nonlin=nn.ReLU, hsizes=[hsize]*hlayers_sol)\n",
    "smap = nm.system.Node(func, [\"b\"], [\"x\"], name=\"smap\")\n",
    "# define rounding model\n",
    "layers_rnd = netFC(input_dim=num_ineq+num_var, hidden_dims=[hsize]*hlayers_rnd, output_dim=num_var)\n",
    "rnd = roundGumbelModel(layers=layers_rnd, param_keys=[\"b\"], var_keys=[\"x\"],  output_keys=[\"x_rnd\"], \n",
    "                       int_ind=model.int_ind, continuous_update=True, name=\"round\")\n",
    "# build neuromancer problem for rounding\n",
    "components = nn.ModuleList([smap, rnd]).to(\"cuda\")\n",
    "loss_fn = nmLinear([\"b\", \"x_rnd\"], num_var, num_ineq, penalty_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e4f10263-5f69-4288-a4e8-8be83929c8d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Validation Loss: 9255.82\n",
      "Epoch 1, Validation Loss: -21.64\n",
      "Epoch 2, Validation Loss: -36.63\n",
      "Epoch 3, Validation Loss: -57.95\n",
      "Epoch 4, Validation Loss: -78.46\n",
      "Epoch 5, Validation Loss: -82.78\n",
      "Epoch 6, Validation Loss: -80.96\n",
      "Epoch 7, Validation Loss: -97.18\n",
      "Epoch 8, Validation Loss: -46.63\n",
      "Epoch 9, Validation Loss: -73.47\n",
      "Epoch 10, Validation Loss: -87.08\n",
      "Epoch 11, Validation Loss: -48.10\n",
      "Epoch 12, Validation Loss: -107.01\n",
      "Epoch 13, Validation Loss: -111.50\n",
      "Epoch 14, Validation Loss: -112.37\n",
      "Epoch 15, Validation Loss: -110.54\n",
      "Epoch 16, Validation Loss: -125.96\n",
      "Epoch 17, Validation Loss: -89.43\n",
      "Epoch 18, Validation Loss: -132.55\n",
      "Epoch 19, Validation Loss: -132.82\n",
      "Epoch 20, Validation Loss: -76.69\n",
      "Epoch 21, Validation Loss: -123.48\n",
      "Epoch 22, Validation Loss: -138.29\n",
      "Epoch 23, Validation Loss: -129.41\n",
      "Epoch 24, Validation Loss: -137.11\n",
      "Epoch 25, Validation Loss: -133.70\n",
      "Epoch 26, Validation Loss: -139.79\n",
      "Epoch 27, Validation Loss: -139.31\n",
      "Epoch 28, Validation Loss: -142.44\n",
      "Epoch 29, Validation Loss: -140.79\n",
      "Epoch 30, Validation Loss: -127.15\n",
      "Epoch 31, Validation Loss: -120.87\n",
      "Epoch 32, Validation Loss: -138.20\n",
      "Epoch 33, Validation Loss: -141.11\n",
      "Epoch 34, Validation Loss: -134.63\n",
      "Epoch 35, Validation Loss: 882.29\n",
      "Epoch 36, Validation Loss: 491.82\n",
      "Epoch 37, Validation Loss: -103.81\n",
      "Epoch 38, Validation Loss: -139.88\n",
      "Epoch 39, Validation Loss: 73.75\n",
      "Epoch 40, Validation Loss: 2.46\n",
      "Epoch 41, Validation Loss: 84.51\n",
      "Epoch 42, Validation Loss: 2308.19\n",
      "Epoch 43, Validation Loss: -136.37\n",
      "Epoch 44, Validation Loss: -135.18\n",
      "Epoch 45, Validation Loss: -140.09\n",
      "Epoch 46, Validation Loss: 33.44\n",
      "Epoch 47, Validation Loss: -139.20\n",
      "Early stopping at epoch 47\n",
      "Best model loaded.\n",
      "Training complete.\n",
      "The training time is 73.87 sec.\n"
     ]
    }
   ],
   "source": [
    "from src.problem.neuromancer.trainer import trainer\n",
    "# training\n",
    "epochs = 200                    # number of training epochs\n",
    "warmup = 20                     # number of epochs to wait before enacting early stopping policy\n",
    "patience = 20                   # number of epochs with no improvement in eval metric to allow before early stopping\n",
    "optimizer = torch.optim.AdamW(components.parameters(), lr=lr)\n",
    "# create a trainer for the problem\n",
    "my_trainer = trainer(components, loss_fn, optimizer, epochs=epochs, patience=patience, warmup=warmup, device=\"cuda\")\n",
    "# training for the rounding problem\n",
    "my_trainer.train(loader_train, loader_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3e835084-eb75-45d9-bcbd-5fb4c613fb41",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 100/100 [00:10<00:00,  9.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          Obj Val  Constraints Viol  Elapsed Time\n",
      "count  100.000000        100.000000    100.000000\n",
      "mean  -143.271993          0.004226      0.003413\n",
      "std      1.381116          0.019353      0.000813\n",
      "min   -145.451933          0.000000      0.001967\n",
      "25%   -144.158258          0.000000      0.003001\n",
      "50%   -143.774509          0.000000      0.003162\n",
      "75%   -142.548244          0.000000      0.003769\n",
      "max   -139.070834          0.138614      0.006208\n",
      "Number of infeasible solution: 6\n"
     ]
    }
   ],
   "source": [
    "params, sols, objvals, conviols, elapseds = [], [], [], [], []\n",
    "for b in tqdm(data_test.datadict[\"b\"][:100]):\n",
    "    # data point as tensor\n",
    "    datapoints = {\"b\": torch.unsqueeze(b, 0).to(\"cuda\"), \n",
    "                  \"name\": \"test\"}\n",
    "    # infer\n",
    "    components.eval()\n",
    "    tick = time.time()\n",
    "    with torch.no_grad():\n",
    "        for comp in components:\n",
    "            datapoints.update(comp(datapoints))\n",
    "    tock = time.time()\n",
    "    # assign params\n",
    "    model.set_param_val({\"b\":b.cpu().numpy()})\n",
    "    # assign vars\n",
    "    x = datapoints[\"x_rnd\"]\n",
    "    for i in range(num_var):\n",
    "        model.vars[\"x\"][i].value = x[0,i].item()\n",
    "    # get solutions\n",
    "    xval, objval = model.get_val()    \n",
    "    params.append(list(b.cpu().numpy()))\n",
    "    sols.append(list(list(xval.values())[0].values()))\n",
    "    objvals.append(objval)\n",
    "    conviols.append(sum(model.cal_violation()))\n",
    "    elapseds.append(tock - tick)\n",
    "df = pd.DataFrame({\"Param\":params, \"Sol\":sols, \"Obj Val\": objvals, \"Constraints Viol\": conviols, \"Elapsed Time\": elapseds})\n",
    "time.sleep(1)\n",
    "print(df.describe())\n",
    "print(\"Number of infeasible solution: {}\".format(np.sum(df[\"Constraints Viol\"] > 0)))\n",
    "df.to_csv(\"result/ln_lr_200-200.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dbf8a72-29b9-49c4-b3c1-0d68895c1c1f",
   "metadata": {},
   "source": [
    "## Learnable Threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "36c57dbb-8dee-4976-8922-82a155fe0128",
   "metadata": {},
   "outputs": [],
   "source": [
    "# random seed\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "torch.cuda.manual_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d46b9817-87ac-4af0-b387-b10c9a4998f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "penalty_weight = 100  # weight of constraint violation penealty\n",
    "hlayers_sol = 5       # number of hidden layers for solution mapping\n",
    "hlayers_rnd = 4       # number of hidden layers for solution mapping\n",
    "hsize = 512           # width of hidden layers for solution mapping\n",
    "lr = 1e-3             # learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "fa8b147f-39c8-4eb7-bb95-93dd8e4cb1cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set problem\n",
    "import neuromancer as nm\n",
    "from src.problem import nmLinear\n",
    "from src.func.layer import netFC\n",
    "from src.func import roundThresholdModel\n",
    "# build neural architecture for the solution map\n",
    "func = nm.modules.blocks.MLP(insize=num_ineq, outsize=num_var, bias=True,\n",
    "                             linear_map=nm.slim.maps[\"linear\"],\n",
    "                             nonlin=nn.ReLU, hsizes=[hsize]*hlayers_sol)\n",
    "smap = nm.system.Node(func, [\"b\"], [\"x\"], name=\"smap\")\n",
    "# define rounding model\n",
    "layers_rnd = netFC(input_dim=num_ineq+num_var, hidden_dims=[hsize]*hlayers_rnd, output_dim=num_var)\n",
    "rnd = roundThresholdModel(layers=layers_rnd, param_keys=[\"b\"], var_keys=[\"x\"],  output_keys=[\"x_rnd\"], \n",
    "                       int_ind=model.int_ind, continuous_update=True, name=\"round\")\n",
    "# build neuromancer problem for rounding\n",
    "components = nn.ModuleList([smap, rnd]).to(\"cuda\")\n",
    "loss_fn = nmLinear([\"b\", \"x_rnd\"], num_var, num_ineq, penalty_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c6f5bb17-8c3c-4dcd-952d-caa1d7c4c4fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Validation Loss: 5004.20\n",
      "Epoch 1, Validation Loss: -45.97\n",
      "Epoch 2, Validation Loss: -19.76\n",
      "Epoch 3, Validation Loss: -26.31\n",
      "Epoch 4, Validation Loss: -79.59\n",
      "Epoch 5, Validation Loss: -74.84\n",
      "Epoch 6, Validation Loss: -82.11\n",
      "Epoch 7, Validation Loss: -96.68\n",
      "Epoch 8, Validation Loss: -101.14\n",
      "Epoch 9, Validation Loss: 73.48\n",
      "Epoch 10, Validation Loss: -89.17\n",
      "Epoch 11, Validation Loss: -100.19\n",
      "Epoch 12, Validation Loss: -94.68\n",
      "Epoch 13, Validation Loss: -93.51\n",
      "Epoch 14, Validation Loss: -107.89\n",
      "Epoch 15, Validation Loss: -119.03\n",
      "Epoch 16, Validation Loss: -122.29\n",
      "Epoch 17, Validation Loss: -132.14\n",
      "Epoch 18, Validation Loss: -128.82\n",
      "Epoch 19, Validation Loss: -90.11\n",
      "Epoch 20, Validation Loss: -105.28\n",
      "Epoch 21, Validation Loss: -127.07\n",
      "Epoch 22, Validation Loss: -88.65\n",
      "Epoch 23, Validation Loss: -34.92\n",
      "Epoch 24, Validation Loss: -138.54\n",
      "Epoch 25, Validation Loss: -126.90\n",
      "Epoch 26, Validation Loss: -122.93\n",
      "Epoch 27, Validation Loss: -139.87\n",
      "Epoch 28, Validation Loss: -142.24\n",
      "Epoch 29, Validation Loss: -133.51\n",
      "Epoch 30, Validation Loss: 450.69\n",
      "Epoch 31, Validation Loss: 267.60\n",
      "Epoch 32, Validation Loss: -140.77\n",
      "Epoch 33, Validation Loss: -130.78\n",
      "Epoch 34, Validation Loss: -110.18\n",
      "Epoch 35, Validation Loss: 1527.28\n",
      "Epoch 36, Validation Loss: -100.22\n",
      "Epoch 37, Validation Loss: -141.18\n",
      "Epoch 38, Validation Loss: -139.64\n",
      "Epoch 39, Validation Loss: -143.58\n",
      "Epoch 40, Validation Loss: -126.64\n",
      "Epoch 41, Validation Loss: -143.34\n",
      "Epoch 42, Validation Loss: -110.99\n",
      "Epoch 43, Validation Loss: -138.12\n",
      "Epoch 44, Validation Loss: -137.63\n",
      "Epoch 45, Validation Loss: -130.49\n",
      "Epoch 46, Validation Loss: 251.45\n",
      "Epoch 47, Validation Loss: -137.11\n",
      "Epoch 48, Validation Loss: -144.90\n",
      "Epoch 49, Validation Loss: -64.10\n",
      "Epoch 50, Validation Loss: -140.03\n",
      "Epoch 51, Validation Loss: -81.01\n",
      "Epoch 52, Validation Loss: -133.89\n",
      "Epoch 53, Validation Loss: -142.54\n",
      "Epoch 54, Validation Loss: -90.98\n",
      "Epoch 55, Validation Loss: -135.89\n",
      "Epoch 56, Validation Loss: -38.73\n",
      "Epoch 57, Validation Loss: -49.33\n",
      "Epoch 58, Validation Loss: -143.91\n",
      "Epoch 59, Validation Loss: -146.34\n",
      "Epoch 60, Validation Loss: -139.73\n",
      "Epoch 61, Validation Loss: -86.77\n",
      "Epoch 62, Validation Loss: -141.66\n",
      "Epoch 63, Validation Loss: -137.26\n",
      "Epoch 64, Validation Loss: -141.95\n",
      "Epoch 65, Validation Loss: -141.94\n",
      "Epoch 66, Validation Loss: -123.68\n",
      "Epoch 67, Validation Loss: -138.61\n",
      "Epoch 68, Validation Loss: -135.70\n",
      "Epoch 69, Validation Loss: -138.32\n",
      "Epoch 70, Validation Loss: 58.99\n",
      "Epoch 71, Validation Loss: -146.11\n",
      "Epoch 72, Validation Loss: -146.22\n",
      "Epoch 73, Validation Loss: -145.97\n",
      "Epoch 74, Validation Loss: -135.63\n",
      "Epoch 75, Validation Loss: -144.76\n",
      "Epoch 76, Validation Loss: -148.42\n",
      "Epoch 77, Validation Loss: -146.27\n",
      "Epoch 78, Validation Loss: -143.69\n",
      "Epoch 79, Validation Loss: -95.75\n",
      "Epoch 80, Validation Loss: -147.03\n",
      "Epoch 81, Validation Loss: -116.72\n",
      "Epoch 82, Validation Loss: -148.65\n",
      "Epoch 83, Validation Loss: 141.84\n",
      "Epoch 84, Validation Loss: -139.55\n",
      "Epoch 85, Validation Loss: -129.83\n",
      "Epoch 86, Validation Loss: -137.56\n",
      "Epoch 87, Validation Loss: -124.66\n",
      "Epoch 88, Validation Loss: 44.43\n",
      "Epoch 89, Validation Loss: -146.42\n",
      "Epoch 90, Validation Loss: -150.29\n",
      "Epoch 91, Validation Loss: -139.54\n",
      "Epoch 92, Validation Loss: -147.29\n",
      "Epoch 93, Validation Loss: -148.26\n",
      "Epoch 94, Validation Loss: -143.01\n",
      "Epoch 95, Validation Loss: -145.67\n",
      "Epoch 96, Validation Loss: -148.37\n",
      "Epoch 97, Validation Loss: -144.14\n",
      "Epoch 98, Validation Loss: 38.57\n",
      "Epoch 99, Validation Loss: -148.00\n",
      "Epoch 100, Validation Loss: -135.08\n",
      "Epoch 101, Validation Loss: -125.70\n",
      "Epoch 102, Validation Loss: -90.13\n",
      "Epoch 103, Validation Loss: -39.99\n",
      "Epoch 104, Validation Loss: -104.99\n",
      "Epoch 105, Validation Loss: -10.88\n",
      "Epoch 106, Validation Loss: -144.03\n",
      "Epoch 107, Validation Loss: -146.54\n",
      "Epoch 108, Validation Loss: 61.22\n",
      "Epoch 109, Validation Loss: -146.66\n",
      "Early stopping at epoch 109\n",
      "Best model loaded.\n",
      "Training complete.\n",
      "The training time is 163.37 sec.\n"
     ]
    }
   ],
   "source": [
    "from src.problem.neuromancer.trainer import trainer\n",
    "# training\n",
    "epochs = 200                    # number of training epochs\n",
    "warmup = 20                     # number of epochs to wait before enacting early stopping policy\n",
    "patience = 20                   # number of epochs with no improvement in eval metric to allow before early stopping\n",
    "optimizer = torch.optim.AdamW(components.parameters(), lr=lr)\n",
    "# create a trainer for the problem\n",
    "my_trainer = trainer(components, loss_fn, optimizer, epochs=epochs, patience=patience, warmup=warmup, device=\"cuda\")\n",
    "# training for the rounding problem\n",
    "my_trainer.train(loader_train, loader_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6584d57e-6b4c-4262-82f5-55735ef17cac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 100/100 [00:13<00:00,  7.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          Obj Val  Constraints Viol  Elapsed Time\n",
      "count  100.000000        100.000000    100.000000\n",
      "mean  -150.722680          0.001161      0.003838\n",
      "std      0.118693          0.009434      0.001032\n",
      "min   -150.940154          0.000000      0.001996\n",
      "25%   -150.802877          0.000000      0.003001\n",
      "50%   -150.738322          0.000000      0.003521\n",
      "75%   -150.704321          0.000000      0.004512\n",
      "max   -150.056339          0.092000      0.007387\n",
      "Number of infeasible solution: 3\n"
     ]
    }
   ],
   "source": [
    "params, sols, objvals, conviols, elapseds = [], [], [], [], []\n",
    "for b in tqdm(data_test.datadict[\"b\"][:100]):\n",
    "    # data point as tensor\n",
    "    datapoints = {\"b\": torch.unsqueeze(b, 0).to(\"cuda\"), \n",
    "                  \"name\": \"test\"}\n",
    "    # infer\n",
    "    components.eval()\n",
    "    tick = time.time()\n",
    "    with torch.no_grad():\n",
    "        for comp in components:\n",
    "            datapoints.update(comp(datapoints))\n",
    "    tock = time.time()\n",
    "    # assign params\n",
    "    model.set_param_val({\"b\":b.cpu().numpy()})\n",
    "    # assign vars\n",
    "    x = datapoints[\"x_rnd\"]\n",
    "    for i in range(num_var):\n",
    "        model.vars[\"x\"][i].value = x[0,i].item()\n",
    "    # get solutions\n",
    "    xval, objval = model.get_val()    \n",
    "    params.append(list(b.cpu().numpy()))\n",
    "    sols.append(list(list(xval.values())[0].values()))\n",
    "    objvals.append(objval)\n",
    "    conviols.append(sum(model.cal_violation()))\n",
    "    elapseds.append(tock - tick)\n",
    "df = pd.DataFrame({\"Param\":params, \"Sol\":sols, \"Obj Val\": objvals, \"Constraints Viol\": conviols, \"Elapsed Time\": elapseds})\n",
    "time.sleep(1)\n",
    "print(df.describe())\n",
    "print(\"Number of infeasible solution: {}\".format(np.sum(df[\"Constraints Viol\"] > 0)))\n",
    "df.to_csv(\"result/ln_lt_200-200.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ee67a6c-67a8-49f1-8623-d3d32a9ac5be",
   "metadata": {},
   "source": [
    "### Parametric Learning Then Rounding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "382a0a8a-d846-4182-ab69-7d618edea9cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# random seed\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "torch.cuda.manual_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2279da2f-e4f2-4f87-979d-f6ea175b7cc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "penalty_weight = 100  # weight of constraint violation penealty\n",
    "hlayers_sol = 5       # number of hidden layers for solution mapping\n",
    "hsize = 512           # width of hidden layers for solution mapping\n",
    "lr = 1e-3             # learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "dc92f89e-28b1-483b-a284-7bded54e9958",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set problem\n",
    "import neuromancer as nm\n",
    "from src.problem import nmLinear\n",
    "from src.func.layer import netFC\n",
    "# build neural architecture for the solution map\n",
    "func = nm.modules.blocks.MLP(insize=num_ineq, outsize=num_var, bias=True,\n",
    "                             linear_map=nm.slim.maps[\"linear\"],\n",
    "                             nonlin=nn.ReLU, hsizes=[hsize]*hlayers_sol)\n",
    "smap = nm.system.Node(func, [\"b\"], [\"x\"], name=\"smap\")\n",
    "# build neuromancer problem for rounding\n",
    "components = nn.ModuleList([smap]).to(\"cuda\")\n",
    "loss_fn = nmLinear([\"b\", \"x\"], num_var, num_ineq, penalty_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1ec4af4d-5722-4162-819c-6c2719bd9d12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Validation Loss: 5051.06\n",
      "Epoch 1, Validation Loss: -36.89\n",
      "Epoch 2, Validation Loss: 7.74\n",
      "Epoch 3, Validation Loss: -63.83\n",
      "Epoch 4, Validation Loss: -80.71\n",
      "Epoch 5, Validation Loss: -81.09\n",
      "Epoch 6, Validation Loss: -63.37\n",
      "Epoch 7, Validation Loss: -82.46\n",
      "Epoch 8, Validation Loss: -96.28\n",
      "Epoch 9, Validation Loss: -99.80\n",
      "Epoch 10, Validation Loss: -96.16\n",
      "Epoch 11, Validation Loss: -103.05\n",
      "Epoch 12, Validation Loss: -110.83\n",
      "Epoch 13, Validation Loss: -108.77\n",
      "Epoch 14, Validation Loss: -109.20\n",
      "Epoch 15, Validation Loss: 110.05\n",
      "Epoch 16, Validation Loss: -105.38\n",
      "Epoch 17, Validation Loss: -120.44\n",
      "Epoch 18, Validation Loss: -12.38\n",
      "Epoch 19, Validation Loss: -114.14\n",
      "Epoch 20, Validation Loss: -129.72\n",
      "Epoch 21, Validation Loss: -133.75\n",
      "Epoch 22, Validation Loss: -131.80\n",
      "Epoch 23, Validation Loss: -137.62\n",
      "Epoch 24, Validation Loss: -138.85\n",
      "Epoch 25, Validation Loss: -139.17\n",
      "Epoch 26, Validation Loss: -139.47\n",
      "Epoch 27, Validation Loss: -136.91\n",
      "Epoch 28, Validation Loss: -141.07\n",
      "Epoch 29, Validation Loss: 48.52\n",
      "Epoch 30, Validation Loss: -143.58\n",
      "Epoch 31, Validation Loss: -98.25\n",
      "Epoch 32, Validation Loss: -18.15\n",
      "Epoch 33, Validation Loss: -146.69\n",
      "Epoch 34, Validation Loss: -146.23\n",
      "Epoch 35, Validation Loss: -147.49\n",
      "Epoch 36, Validation Loss: -142.96\n",
      "Epoch 37, Validation Loss: -141.65\n",
      "Epoch 38, Validation Loss: -118.88\n",
      "Epoch 39, Validation Loss: -136.77\n",
      "Epoch 40, Validation Loss: -147.95\n",
      "Epoch 41, Validation Loss: -145.62\n",
      "Epoch 42, Validation Loss: -146.80\n",
      "Epoch 43, Validation Loss: -90.13\n",
      "Epoch 44, Validation Loss: -140.86\n",
      "Epoch 45, Validation Loss: 65.52\n",
      "Epoch 46, Validation Loss: -146.23\n",
      "Epoch 47, Validation Loss: -134.77\n",
      "Epoch 48, Validation Loss: -144.70\n",
      "Epoch 49, Validation Loss: -148.82\n",
      "Epoch 50, Validation Loss: -149.42\n",
      "Epoch 51, Validation Loss: -148.68\n",
      "Epoch 52, Validation Loss: -146.97\n",
      "Epoch 53, Validation Loss: -146.51\n",
      "Epoch 54, Validation Loss: -148.91\n",
      "Epoch 55, Validation Loss: -0.91\n",
      "Epoch 56, Validation Loss: -149.02\n",
      "Epoch 57, Validation Loss: -140.12\n",
      "Epoch 58, Validation Loss: -82.32\n",
      "Epoch 59, Validation Loss: -138.05\n",
      "Epoch 60, Validation Loss: -150.24\n",
      "Epoch 61, Validation Loss: -29.40\n",
      "Epoch 62, Validation Loss: -130.82\n",
      "Epoch 63, Validation Loss: -140.29\n",
      "Epoch 64, Validation Loss: -120.27\n",
      "Epoch 65, Validation Loss: -149.16\n",
      "Epoch 66, Validation Loss: -148.50\n",
      "Epoch 67, Validation Loss: -149.83\n",
      "Epoch 68, Validation Loss: -130.95\n",
      "Epoch 69, Validation Loss: -146.31\n",
      "Epoch 70, Validation Loss: -149.90\n",
      "Epoch 71, Validation Loss: -153.20\n",
      "Epoch 72, Validation Loss: -151.10\n",
      "Epoch 73, Validation Loss: -117.12\n",
      "Epoch 74, Validation Loss: -125.22\n",
      "Epoch 75, Validation Loss: -149.95\n",
      "Epoch 76, Validation Loss: -150.68\n",
      "Epoch 77, Validation Loss: -152.39\n",
      "Epoch 78, Validation Loss: -150.59\n",
      "Epoch 79, Validation Loss: -141.63\n",
      "Epoch 80, Validation Loss: -150.80\n",
      "Epoch 81, Validation Loss: -148.23\n",
      "Epoch 82, Validation Loss: -150.80\n",
      "Epoch 83, Validation Loss: -152.32\n",
      "Epoch 84, Validation Loss: -152.66\n",
      "Epoch 85, Validation Loss: -151.16\n",
      "Epoch 86, Validation Loss: -151.17\n",
      "Epoch 87, Validation Loss: -147.11\n",
      "Epoch 88, Validation Loss: -148.44\n",
      "Epoch 89, Validation Loss: -151.60\n",
      "Epoch 90, Validation Loss: -147.46\n",
      "Early stopping at epoch 90\n",
      "Best model loaded.\n",
      "Training complete.\n",
      "The training time is 54.87 sec.\n"
     ]
    }
   ],
   "source": [
    "from src.problem.neuromancer.trainer import trainer\n",
    "# training\n",
    "epochs = 200                    # number of training epochs\n",
    "warmup = 20                     # number of epochs to wait before enacting early stopping policy\n",
    "patience = 20                   # number of epochs with no improvement in eval metric to allow before early stopping\n",
    "optimizer = torch.optim.AdamW(components.parameters(), lr=lr)\n",
    "# create a trainer for the problem\n",
    "my_trainer = trainer(components, loss_fn, optimizer, epochs=epochs, patience=patience, warmup=warmup, device=\"cuda\")\n",
    "# training for the rounding problem\n",
    "my_trainer.train(loader_train, loader_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8eb78c73-6a92-4639-b48c-8e9aa0c8dd37",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 100/100 [00:11<00:00,  8.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          Obj Val  Constraints Viol  Elapsed Time\n",
      "count  100.000000        100.000000    100.000000\n",
      "mean  -154.078087          0.257414      0.000934\n",
      "std      0.028788          0.247080      0.000525\n",
      "min   -154.159454          0.000000      0.000000\n",
      "25%   -154.086661          0.043950      0.000997\n",
      "50%   -154.060520          0.187798      0.001005\n",
      "75%   -154.060520          0.436228      0.001099\n",
      "max   -154.060520          1.102671      0.002430\n",
      "Number of infeasible solution: 81\n"
     ]
    }
   ],
   "source": [
    "from src.heuristic import naive_round\n",
    "params, sols, objvals, conviols, elapseds = [], [], [], [], []\n",
    "for b in tqdm(data_test.datadict[\"b\"][:100]):\n",
    "    # data point as tensor\n",
    "    datapoints = {\"b\": torch.unsqueeze(b, 0).to(\"cuda\"), \n",
    "                  \"name\": \"test\"}\n",
    "    # infer\n",
    "    components.eval()\n",
    "    tick = time.time()\n",
    "    with torch.no_grad():\n",
    "        for comp in components:\n",
    "            datapoints.update(comp(datapoints))\n",
    "    tock = time.time()\n",
    "    # assign params\n",
    "    model.set_param_val({\"b\":b.cpu().numpy()})\n",
    "    # assign vars\n",
    "    x = datapoints[\"x\"]\n",
    "    for i in range(num_var):\n",
    "        model.vars[\"x\"][i].value = x[0,i].item()\n",
    "    # get solutions\n",
    "    xval_rel, _ = model.get_val()\n",
    "    xval, objval = naive_round(xval_rel, model)\n",
    "    params.append(list(b.cpu().numpy()))\n",
    "    sols.append(list(list(xval.values())[0].values()))\n",
    "    objvals.append(objval)\n",
    "    conviols.append(sum(model.cal_violation()))\n",
    "    elapseds.append(tock - tick)\n",
    "df = pd.DataFrame({\"Param\":params, \"Sol\":sols, \"Obj Val\": objvals, \"Constraints Viol\": conviols, \"Elapsed Time\": elapseds})\n",
    "time.sleep(1)\n",
    "print(df.describe())\n",
    "print(\"Number of infeasible solution: {}\".format(np.sum(df[\"Constraints Viol\"] > 0)))\n",
    "df.to_csv(\"result/ln_pr_200-200.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66d0eb0b-4b0d-44d3-8ca0-df0ac89d3804",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
