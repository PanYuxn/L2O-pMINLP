{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "af81a15b-86e0-4efb-bb9e-aa728a05db62",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch import nn\n",
    "from tqdm import tqdm\n",
    "from run import utils"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e765d0b-2e5e-48c9-af3c-48965a131701",
   "metadata": {},
   "source": [
    "## 100x100 Convex Quadratic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bbb105ee-7af0-4f95-bbe7-05d9cd280536",
   "metadata": {},
   "outputs": [],
   "source": [
    "# random seed\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "torch.cuda.manual_seed(42)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a1e6735d-545a-4550-9810-6b3c43969148",
   "metadata": {},
   "outputs": [],
   "source": [
    "# init\n",
    "num_var = 100     # number of variables\n",
    "num_ineq = 100    # number of constraints\n",
    "num_data = 10000  # number of data\n",
    "test_size = 1000  # number of test size\n",
    "val_size = 1000   # number of validation size\n",
    "train_size = num_data - test_size - val_size\n",
    "\n",
    "\n",
    "# data sample from uniform distribution\n",
    "b_samples = torch.from_numpy(np.random.uniform(-1, 1, size=(num_data, num_ineq))).float()\n",
    "data = {\"b\":b_samples}\n",
    "# data split\n",
    "from src.utlis import data_split\n",
    "data_train, data_test, data_dev = data_split(data, test_size=test_size, val_size=val_size)\n",
    "\n",
    "# torch dataloaders\n",
    "from torch.utils.data import DataLoader\n",
    "batch_size = 64\n",
    "loader_train = DataLoader(data_train, batch_size, num_workers=0, collate_fn=data_train.collate_fn, shuffle=True)\n",
    "loader_test = DataLoader(data_test, batch_size, num_workers=0, collate_fn=data_test.collate_fn, shuffle=False)\n",
    "loader_dev = DataLoader(data_dev, batch_size, num_workers=0, collate_fn=data_dev.collate_fn, shuffle=False)\n",
    "\n",
    "# init model\n",
    "from src.problem import msQuadratic\n",
    "model = msQuadratic(num_var, num_ineq, timelimit=600)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "796834c9-e752-4606-aea9-82531401b9f6",
   "metadata": {},
   "source": [
    "### Rounding Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fa2fce8d-e7db-4203-946c-83aa528f4852",
   "metadata": {},
   "outputs": [],
   "source": [
    "# random seed\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "torch.cuda.manual_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6b063dbb-1526-43d6-bfa5-cbceff992558",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Iters 0, Validation Loss: 3603.61\n",
      "Epoch 0, Iters 125, Training Loss: 251.62, Validation Loss: 4.14\n",
      "Epoch 1, Iters 250, Training Loss: 0.69, Validation Loss: -2.77\n",
      "Epoch 2, Iters 375, Training Loss: -3.31, Validation Loss: -5.74\n",
      "Epoch 3, Iters 500, Training Loss: -5.01, Validation Loss: -7.80\n",
      "Epoch 4, Iters 625, Training Loss: -5.68, Validation Loss: -6.14\n",
      "Epoch 5, Iters 750, Training Loss: -6.36, Validation Loss: -8.47\n",
      "Epoch 6, Iters 875, Training Loss: -7.01, Validation Loss: -8.45\n",
      "Epoch 7, Iters 1000, Training Loss: -7.43, Validation Loss: -8.88\n",
      "Epoch 8, Iters 1125, Training Loss: -7.96, Validation Loss: -8.79\n",
      "Epoch 9, Iters 1250, Training Loss: -8.23, Validation Loss: -9.00\n",
      "Epoch 10, Iters 1375, Training Loss: -8.38, Validation Loss: -8.97\n",
      "Epoch 11, Iters 1500, Training Loss: -8.79, Validation Loss: -9.72\n",
      "Epoch 12, Iters 1625, Training Loss: -8.99, Validation Loss: -9.65\n",
      "Epoch 13, Iters 1750, Training Loss: -9.37, Validation Loss: -10.57\n",
      "Epoch 14, Iters 1875, Training Loss: -9.60, Validation Loss: -9.88\n",
      "Epoch 15, Iters 2000, Training Loss: -9.51, Validation Loss: -10.10\n",
      "Epoch 16, Iters 2125, Training Loss: -9.80, Validation Loss: -10.83\n",
      "Epoch 17, Iters 2250, Training Loss: -9.83, Validation Loss: -10.12\n",
      "Epoch 18, Iters 2375, Training Loss: -9.95, Validation Loss: -9.80\n",
      "Epoch 19, Iters 2500, Training Loss: -10.11, Validation Loss: -9.99\n",
      "Epoch 20, Iters 2625, Training Loss: -10.03, Validation Loss: -11.34\n",
      "Epoch 21, Iters 2750, Training Loss: -10.61, Validation Loss: -11.28\n",
      "Epoch 22, Iters 2875, Training Loss: -10.76, Validation Loss: -9.82\n",
      "Epoch 23, Iters 3000, Training Loss: -10.89, Validation Loss: -11.55\n",
      "Epoch 24, Iters 3125, Training Loss: -10.51, Validation Loss: -10.97\n",
      "Epoch 25, Iters 3250, Training Loss: -10.62, Validation Loss: -11.75\n",
      "Epoch 26, Iters 3375, Training Loss: -10.94, Validation Loss: -11.80\n",
      "Epoch 27, Iters 3500, Training Loss: -11.13, Validation Loss: -11.58\n",
      "Epoch 28, Iters 3625, Training Loss: -11.52, Validation Loss: -11.23\n",
      "Epoch 29, Iters 3750, Training Loss: -11.13, Validation Loss: -12.44\n",
      "Epoch 30, Iters 3875, Training Loss: -11.67, Validation Loss: -12.39\n",
      "Epoch 31, Iters 4000, Training Loss: -11.59, Validation Loss: -12.36\n",
      "Epoch 32, Iters 4125, Training Loss: -11.85, Validation Loss: -11.98\n",
      "Epoch 33, Iters 4250, Training Loss: -11.39, Validation Loss: -10.94\n",
      "Epoch 34, Iters 4375, Training Loss: -11.03, Validation Loss: -10.53\n",
      "Epoch 35, Iters 4500, Training Loss: -11.46, Validation Loss: -12.06\n",
      "Epoch 36, Iters 4625, Training Loss: -11.72, Validation Loss: -11.21\n",
      "Epoch 37, Iters 4750, Training Loss: -11.97, Validation Loss: -12.71\n",
      "Epoch 38, Iters 4875, Training Loss: -11.53, Validation Loss: -12.47\n",
      "Epoch 39, Iters 5000, Training Loss: -11.38, Validation Loss: -11.29\n",
      "Epoch 40, Iters 5125, Training Loss: -11.78, Validation Loss: -11.66\n",
      "Epoch 41, Iters 5250, Training Loss: -11.60, Validation Loss: -12.22\n",
      "Epoch 42, Iters 5375, Training Loss: -11.63, Validation Loss: -10.96\n",
      "Epoch 43, Iters 5500, Training Loss: -11.05, Validation Loss: -9.96\n",
      "Epoch 44, Iters 5625, Training Loss: -11.04, Validation Loss: -12.35\n",
      "Epoch 45, Iters 5750, Training Loss: -11.48, Validation Loss: -11.95\n",
      "Epoch 46, Iters 5875, Training Loss: -11.48, Validation Loss: -11.17\n",
      "Epoch 47, Iters 6000, Training Loss: -12.06, Validation Loss: -9.54\n",
      "Epoch 48, Iters 6125, Training Loss: -11.45, Validation Loss: -10.30\n",
      "Epoch 49, Iters 6250, Training Loss: -12.01, Validation Loss: -12.03\n",
      "Epoch 50, Iters 6375, Training Loss: -10.96, Validation Loss: -10.36\n",
      "Epoch 51, Iters 6500, Training Loss: -11.42, Validation Loss: -12.70\n",
      "Epoch 52, Iters 6625, Training Loss: -10.94, Validation Loss: -12.28\n",
      "Epoch 53, Iters 6750, Training Loss: -11.09, Validation Loss: -12.49\n",
      "Epoch 54, Iters 6875, Training Loss: -11.07, Validation Loss: -12.92\n",
      "Epoch 55, Iters 7000, Training Loss: -11.48, Validation Loss: -10.53\n",
      "Epoch 56, Iters 7125, Training Loss: -10.47, Validation Loss: -11.70\n",
      "Epoch 57, Iters 7250, Training Loss: -11.70, Validation Loss: 24.96\n",
      "Epoch 58, Iters 7375, Training Loss: -12.13, Validation Loss: -12.19\n",
      "Epoch 59, Iters 7500, Training Loss: -11.80, Validation Loss: -12.95\n",
      "Epoch 60, Iters 7625, Training Loss: -11.77, Validation Loss: -12.27\n",
      "Epoch 61, Iters 7750, Training Loss: -11.64, Validation Loss: -12.88\n",
      "Epoch 62, Iters 7875, Training Loss: -11.85, Validation Loss: -12.32\n",
      "Epoch 63, Iters 8000, Training Loss: -11.51, Validation Loss: -12.65\n",
      "Epoch 64, Iters 8125, Training Loss: -11.41, Validation Loss: -11.82\n",
      "Epoch 65, Iters 8250, Training Loss: -11.30, Validation Loss: -11.96\n",
      "Epoch 66, Iters 8375, Training Loss: -11.49, Validation Loss: -12.47\n",
      "Epoch 67, Iters 8500, Training Loss: -11.28, Validation Loss: -12.00\n",
      "Epoch 68, Iters 8625, Training Loss: -12.04, Validation Loss: -12.91\n",
      "Epoch 69, Iters 8750, Training Loss: -11.36, Validation Loss: -9.93\n",
      "Epoch 70, Iters 8875, Training Loss: -11.95, Validation Loss: -11.47\n",
      "Epoch 71, Iters 9000, Training Loss: -11.98, Validation Loss: -12.57\n",
      "Epoch 72, Iters 9125, Training Loss: -12.49, Validation Loss: -13.49\n",
      "Epoch 73, Iters 9250, Training Loss: -13.03, Validation Loss: -12.93\n",
      "Epoch 74, Iters 9375, Training Loss: -12.45, Validation Loss: -13.47\n",
      "Epoch 75, Iters 9500, Training Loss: -12.23, Validation Loss: -13.02\n",
      "Epoch 76, Iters 9625, Training Loss: -11.53, Validation Loss: -12.58\n",
      "Epoch 77, Iters 9750, Training Loss: -11.85, Validation Loss: -12.76\n",
      "Epoch 78, Iters 9875, Training Loss: -12.33, Validation Loss: -12.82\n",
      "Epoch 79, Iters 10000, Training Loss: -12.44, Validation Loss: -11.74\n",
      "Epoch 80, Iters 10125, Training Loss: -12.44, Validation Loss: -12.28\n",
      "Epoch 81, Iters 10250, Training Loss: -11.97, Validation Loss: -12.79\n",
      "Epoch 82, Iters 10375, Training Loss: -12.64, Validation Loss: -12.93\n",
      "Epoch 83, Iters 10500, Training Loss: -12.14, Validation Loss: -10.13\n",
      "Epoch 84, Iters 10625, Training Loss: -11.66, Validation Loss: -11.61\n",
      "Epoch 85, Iters 10750, Training Loss: -12.49, Validation Loss: -12.82\n",
      "Epoch 86, Iters 10875, Training Loss: -12.39, Validation Loss: -10.94\n",
      "Epoch 87, Iters 11000, Training Loss: -12.38, Validation Loss: -11.46\n",
      "Epoch 88, Iters 11125, Training Loss: -12.21, Validation Loss: -12.18\n",
      "Epoch 89, Iters 11250, Training Loss: -12.59, Validation Loss: -12.18\n",
      "Epoch 90, Iters 11375, Training Loss: -12.60, Validation Loss: -2.71\n",
      "Epoch 91, Iters 11500, Training Loss: -11.92, Validation Loss: -12.19\n",
      "Epoch 92, Iters 11625, Training Loss: -12.39, Validation Loss: -11.15\n",
      "Early stopping at iters 11625\n",
      "Best model loaded.\n",
      "Training complete.\n",
      "The training time is 140.35 sec.\n"
     ]
    }
   ],
   "source": [
    "# hyperparameters\n",
    "penalty_weight = 100  # weight of constraint violation penealty\n",
    "hlayers_sol = 5       # number of hidden layers for solution mapping\n",
    "hlayers_rnd = 4       # number of hidden layers for solution mapping\n",
    "hsize = 256           # width of hidden layers for solution mapping\n",
    "lr = 1e-3             # learning rate\n",
    "\n",
    "# set problem\n",
    "import neuromancer as nm\n",
    "from src.problem import nmQuadratic\n",
    "from src.func.layer import netFC\n",
    "from src.func import roundGumbelModel\n",
    "# build neural architecture for the solution map\n",
    "func = nm.modules.blocks.MLP(insize=num_ineq, outsize=num_var, bias=True,\n",
    "                             linear_map=nm.slim.maps[\"linear\"],\n",
    "                             nonlin=nn.ReLU, hsizes=[hsize]*hlayers_sol)\n",
    "smap = nm.system.Node(func, [\"b\"], [\"x\"], name=\"smap\")\n",
    "# define rounding model\n",
    "layers_rnd = netFC(input_dim=num_ineq+num_var, hidden_dims=[hsize]*hlayers_rnd, output_dim=num_var)\n",
    "rnd = roundGumbelModel(layers=layers_rnd, param_keys=[\"b\"], var_keys=[\"x\"],  output_keys=[\"x_rnd\"], \n",
    "                       int_ind={\"x\":range(num_var)}, continuous_update=True, name=\"round\")\n",
    "# build neuromancer problem for rounding\n",
    "components = nn.ModuleList([smap, rnd]).to(\"cuda\")\n",
    "loss_fn = nmQuadratic([\"b\", \"x_rnd\"], num_var, num_ineq, penalty_weight)\n",
    "\n",
    "from src.problem.neuromancer.trainer import trainer\n",
    "# training\n",
    "epochs = 200                    # number of training epochs\n",
    "warmup = 20                     # number of epochs to wait before enacting early stopping policy\n",
    "patience = 20                   # number of epochs with no improvement in eval metric to allow before early stopping\n",
    "optimizer = torch.optim.AdamW(components.parameters(), lr=lr)\n",
    "# create a trainer for the problem\n",
    "my_trainer = trainer(components, loss_fn, optimizer, epochs=epochs, patience=patience, warmup=warmup, device=\"cuda\")\n",
    "# training for the rounding problem\n",
    "my_trainer.train(loader_train, loader_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "63f7c86a-efaf-49aa-bf38-b63ccdd125e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Obj: -13.5470 with 0.0000 violation in 0.0037 sec.\n"
     ]
    }
   ],
   "source": [
    "# get b\n",
    "b = data_test.datadict[\"b\"][0]\n",
    "# data point as tensor\n",
    "datapoints = {\"b\": torch.unsqueeze(b, 0).to(\"cuda\"), \"name\": \"test\"}\n",
    "# infer\n",
    "components.eval()\n",
    "tick = time.time()\n",
    "with torch.no_grad():\n",
    "    for comp in components:\n",
    "        datapoints.update(comp(datapoints))\n",
    "tock = time.time()\n",
    "elapsed = tock - tick\n",
    "# assign params\n",
    "model.set_param_val({\"b\":b.cpu().numpy()})\n",
    "# assign vars\n",
    "x = datapoints[\"x_rnd\"]\n",
    "for i in range(num_var):\n",
    "    model.vars[\"x\"][i].value = x[0,i].item()\n",
    "# get solutions\n",
    "xval, objval = model.get_val()\n",
    "viol = np.mean(model.cal_violation())\n",
    "print(f\"Obj: {objval:.4f} with {viol:.4f} violation in {elapsed:.4f} sec.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1921df2-1c7a-4065-a7ba-9ba1787fce8e",
   "metadata": {},
   "source": [
    "### Learnable Threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "66c7c399-8db6-400c-b7f9-880cc7edf85a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# random seed\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "torch.cuda.manual_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cd35ed19-57fb-4d85-9770-d4d52f99e0f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Iters 0, Validation Loss: 2485.01\n",
      "Epoch 0, Iters 125, Training Loss: 251.90, Validation Loss: 3.37\n",
      "Epoch 1, Iters 250, Training Loss: -4.55, Validation Loss: -6.44\n",
      "Epoch 2, Iters 375, Training Loss: -7.11, Validation Loss: -8.40\n",
      "Epoch 3, Iters 500, Training Loss: -7.86, Validation Loss: -9.23\n",
      "Epoch 4, Iters 625, Training Loss: -8.52, Validation Loss: -9.12\n",
      "Epoch 5, Iters 750, Training Loss: -9.32, Validation Loss: -10.19\n",
      "Epoch 6, Iters 875, Training Loss: -9.05, Validation Loss: -9.11\n",
      "Epoch 7, Iters 1000, Training Loss: -9.40, Validation Loss: -9.75\n",
      "Epoch 8, Iters 1125, Training Loss: -9.41, Validation Loss: -8.82\n",
      "Epoch 9, Iters 1250, Training Loss: -9.85, Validation Loss: -10.60\n",
      "Epoch 10, Iters 1375, Training Loss: -10.27, Validation Loss: -11.07\n",
      "Epoch 11, Iters 1500, Training Loss: -10.46, Validation Loss: -11.12\n",
      "Epoch 12, Iters 1625, Training Loss: -10.23, Validation Loss: -11.44\n",
      "Epoch 13, Iters 1750, Training Loss: -10.50, Validation Loss: -9.58\n",
      "Epoch 14, Iters 1875, Training Loss: -10.49, Validation Loss: -11.65\n",
      "Epoch 15, Iters 2000, Training Loss: -10.85, Validation Loss: -11.15\n",
      "Epoch 16, Iters 2125, Training Loss: -11.09, Validation Loss: -8.22\n",
      "Epoch 17, Iters 2250, Training Loss: -10.86, Validation Loss: -8.99\n",
      "Epoch 18, Iters 2375, Training Loss: -10.85, Validation Loss: -11.03\n",
      "Epoch 19, Iters 2500, Training Loss: -11.09, Validation Loss: -11.27\n",
      "Epoch 20, Iters 2625, Training Loss: -11.25, Validation Loss: -10.81\n",
      "Epoch 21, Iters 2750, Training Loss: -11.11, Validation Loss: -10.77\n",
      "Epoch 22, Iters 2875, Training Loss: -11.42, Validation Loss: -11.66\n",
      "Epoch 23, Iters 3000, Training Loss: -11.25, Validation Loss: -11.35\n",
      "Epoch 24, Iters 3125, Training Loss: -11.33, Validation Loss: -11.87\n",
      "Epoch 25, Iters 3250, Training Loss: -11.60, Validation Loss: -11.33\n",
      "Epoch 26, Iters 3375, Training Loss: -11.70, Validation Loss: -12.19\n",
      "Epoch 27, Iters 3500, Training Loss: -11.94, Validation Loss: -10.84\n",
      "Epoch 28, Iters 3625, Training Loss: -11.72, Validation Loss: -11.80\n",
      "Epoch 29, Iters 3750, Training Loss: -11.64, Validation Loss: -11.16\n",
      "Epoch 30, Iters 3875, Training Loss: -11.89, Validation Loss: -9.79\n",
      "Epoch 31, Iters 4000, Training Loss: -11.47, Validation Loss: -11.84\n",
      "Epoch 32, Iters 4125, Training Loss: -11.91, Validation Loss: -12.44\n",
      "Epoch 33, Iters 4250, Training Loss: -11.88, Validation Loss: -12.96\n",
      "Epoch 34, Iters 4375, Training Loss: -11.98, Validation Loss: -12.06\n",
      "Epoch 35, Iters 4500, Training Loss: -12.14, Validation Loss: -13.03\n",
      "Epoch 36, Iters 4625, Training Loss: -12.02, Validation Loss: -12.30\n",
      "Epoch 37, Iters 4750, Training Loss: -11.99, Validation Loss: -12.71\n",
      "Epoch 38, Iters 4875, Training Loss: -11.90, Validation Loss: -12.08\n",
      "Epoch 39, Iters 5000, Training Loss: -11.90, Validation Loss: -12.37\n",
      "Epoch 40, Iters 5125, Training Loss: -11.95, Validation Loss: -12.18\n",
      "Epoch 41, Iters 5250, Training Loss: -12.21, Validation Loss: -12.16\n",
      "Epoch 42, Iters 5375, Training Loss: -11.91, Validation Loss: -12.87\n",
      "Epoch 43, Iters 5500, Training Loss: -12.19, Validation Loss: -12.81\n",
      "Epoch 44, Iters 5625, Training Loss: -12.42, Validation Loss: -12.96\n",
      "Epoch 45, Iters 5750, Training Loss: -12.25, Validation Loss: -10.94\n",
      "Epoch 46, Iters 5875, Training Loss: -12.11, Validation Loss: -12.40\n",
      "Epoch 47, Iters 6000, Training Loss: -12.08, Validation Loss: -12.19\n",
      "Epoch 48, Iters 6125, Training Loss: -12.32, Validation Loss: -13.16\n",
      "Epoch 49, Iters 6250, Training Loss: -12.25, Validation Loss: -12.75\n",
      "Epoch 50, Iters 6375, Training Loss: -12.23, Validation Loss: -12.54\n",
      "Epoch 51, Iters 6500, Training Loss: -12.14, Validation Loss: -12.28\n",
      "Epoch 52, Iters 6625, Training Loss: -11.93, Validation Loss: -12.27\n",
      "Epoch 53, Iters 6750, Training Loss: -12.29, Validation Loss: -12.82\n",
      "Epoch 54, Iters 6875, Training Loss: -12.67, Validation Loss: -12.37\n",
      "Epoch 55, Iters 7000, Training Loss: -12.30, Validation Loss: -12.23\n",
      "Epoch 56, Iters 7125, Training Loss: -12.25, Validation Loss: -11.97\n",
      "Epoch 57, Iters 7250, Training Loss: -12.41, Validation Loss: -12.19\n",
      "Epoch 58, Iters 7375, Training Loss: -12.05, Validation Loss: -11.89\n",
      "Epoch 59, Iters 7500, Training Loss: -12.64, Validation Loss: -12.28\n",
      "Epoch 60, Iters 7625, Training Loss: -12.43, Validation Loss: -12.73\n",
      "Epoch 61, Iters 7750, Training Loss: -12.03, Validation Loss: -12.58\n",
      "Epoch 62, Iters 7875, Training Loss: -12.66, Validation Loss: -12.24\n",
      "Epoch 63, Iters 8000, Training Loss: -12.55, Validation Loss: -13.46\n",
      "Epoch 64, Iters 8125, Training Loss: -12.43, Validation Loss: -10.59\n",
      "Epoch 65, Iters 8250, Training Loss: -12.46, Validation Loss: -12.90\n",
      "Epoch 66, Iters 8375, Training Loss: -12.24, Validation Loss: -12.34\n",
      "Epoch 67, Iters 8500, Training Loss: -12.69, Validation Loss: -12.67\n",
      "Epoch 68, Iters 8625, Training Loss: -12.49, Validation Loss: -12.87\n",
      "Epoch 69, Iters 8750, Training Loss: -12.60, Validation Loss: -11.48\n",
      "Epoch 70, Iters 8875, Training Loss: -12.39, Validation Loss: -12.55\n",
      "Epoch 71, Iters 9000, Training Loss: -12.78, Validation Loss: -12.11\n",
      "Epoch 72, Iters 9125, Training Loss: -12.31, Validation Loss: -11.57\n",
      "Epoch 73, Iters 9250, Training Loss: -12.26, Validation Loss: -12.79\n",
      "Epoch 74, Iters 9375, Training Loss: -12.35, Validation Loss: -12.62\n",
      "Epoch 75, Iters 9500, Training Loss: -12.81, Validation Loss: -12.63\n",
      "Epoch 76, Iters 9625, Training Loss: -12.79, Validation Loss: -12.77\n",
      "Epoch 77, Iters 9750, Training Loss: -12.80, Validation Loss: -12.78\n",
      "Epoch 78, Iters 9875, Training Loss: -12.88, Validation Loss: -12.98\n",
      "Epoch 79, Iters 10000, Training Loss: -12.77, Validation Loss: -12.91\n",
      "Epoch 80, Iters 10125, Training Loss: -12.88, Validation Loss: -12.14\n",
      "Epoch 81, Iters 10250, Training Loss: -12.88, Validation Loss: -13.17\n",
      "Epoch 82, Iters 10375, Training Loss: -12.70, Validation Loss: -12.86\n",
      "Epoch 83, Iters 10500, Training Loss: -12.50, Validation Loss: -10.34\n",
      "Early stopping at iters 10500\n",
      "Best model loaded.\n",
      "Training complete.\n",
      "The training time is 124.83 sec.\n"
     ]
    }
   ],
   "source": [
    "# hyperparameters\n",
    "penalty_weight = 100  # weight of constraint violation penealty\n",
    "hlayers_sol = 5       # number of hidden layers for solution mapping\n",
    "hlayers_rnd = 4       # number of hidden layers for solution mapping\n",
    "hsize = 256           # width of hidden layers for solution mapping\n",
    "lr = 1e-3             # learning rate\n",
    "\n",
    "# set problem\n",
    "import neuromancer as nm\n",
    "from src.problem import nmQuadratic\n",
    "from src.func.layer import netFC\n",
    "from src.func import roundThresholdModel\n",
    "# build neural architecture for the solution map\n",
    "func = nm.modules.blocks.MLP(insize=num_ineq, outsize=num_var, bias=True,\n",
    "                             linear_map=nm.slim.maps[\"linear\"],\n",
    "                             nonlin=nn.ReLU, hsizes=[hsize]*hlayers_sol)\n",
    "smap = nm.system.Node(func, [\"b\"], [\"x\"], name=\"smap\")\n",
    "# define rounding model\n",
    "layers_rnd = netFC(input_dim=num_ineq+num_var, hidden_dims=[hsize]*hlayers_rnd, output_dim=num_var)\n",
    "rnd = roundThresholdModel(layers=layers_rnd, param_keys=[\"b\"], var_keys=[\"x\"],  output_keys=[\"x_rnd\"], \n",
    "                       int_ind=model.int_ind, continuous_update=True, name=\"round\")\n",
    "# build neuromancer problem for rounding\n",
    "components = nn.ModuleList([smap, rnd]).to(\"cuda\")\n",
    "loss_fn = nmQuadratic([\"b\", \"x_rnd\"], num_var, num_ineq, penalty_weight)\n",
    "\n",
    "from src.problem.neuromancer.trainer import trainer\n",
    "# training\n",
    "epochs = 200                    # number of training epochs\n",
    "warmup = 20                     # number of epochs to wait before enacting early stopping policy\n",
    "patience = 20                   # number of epochs with no improvement in eval metric to allow before early stopping\n",
    "optimizer = torch.optim.AdamW(components.parameters(), lr=lr)\n",
    "# create a trainer for the problem\n",
    "my_trainer = trainer(components, loss_fn, optimizer, epochs=epochs, patience=patience, warmup=warmup, device=\"cuda\")\n",
    "# training for the rounding problem\n",
    "my_trainer.train(loader_train, loader_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8c891f4a-61b3-4955-9377-62cd42127ced",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Obj: -13.6711 with 0.0000 violation in 0.0040 sec.\n"
     ]
    }
   ],
   "source": [
    "# get b\n",
    "b = data_test.datadict[\"b\"][0]\n",
    "# data point as tensor\n",
    "datapoints = {\"b\": torch.unsqueeze(b, 0).to(\"cuda\"), \"name\": \"test\"}\n",
    "# infer\n",
    "components.eval()\n",
    "tick = time.time()\n",
    "with torch.no_grad():\n",
    "    for comp in components:\n",
    "        datapoints.update(comp(datapoints))\n",
    "tock = time.time()\n",
    "elapsed = tock - tick\n",
    "# assign params\n",
    "model.set_param_val({\"b\":b.cpu().numpy()})\n",
    "# assign vars\n",
    "x = datapoints[\"x_rnd\"]\n",
    "for i in range(num_var):\n",
    "    model.vars[\"x\"][i].value = x[0,i].item()\n",
    "# get solutions\n",
    "xval, objval = model.get_val()\n",
    "viol = np.mean(model.cal_violation())\n",
    "print(f\"Obj: {objval:.4f} with {viol:.4f} violation in {elapsed:.4f} sec.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e1bec0c-a3cf-488d-a63d-b797daced307",
   "metadata": {},
   "source": [
    "### Exact Solver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "95735703-14bf-430c-8976-56cfea0a7268",
   "metadata": {},
   "outputs": [],
   "source": [
    "# random seed\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "torch.cuda.manual_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "25196e08-bf91-426b-a256-e37fde21da34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Solver script file: 'C:\\Users\\lucas\\AppData\\Local\\Temp\\tmpngu7okky.gurobi.script'\n",
      "Solver log file: 'C:\\Users\\lucas\\AppData\\Local\\Temp\\tmp2uxkdxe6.gurobi.log'\n",
      "Solver solution file: 'C:\\Users\\lucas\\AppData\\Local\\Temp\\tmpcwvkhpgt.gurobi.txt'\n",
      "Solver problem files: ('C:\\\\Users\\\\lucas\\\\AppData\\\\Local\\\\Temp\\\\tmpn3su24zw.pyomo.lp',)\n",
      "Set parameter Username\n",
      "Academic license - for non-commercial use only - expires 2024-12-25\n",
      "Read LP format model from file C:\\Users\\lucas\\AppData\\Local\\Temp\\tmpn3su24zw.pyomo.lp\n",
      "Reading time = 0.01 seconds\n",
      "x1: 100 rows, 100 columns, 10000 nonzeros\n",
      "Set parameter TimeLimit to value 600\n",
      "Gurobi Optimizer version 11.0.0 build v11.0.0rc2 (win64 - Windows 11+.0 (22631.2))\n",
      "\n",
      "CPU model: AMD Ryzen 7 5800HS with Radeon Graphics, instruction set [SSE2|AVX|AVX2]\n",
      "Thread count: 8 physical cores, 16 logical processors, using up to 16 threads\n",
      "\n",
      "Optimize a model with 100 rows, 100 columns and 10000 nonzeros\n",
      "Model fingerprint: 0x022e99b5\n",
      "Model has 100 quadratic objective terms\n",
      "Variable types: 0 continuous, 100 integer (0 binary)\n",
      "Coefficient statistics:\n",
      "  Matrix range     [7e-07, 4e-01]\n",
      "  Objective range  [1e-03, 1e-01]\n",
      "  QObjective range [2e-04, 1e-02]\n",
      "  Bounds range     [0e+00, 0e+00]\n",
      "  RHS range        [4e-03, 1e+00]\n",
      "Presolve time: 0.00s\n",
      "Presolved: 100 rows, 100 columns, 10000 nonzeros\n",
      "Presolved model has 100 quadratic objective terms\n",
      "Variable types: 0 continuous, 100 integer (0 binary)\n",
      "\n",
      "Root relaxation: objective -2.230563e+01, 55 iterations, 0.00 seconds (0.01 work units)\n",
      "\n",
      "    Nodes    |    Current Node    |     Objective Bounds      |     Work\n",
      " Expl Unexpl |  Obj  Depth IntInf | Incumbent    BestBd   Gap | It/Node Time\n",
      "\n",
      "     0     0  -22.30563    0  100          -  -22.30563      -     -    0s\n",
      "     0     0  -22.30398    0  100          -  -22.30398      -     -    0s\n",
      "     0     2  -22.30398    0  100          -  -22.30398      -     -    0s\n",
      " 25454 22437  -20.60609  174   17          -  -22.29853      -  11.0    5s\n",
      " 55296 50137  -22.08610  102   41          -  -22.29693      -  12.8   10s\n",
      " 86999 79845  -22.25337   47   69          -  -22.29612      -  13.5   15s\n",
      " 120344 110857  -22.12674   89   49          -  -22.29570      -  14.0   20s\n",
      " 146774 135259  -22.12714   87   50          -  -22.29542      -  14.3   25s\n",
      " 175547 161688  -22.04601   92   49          -  -22.29515      -  14.5   30s\n",
      " 201776 186671  -21.84539  126   34          -  -22.29489      -  14.7   35s\n",
      " 231066 213312  -21.93044  117   35          -  -22.29468      -  14.9   40s\n",
      " 258013 237736  -21.62892  135   30          -  -22.29447      -  15.0   45s\n",
      "H279701 256649                    9.210528e+17  -22.29431   100%  15.1   49s\n",
      " 280335 256665  -22.06135   99   47 9.2105e+17  -22.29431   100%  15.1   50s\n",
      " 294600 271461  -21.14927  163   32 9.2105e+17  -22.29406   100%  14.9   55s\n",
      " 312766 288300  -22.28424   26   85 9.2105e+17  -22.29390   100%  14.6   60s\n",
      " 332848 307143  -22.27931   42   74 9.2105e+17  -22.29370   100%  14.4   65s\n",
      "H343320 314816                     -19.6837120  -22.29369  13.3%  14.3   68s\n",
      "H345968 312458                     -20.1223765  -22.29369  10.8%  14.3   71s\n",
      "H345973 310149                     -20.3540947  -22.29369  9.53%  14.3   71s\n",
      "H348389 316341                     -20.7598846  -22.29361  7.39%  14.3   73s\n",
      " 362000 315678  -21.70887  136   33  -20.75988  -22.29349  7.39%  14.2   76s\n",
      "H362015 314953                     -20.7815701  -22.29349  7.28%  14.2   76s\n",
      " 366952 321616  -21.52225  150   32  -20.78157  -22.29348  7.28%  14.1   80s\n",
      " 382675 336056  -22.29242   25   86  -20.78157  -22.29329  7.27%  14.0   85s\n",
      "H399595 343945                     -20.9265157  -22.29324  6.53%  13.9   87s\n",
      "H401238 336570                     -21.0435634  -22.29323  5.94%  13.9   88s\n",
      " 410931 348303  -21.92167  113   36  -21.04356  -22.29317  5.94%  13.8   90s\n",
      " 428582 363575  -22.17375   75   55  -21.04356  -22.29305  5.94%  13.7   95s\n",
      " 462764 394795  -21.67745  135   27  -21.04356  -22.29285  5.94%  13.5  100s\n",
      " 498077 427537  -21.88672  112   40  -21.04356  -22.29263  5.94%  13.4  105s\n",
      " 533381 459090  -21.90574  125   27  -21.04356  -22.29250  5.94%  13.3  110s\n",
      " 568916 491527  -22.25280   49   67  -21.04356  -22.29232  5.93%  13.2  115s\n",
      " 603411 522710  -22.15416   90   49  -21.04356  -22.29219  5.93%  13.1  120s\n",
      " 636489 553495  -22.23366   63   58  -21.04356  -22.29205  5.93%  13.1  125s\n",
      " 671319 585295  -21.86886  101   48  -21.04356  -22.29193  5.93%  13.0  130s\n",
      " 706367 617464  -22.20970   67   60  -21.04356  -22.29178  5.93%  13.0  135s\n",
      " 740562 648447  -22.02983   96   49  -21.04356  -22.29165  5.93%  12.9  140s\n",
      " 772551 677242  -22.27807   37   75  -21.04356  -22.29155  5.93%  12.9  145s\n",
      " 802267 703791  -22.17174   88   50  -21.04356  -22.29145  5.93%  12.9  150s\n",
      " 829074 729346  -21.06171  159   31  -21.04356  -22.29138  5.93%  12.8  155s\n",
      " 857892 755024  -22.22640   62   58  -21.04356  -22.29126  5.93%  12.8  160s\n",
      " 891532 786371  -21.69495  125   32  -21.04356  -22.29115  5.93%  12.8  165s\n",
      " 917681 810502  -22.07777   88   50  -21.04356  -22.29109  5.93%  12.8  170s\n",
      " 950092 839114  -22.27060   37   77  -21.04356  -22.29097  5.93%  12.8  175s\n",
      " 981697 868272  -22.20233   60   60  -21.04356  -22.29090  5.93%  12.8  180s\n",
      " 1013268 897086  -22.22413   57   62  -21.04356  -22.29081  5.93%  12.7  185s\n",
      " 1048641 928613  -21.73533  134   31  -21.04356  -22.29072  5.93%  12.7  190s\n",
      " 1080997 957883  -21.17724  145   33  -21.04356  -22.29065  5.93%  12.7  195s\n",
      " 1112154 984190  -21.89323  120   45  -21.04356  -22.29058  5.93%  12.7  201s\n",
      " 1132208 1004649  -22.14135   77   55  -21.04356  -22.29054  5.93%  12.6  205s\n",
      " 1164813 1034942  -21.72783  132   34  -21.04356  -22.29048  5.93%  12.6  210s\n",
      " 1194634 1061428  -22.20290   67   56  -21.04356  -22.29041  5.93%  12.6  215s\n",
      " 1226541 1088103  -22.05955  109   45  -21.04356  -22.29033  5.92%  12.6  220s\n",
      " 1250258 1112118  -21.98008  115   40  -21.04356  -22.29028  5.92%  12.6  225s\n",
      " 1283327 1142006  -21.81963  123   28  -21.04356  -22.29021  5.92%  12.6  230s\n",
      " 1317435 1172800  -21.70101  134   29  -21.04356  -22.29014  5.92%  12.6  235s\n",
      " 1341825 1193000  -22.04258  102   42  -21.04356  -22.29011  5.92%  12.6  240s\n",
      " 1368254 1219415  -22.20611   72   56  -21.04356  -22.29005  5.92%  12.6  245s\n",
      " 1399668 1247312  -21.56917  137   31  -21.04356  -22.28999  5.92%  12.6  250s\n",
      " 1433686 1278238     cutoff  154       -21.04356  -22.28992  5.92%  12.6  255s\n",
      " 1465528 1307285  -22.23681   51   67  -21.04356  -22.28986  5.92%  12.6  260s\n",
      " 1483861 1324652  -22.27610   37   78  -21.04356  -22.28983  5.92%  12.6  265s\n",
      " 1512085 1349670  -22.11867   89   55  -21.04356  -22.28978  5.92%  12.6  270s\n",
      " 1542298 1376962  -21.77734  120   34  -21.04356  -22.28973  5.92%  12.6  275s\n",
      " 1569117 1401090  -22.23914   57   67  -21.04356  -22.28970  5.92%  12.6  280s\n",
      " 1590258 1418581  -22.26020   46   73  -21.04356  -22.28966  5.92%  12.6  285s\n",
      " 1611740 1440388  -22.25070   54   65  -21.04356  -22.28964  5.92%  12.6  290s\n",
      " 1637349 1464132  -21.83227  118   37  -21.04356  -22.28960  5.92%  12.6  295s\n",
      " 1668859 1492175  -21.67509  123   33  -21.04356  -22.28955  5.92%  12.6  300s\n",
      " 1700250 1520252  -22.19523   69   57  -21.04356  -22.28949  5.92%  12.6  305s\n",
      " 1718654 1537015  -22.28653   27   84  -21.04356  -22.28947  5.92%  12.6  310s\n",
      " 1747786 1563447  -22.17797   69   57  -21.04356  -22.28944  5.92%  12.6  315s\n",
      " 1777161 1590870  -21.78466  113   46  -21.04356  -22.28941  5.92%  12.6  320s\n",
      " 1807026 1617609  -22.18604   76   52  -21.04356  -22.28936  5.92%  12.6  325s\n",
      " 1839307 1646916  -21.82087  124   31  -21.04356  -22.28932  5.92%  12.6  330s\n",
      " 1857779 1663786  -22.02120   97   51  -21.04356  -22.28929  5.92%  12.6  335s\n",
      " 1888832 1692360  -21.71195  144   34  -21.04356  -22.28925  5.92%  12.6  340s\n",
      " 1918560 1719397  -22.21135   67   59  -21.04356  -22.28921  5.92%  12.6  345s\n",
      " 1952238 1749346  -21.59723  133   35  -21.04356  -22.28916  5.92%  12.6  350s\n",
      " 1968197 1763024  -22.25913   31   82  -21.04356  -22.28914  5.92%  12.6  355s\n",
      " 1995979 1789014  -22.15574   77   51  -21.04356  -22.28911  5.92%  12.6  360s\n",
      " 2027640 1817873  -22.00896  104   42  -21.04356  -22.28906  5.92%  12.6  365s\n",
      " 2058954 1845912  -22.14903   87   46  -21.04356  -22.28902  5.92%  12.6  370s\n",
      " 2084817 1867217  -22.16566   86   50  -21.04356  -22.28900  5.92%  12.6  375s\n",
      " 2105444 1887765  -21.76390  130   32  -21.04356  -22.28897  5.92%  12.6  380s\n",
      " 2136363 1916110  -21.96321  110   39  -21.04356  -22.28894  5.92%  12.6  385s\n",
      "H2155541 1887093                     -21.1332911  -22.28892  5.47%  12.6  388s\n",
      " 2160617 1893623  -21.94684  102   45  -21.13329  -22.28891  5.47%  12.6  390s\n",
      " 2184495 1914827  -21.41944  151   27  -21.13329  -22.28888  5.47%  12.6  395s\n",
      " 2208542 1933926     cutoff  166       -21.13329  -22.28886  5.47%  12.5  400s\n",
      " 2231446 1956574  -21.66147  143   32  -21.13329  -22.28882  5.47%  12.5  405s\n",
      " 2256679 1980271  -21.86911  124   32  -21.13329  -22.28880  5.47%  12.5  410s\n",
      " 2284903 2005131  -21.25995  168   20  -21.13329  -22.28876  5.47%  12.5  415s\n",
      " 2309819 2026053  -21.58943  128   30  -21.13329  -22.28874  5.47%  12.5  422s\n",
      " 2321595 2038668  -22.19669   66   61  -21.13329  -22.28873  5.47%  12.5  425s\n",
      " 2350599 2065482  -21.60402  132   43  -21.13329  -22.28871  5.47%  12.5  430s\n",
      " 2376989 2089830  -22.16812   75   51  -21.13329  -22.28868  5.47%  12.5  435s\n",
      " 2405653 2114712  -21.52031  154   21  -21.13329  -22.28864  5.47%  12.5  440s\n",
      " 2429057 2134014  -21.96955  105   36  -21.13329  -22.28862  5.47%  12.5  446s\n",
      " 2446604 2152156  -22.19434   72   53  -21.13329  -22.28860  5.47%  12.5  450s\n",
      " 2474131 2176978  -21.25754  161   24  -21.13329  -22.28857  5.47%  12.5  455s\n",
      " 2504719 2204574  -22.28585   30   83  -21.13329  -22.28854  5.47%  12.5  460s\n",
      " 2529157 2226337  -21.63156  134   32  -21.13329  -22.28851  5.47%  12.5  465s\n",
      " 2545120 2239029  -22.22236   63   60  -21.13329  -22.28849  5.47%  12.5  470s\n",
      " 2561211 2256050  -21.27874  155   24  -21.13329  -22.28849  5.47%  12.5  475s\n",
      " 2586406 2278964  -22.04879   99   47  -21.13329  -22.28846  5.47%  12.5  480s\n",
      " 2613374 2302958  -21.65319  134   35  -21.13329  -22.28844  5.47%  12.5  485s\n",
      " 2637562 2324899  -22.20408   66   54  -21.13329  -22.28842  5.47%  12.5  490s\n",
      " 2666360 2350396  -22.02330   94   51  -21.13329  -22.28838  5.47%  12.5  495s\n",
      " 2675350 2356855  -22.23588   59   60  -21.13329  -22.28838  5.47%  12.5  500s\n",
      " 2696617 2378381  -22.18203   79   51  -21.13329  -22.28836  5.47%  12.5  505s\n",
      " 2720597 2400387  -21.67864  130   33  -21.13329  -22.28834  5.47%  12.5  510s\n",
      " 2742780 2420952  -21.85412  118   31  -21.13329  -22.28832  5.47%  12.5  515s\n",
      " 2766934 2442632  -21.66622  128   34  -21.13329  -22.28830  5.47%  12.5  520s\n",
      " 2792199 2465380  -22.24422   56   63  -21.13329  -22.28828  5.47%  12.5  525s\n",
      " 2798021 2471157  -22.07123   96   47  -21.13329  -22.28827  5.47%  12.5  530s\n",
      " 2823458 2493643  -22.26181   45   71  -21.13329  -22.28825  5.47%  12.5  535s\n",
      " 2850334 2518557  -21.69542  126   46  -21.13329  -22.28824  5.47%  12.5  540s\n",
      " 2873617 2538821  -22.19421   65   66  -21.13329  -22.28821  5.46%  12.5  545s\n",
      " 2900388 2563765  -22.19063   74   51  -21.13329  -22.28819  5.46%  12.5  550s\n",
      " 2916691 2576204  -22.23191   63   61  -21.13329  -22.28817  5.46%  12.5  556s\n",
      " 2932723 2592766  -21.54339  144   25  -21.13329  -22.28816  5.46%  12.5  560s\n",
      " 2953152 2611940  -22.23562   60   58  -21.13329  -22.28814  5.46%  12.5  565s\n",
      " 2979494 2635312  -22.19321   57   65  -21.13329  -22.28812  5.46%  12.5  570s\n",
      " 2999605 2654054  -21.89504  114   38  -21.13329  -22.28811  5.46%  12.5  575s\n",
      " 3026050 2678077  -21.29188  157   26  -21.13329  -22.28809  5.46%  12.5  580s\n",
      " 3038776 2687681  -22.25739   39   77  -21.13329  -22.28808  5.46%  12.5  586s\n",
      " 3050559 2700387  -22.20754   72   59  -21.13329  -22.28807  5.46%  12.5  590s\n",
      " 3075076 2722507  -21.91977  113   34  -21.13329  -22.28805  5.46%  12.5  595s\n",
      " 3100455 2744139  -21.16702  175   17  -21.13329  -22.28803  5.46%  12.5  600s\n",
      "\n",
      "Explored 3101390 nodes (38636433 simplex iterations) in 600.02 seconds (576.42 work units)\n",
      "Thread count was 16 (of 16 available processors)\n",
      "\n",
      "Solution count 9: -21.1333 -21.0436 -20.9265 ... 9.21053e+17\n",
      "\n",
      "Time limit reached\n",
      "Best objective -2.113329108786e+01, best bound -2.228803079931e+01, gap 5.4641%\n"
     ]
    }
   ],
   "source": [
    "# get b\n",
    "b = data_test.datadict[\"b\"][0].cpu().numpy()\n",
    "# set n\n",
    "model.set_param_val({\"b\":b})\n",
    "# solve\n",
    "xval, objval = model.solve(\"gurobi\", tee=True, keepfiles=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b998840-e791-4b70-9144-cedc3f171c05",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
