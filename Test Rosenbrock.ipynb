{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "be61b914-e1af-43b9-9766-5092593f92dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x18494f853b0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch import nn\n",
    "from tqdm import tqdm\n",
    "\n",
    "# random seed\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "35fe4527-9dbf-48f0-bfb3-62cda59092aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# turn off warning\n",
    "import logging\n",
    "logging.getLogger('pyomo.core').setLevel(logging.ERROR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5560447-da9d-4cbe-aab4-538aa11c8ee8",
   "metadata": {},
   "source": [
    "## Problem Setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "52363c72-0d20-4c08-bd00-0f62aa73686c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# init\n",
    "num_data = 5000   # number of data\n",
    "num_vars = 5      # number of decision variables\n",
    "num_ints = 5      # number of integer decision variables\n",
    "test_size = 1000  # number of test size\n",
    "val_size = 1000   # number of validation size\n",
    "train_size = num_data - test_size - val_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "21938c00-d73f-4255-bc7c-bce6e8034cf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters as input data\n",
    "p_train = np.random.uniform(1.0, 6.0, (train_size, 1)).astype(np.float32)\n",
    "a_train = np.random.uniform(0.2, 1.2, (train_size, num_vars-1)).astype(np.float32)\n",
    "p_test = np.random.uniform(1.0, 6.0, (test_size, 1)).astype(np.float32)\n",
    "a_test = np.random.uniform(0.2, 1.2, (test_size, num_vars-1)).astype(np.float32)\n",
    "p_dev = np.random.uniform(1.0, 6.0, (val_size, 1)).astype(np.float32)\n",
    "a_dev = np.random.uniform(0.2, 1.2, (val_size, num_vars-1)).astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ab7678b7-f3c9-4066-816b-a13131bcd650",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nm datasets\n",
    "from neuromancer.dataset import DictDataset\n",
    "data_train = DictDataset({\"p\":p_train, \"a\":a_train}, name=\"train\")\n",
    "data_test = DictDataset({\"p\":p_test, \"a\":a_test}, name=\"test\")\n",
    "data_dev = DictDataset({\"p\":p_dev, \"a\":a_test}, name=\"dev\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c91eaf9b-55bd-417f-a544-84fde74c6dce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch dataloaders\n",
    "from torch.utils.data import DataLoader\n",
    "loader_train = DataLoader(data_train, batch_size=32, num_workers=0, collate_fn=data_train.collate_fn, shuffle=True)\n",
    "loader_test = DataLoader(data_test, batch_size=32, num_workers=0, collate_fn=data_test.collate_fn, shuffle=False)\n",
    "loader_dev = DataLoader(data_dev, batch_size=32, num_workers=0, collate_fn=data_dev.collate_fn, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "137f16db-28e1-4c9e-bdd3-b747f670c04d",
   "metadata": {},
   "source": [
    "## NM Problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "38c1c5a4-038b-4776-8a44-8ebb5a351e6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import neuromancer as nm\n",
    "from problem.neural import probRosenbrock\n",
    "\n",
    "def getNMProb(round_module):\n",
    "    # parameters\n",
    "    p = nm.constraint.variable(\"p\")\n",
    "    a = nm.constraint.variable(\"a\")\n",
    "    # variables\n",
    "    x_bar = nm.constraint.variable(\"x_bar\")\n",
    "    x_rnd = nm.constraint.variable(\"x_rnd\")\n",
    "\n",
    "    # model\n",
    "    obj_bar, constrs_bar = probRosenbrock(x_bar, p, a, num_vars=num_vars, alpha=100)\n",
    "    obj_rnd, constrs_rnd = probRosenbrock(x_rnd, p, a, num_vars=num_vars, alpha=100)\n",
    "\n",
    "    # define neural architecture for the solution mapping\n",
    "    func = nm.modules.blocks.MLP(insize=num_vars, outsize=num_vars, bias=True,\n",
    "                                 linear_map=nm.slim.maps[\"linear\"], nonlin=nn.ReLU, hsizes=[80]*4)\n",
    "    # solution map from model parameters: sol_map(p) -> x\n",
    "    sol_map = nm.system.Node(func, [\"p\", \"a\"], [\"x_bar\"], name=\"smap\")\n",
    "\n",
    "    # penalty loss for mapping\n",
    "    components = [sol_map]\n",
    "    loss = nm.loss.PenaltyLoss(obj_bar, constrs_bar)\n",
    "    problem = nm.problem.Problem(components, loss)\n",
    "\n",
    "    # penalty loss for rounding\n",
    "    components = [sol_map, round_module]\n",
    "    loss = nm.loss.PenaltyLoss(obj_rnd, constrs_rnd)\n",
    "    problem_rnd = nm.problem.Problem(components, loss)\n",
    "\n",
    "    return problem, problem_rnd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2b99751-3a6e-46dc-9e33-fdf68d813034",
   "metadata": {},
   "source": [
    "## Exact Solver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f2e7f07c-6a97-44d6-aca7-56f395e0c964",
   "metadata": {},
   "outputs": [],
   "source": [
    "from problem.solver import exactRosenbrock\n",
    "model = exactRosenbrock(n_vars=num_vars, n_integers=num_ints)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "77e326fa-de2f-415e-a4d4-1f28eef6511a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 1000/1000 [01:37<00:00, 10.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           Obj Val  Constraints Viol  Elapsed Time\n",
      "count  1000.000000            1000.0   1000.000000\n",
      "mean      1.786264               0.0      0.097387\n",
      "std       1.361469               0.0      0.034884\n",
      "min       0.000000               0.0      0.062039\n",
      "25%       0.466428               0.0      0.076015\n",
      "50%       1.731629               0.0      0.089871\n",
      "75%       2.953879               0.0      0.092289\n",
      "max       4.199299               0.0      0.244033\n"
     ]
    }
   ],
   "source": [
    "sols, objvals, conviols, elapseds = [], [], [], []\n",
    "for p, a in tqdm(list(zip(p_test, a_test))):\n",
    "    model.setParamValue(p, *a)\n",
    "    tick = time.time()\n",
    "    xval, objval = model.solve(\"scip\")\n",
    "    tock = time.time()\n",
    "    sols.append(list(xval.values()))\n",
    "    objvals.append(objval)\n",
    "    conviols.append(sum(model.calViolation()))\n",
    "    elapseds.append(tock - tick)\n",
    "df = pd.DataFrame({\"Sol\":sols, \"Obj Val\": objvals, \"Constraints Viol\": conviols, \"Elapsed Time\": elapseds})\n",
    "time.sleep(1)\n",
    "print(df.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e30ef9e5-41b5-4b4b-87b3-6c628fa51430",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sol</th>\n",
       "      <th>Obj Val</th>\n",
       "      <th>Constraints Viol</th>\n",
       "      <th>Elapsed Time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[1.0, 0.0, 1.0, 1.0, 0.0]</td>\n",
       "      <td>1.634025</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.131417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[1.0, 1.0, 1.0, 1.0, 0.0]</td>\n",
       "      <td>0.261138</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.091855</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[1.0, 1.0, 1.0, 0.0, 0.0]</td>\n",
       "      <td>1.244660</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.076746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[1.0, 0.0, 0.0, 0.0, 0.0]</td>\n",
       "      <td>3.591436</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.074814</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[1.0, 0.0, 0.0, 0.0, 0.0]</td>\n",
       "      <td>4.187316</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.078243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>[1.0, 0.0, 0.0, 0.0, 0.0]</td>\n",
       "      <td>4.089960</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.075583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>[1.0, 1.0, 1.0, 0.0, 0.0]</td>\n",
       "      <td>1.625817</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.076519</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>[1.0, 1.0, 1.0, 1.0, 0.0]</td>\n",
       "      <td>0.959324</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.106097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>[1.0, 1.0, 1.0, 1.0, 0.0]</td>\n",
       "      <td>1.077039</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.088842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>[1.0, 1.0, 1.0, 1.0, 1.0]</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.091446</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                           Sol   Obj Val  Constraints Viol  Elapsed Time\n",
       "0    [1.0, 0.0, 1.0, 1.0, 0.0]  1.634025               0.0      0.131417\n",
       "1    [1.0, 1.0, 1.0, 1.0, 0.0]  0.261138               0.0      0.091855\n",
       "2    [1.0, 1.0, 1.0, 0.0, 0.0]  1.244660               0.0      0.076746\n",
       "3    [1.0, 0.0, 0.0, 0.0, 0.0]  3.591436               0.0      0.074814\n",
       "4    [1.0, 0.0, 0.0, 0.0, 0.0]  4.187316               0.0      0.078243\n",
       "..                         ...       ...               ...           ...\n",
       "995  [1.0, 0.0, 0.0, 0.0, 0.0]  4.089960               0.0      0.075583\n",
       "996  [1.0, 1.0, 1.0, 0.0, 0.0]  1.625817               0.0      0.076519\n",
       "997  [1.0, 1.0, 1.0, 1.0, 0.0]  0.959324               0.0      0.106097\n",
       "998  [1.0, 1.0, 1.0, 1.0, 0.0]  1.077039               0.0      0.088842\n",
       "999  [1.0, 1.0, 1.0, 1.0, 1.0]  0.000000               0.0      0.091446\n",
       "\n",
       "[1000 rows x 4 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b3e1f73-eaa5-40d9-97fb-3b70009df41b",
   "metadata": {},
   "source": [
    "## Heuristic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "987f3b11-c41c-4012-b2ef-935f62099616",
   "metadata": {},
   "outputs": [],
   "source": [
    "from heuristic import naive_round"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "161556c9-67af-4686-bfec-d0fdea488bad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# relaxed model\n",
    "model_rel = model.relax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f0a6d751-9afa-4b4c-abbb-1b2c9ba6ad08",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 1000/1000 [18:34<00:00,  1.11s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           Obj Val  Constraints Viol  Elapsed Time\n",
      "count  1000.000000       1000.000000   1000.000000\n",
      "mean      0.346535          0.014486      1.113325\n",
      "std       0.572597          0.104144      0.357769\n",
      "min       0.000000          0.000000      0.432500\n",
      "25%       0.000000          0.000000      0.881712\n",
      "50%       0.000000          0.000000      1.009262\n",
      "75%       0.372265          0.000000      1.320632\n",
      "max       2.503698          0.762442      2.361451\n"
     ]
    }
   ],
   "source": [
    "sols, objvals, conviols, elapseds = [], [], [], []\n",
    "for p, a in tqdm(list(zip(p_test, a_test))):\n",
    "    model_rel.setParamValue(p, *a)\n",
    "    tick = time.time()\n",
    "    xval_init, _ = model_rel.solve(\"scip\", max_iter=100)\n",
    "    naive_round(xval_init, model)\n",
    "    tock = time.time()\n",
    "    xval, objval = model.getVal()\n",
    "    sols.append(list(xval.values()))\n",
    "    objvals.append(objval)\n",
    "    conviols.append(sum(model.calViolation()))\n",
    "    elapseds.append(tock - tick)\n",
    "df = pd.DataFrame({\"Sol\":sols, \"Obj Val\": objvals, \"Constraints Viol\": conviols, \"Elapsed Time\": elapseds})\n",
    "time.sleep(1)\n",
    "print(df.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d961673a-7420-4260-838a-1ef2fb765ceb",
   "metadata": {},
   "source": [
    "## Learning to Round"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2ec0072d-675a-4430-ac06-07936115a28d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from model.layer import netFC\n",
    "from model.round import roundModel\n",
    "# round x\n",
    "layers_rnd = netFC(input_dim=num_vars*2, hidden_dims=[80]*4, output_dim=num_vars)\n",
    "round_func = roundModel(layers=layers_rnd, param_keys=[\"p\", \"a\"], var_keys=[\"x_bar\"], output_keys=[\"x_rnd\"],\n",
    "                        int_ind={\"x_bar\":model.intInd}, name=\"round\")\n",
    "_, problem = getNMProb(round_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ea14d3a4-62f7-409b-874e-c9c58e0322a8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0  train_loss: 49.70849609375\n",
      "epoch: 1  train_loss: 41.51247024536133\n",
      "epoch: 2  train_loss: 40.15127944946289\n",
      "epoch: 3  train_loss: 50.554386138916016\n",
      "epoch: 4  train_loss: 45.82106018066406\n",
      "epoch: 5  train_loss: 49.82112121582031\n",
      "epoch: 6  train_loss: 40.62741470336914\n",
      "epoch: 7  train_loss: 39.87377166748047\n",
      "epoch: 8  train_loss: 47.59299087524414\n",
      "epoch: 9  train_loss: 42.070716857910156\n",
      "epoch: 10  train_loss: 41.75148010253906\n",
      "epoch: 11  train_loss: 37.79279327392578\n",
      "epoch: 12  train_loss: 46.500038146972656\n",
      "epoch: 13  train_loss: 39.58802032470703\n",
      "epoch: 14  train_loss: 45.82360076904297\n",
      "epoch: 15  train_loss: 49.97172546386719\n",
      "epoch: 16  train_loss: 31.36939239501953\n",
      "epoch: 17  train_loss: 40.5133171081543\n",
      "epoch: 18  train_loss: 38.941890716552734\n",
      "epoch: 19  train_loss: 26.786985397338867\n",
      "epoch: 20  train_loss: 28.458574295043945\n",
      "epoch: 21  train_loss: 25.684894561767578\n",
      "epoch: 22  train_loss: 28.383136749267578\n",
      "epoch: 23  train_loss: 26.57742691040039\n",
      "epoch: 24  train_loss: 43.52534484863281\n",
      "epoch: 25  train_loss: 40.52865219116211\n",
      "epoch: 26  train_loss: 32.44198226928711\n",
      "epoch: 27  train_loss: 43.12491226196289\n",
      "epoch: 28  train_loss: 44.91938018798828\n",
      "epoch: 29  train_loss: 33.34228515625\n",
      "epoch: 30  train_loss: 18.703275680541992\n",
      "epoch: 31  train_loss: 23.830013275146484\n",
      "epoch: 32  train_loss: 39.30241394042969\n",
      "epoch: 33  train_loss: 40.1699333190918\n",
      "epoch: 34  train_loss: 33.57280731201172\n",
      "epoch: 35  train_loss: 27.90878677368164\n",
      "epoch: 36  train_loss: 32.644596099853516\n",
      "epoch: 37  train_loss: 25.16240119934082\n",
      "epoch: 38  train_loss: 54.91907501220703\n",
      "epoch: 39  train_loss: 49.79977798461914\n",
      "epoch: 40  train_loss: 51.61915588378906\n",
      "epoch: 41  train_loss: 49.10498046875\n",
      "epoch: 42  train_loss: 52.333683013916016\n",
      "epoch: 43  train_loss: 56.93072509765625\n",
      "epoch: 44  train_loss: 44.83806610107422\n",
      "epoch: 45  train_loss: 62.353416442871094\n",
      "epoch: 46  train_loss: 54.86134719848633\n",
      "epoch: 47  train_loss: 52.71677017211914\n",
      "epoch: 48  train_loss: 29.382095336914062\n",
      "epoch: 49  train_loss: 34.290245056152344\n",
      "epoch: 50  train_loss: 38.36418914794922\n",
      "epoch: 51  train_loss: 37.90885543823242\n",
      "epoch: 52  train_loss: 31.834516525268555\n",
      "epoch: 53  train_loss: 32.25574493408203\n",
      "epoch: 54  train_loss: 22.86781120300293\n",
      "epoch: 55  train_loss: 24.081729888916016\n",
      "epoch: 56  train_loss: 30.568222045898438\n",
      "epoch: 57  train_loss: 32.60178756713867\n",
      "epoch: 58  train_loss: 25.963228225708008\n",
      "epoch: 59  train_loss: 41.927032470703125\n",
      "epoch: 60  train_loss: 44.99372100830078\n",
      "epoch: 61  train_loss: 30.726232528686523\n",
      "epoch: 62  train_loss: 33.80022048950195\n",
      "epoch: 63  train_loss: 42.083805084228516\n",
      "epoch: 64  train_loss: 44.8328742980957\n",
      "epoch: 65  train_loss: 35.638126373291016\n",
      "epoch: 66  train_loss: 37.132530212402344\n",
      "epoch: 67  train_loss: 33.09760284423828\n",
      "epoch: 68  train_loss: 52.80056381225586\n",
      "epoch: 69  train_loss: 22.53054428100586\n",
      "epoch: 70  train_loss: 37.79369354248047\n",
      "epoch: 71  train_loss: 42.547611236572266\n",
      "epoch: 72  train_loss: 55.63594436645508\n",
      "epoch: 73  train_loss: 33.663787841796875\n",
      "epoch: 74  train_loss: 26.63146209716797\n",
      "epoch: 75  train_loss: 30.347936630249023\n",
      "epoch: 76  train_loss: 58.81232452392578\n",
      "epoch: 77  train_loss: 35.66627502441406\n",
      "epoch: 78  train_loss: 55.187931060791016\n",
      "epoch: 79  train_loss: 51.707149505615234\n",
      "epoch: 80  train_loss: 33.41343307495117\n",
      "epoch: 81  train_loss: 54.98170471191406\n",
      "epoch: 82  train_loss: 54.614864349365234\n",
      "epoch: 83  train_loss: 40.99385070800781\n",
      "epoch: 84  train_loss: 37.16100311279297\n",
      "epoch: 85  train_loss: 35.60750198364258\n",
      "epoch: 86  train_loss: 43.148990631103516\n",
      "epoch: 87  train_loss: 36.961387634277344\n",
      "epoch: 88  train_loss: 35.296329498291016\n",
      "epoch: 89  train_loss: 33.0561408996582\n",
      "epoch: 90  train_loss: 31.741870880126953\n",
      "epoch: 91  train_loss: 41.70676040649414\n",
      "epoch: 92  train_loss: 31.949298858642578\n",
      "epoch: 93  train_loss: 39.72056579589844\n",
      "epoch: 94  train_loss: 19.189943313598633\n",
      "epoch: 95  train_loss: 27.702579498291016\n",
      "epoch: 96  train_loss: 26.443897247314453\n",
      "epoch: 97  train_loss: 29.333038330078125\n",
      "epoch: 98  train_loss: 36.631351470947266\n",
      "epoch: 99  train_loss: 47.89558029174805\n",
      "epoch: 100  train_loss: 59.96437454223633\n",
      "epoch: 101  train_loss: 37.926170349121094\n",
      "epoch: 102  train_loss: 33.80072021484375\n",
      "epoch: 103  train_loss: 48.59488296508789\n",
      "epoch: 104  train_loss: 56.154170989990234\n",
      "epoch: 105  train_loss: 49.32158660888672\n",
      "epoch: 106  train_loss: 67.3429183959961\n",
      "epoch: 107  train_loss: 35.531898498535156\n",
      "epoch: 108  train_loss: 33.8956413269043\n",
      "epoch: 109  train_loss: 40.5483283996582\n",
      "epoch: 110  train_loss: 53.459712982177734\n",
      "epoch: 111  train_loss: 46.87699508666992\n",
      "epoch: 112  train_loss: 53.34752655029297\n",
      "epoch: 113  train_loss: 39.30107116699219\n",
      "epoch: 114  train_loss: 32.891170501708984\n",
      "epoch: 115  train_loss: 33.62191390991211\n",
      "epoch: 116  train_loss: 33.53410339355469\n",
      "epoch: 117  train_loss: 48.58388137817383\n",
      "epoch: 118  train_loss: 48.21250534057617\n",
      "epoch: 119  train_loss: 58.45820617675781\n",
      "epoch: 120  train_loss: 38.2036018371582\n",
      "epoch: 121  train_loss: 36.246620178222656\n",
      "epoch: 122  train_loss: 19.718069076538086\n",
      "epoch: 123  train_loss: 37.93036651611328\n",
      "epoch: 124  train_loss: 52.54172897338867\n",
      "epoch: 125  train_loss: 50.12855911254883\n",
      "epoch: 126  train_loss: 53.24464416503906\n",
      "epoch: 127  train_loss: 46.45599365234375\n",
      "epoch: 128  train_loss: 45.3583869934082\n",
      "epoch: 129  train_loss: 40.88192367553711\n",
      "epoch: 130  train_loss: 46.10515594482422\n",
      "epoch: 131  train_loss: 62.674354553222656\n",
      "epoch: 132  train_loss: 58.01472473144531\n",
      "epoch: 133  train_loss: 46.89524841308594\n",
      "epoch: 134  train_loss: 56.954017639160156\n",
      "epoch: 135  train_loss: 46.08685302734375\n",
      "epoch: 136  train_loss: 41.12287139892578\n",
      "epoch: 137  train_loss: 40.73546600341797\n",
      "epoch: 138  train_loss: 46.3673210144043\n",
      "epoch: 139  train_loss: 54.0826530456543\n",
      "epoch: 140  train_loss: 54.11771774291992\n",
      "epoch: 141  train_loss: 56.66830062866211\n",
      "epoch: 142  train_loss: 45.98710632324219\n",
      "epoch: 143  train_loss: 55.46737289428711\n",
      "epoch: 144  train_loss: 46.11493682861328\n",
      "epoch: 145  train_loss: 35.566123962402344\n",
      "Early stopping!!!\n"
     ]
    }
   ],
   "source": [
    "# training\n",
    "lr = 0.001    # step size for gradient descent\n",
    "epochs = 400  # number of training epochs\n",
    "warmup = 50   # number of epochs to wait before enacting early stopping policy\n",
    "patience = 50 # number of epochs with no improvement in eval metric to allow before early stopping\n",
    "# set adamW as optimizer\n",
    "optimizer = torch.optim.AdamW(problem.parameters(), lr=lr)\n",
    "# define trainer\n",
    "trainer = nm.trainer.Trainer(problem, loader_train, loader_dev, loader_test,\n",
    "                             optimizer, epochs=epochs, patience=patience, warmup=warmup)\n",
    "best_model = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "04b034b7-d404-4e6d-aa6f-11ae008ef1d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:05<00:00, 184.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           Obj Val  Constraints Viol  Elapsed Time\n",
      "count  1000.000000       1000.000000   1000.000000\n",
      "mean      3.580870          0.704800      0.004821\n",
      "std       0.655242          0.857373      0.001216\n",
      "min       1.750512          0.000000      0.002003\n",
      "25%       3.246814          0.000000      0.003999\n",
      "50%       3.766097          0.000000      0.004556\n",
      "75%       3.766097          1.762442      0.005614\n",
      "max       4.519283          2.762442      0.010525\n"
     ]
    }
   ],
   "source": [
    "sols, objvals, conviols, elapseds = [], [], [], []\n",
    "for p, a in tqdm(list(zip(p_test, a_test))):\n",
    "    datapoints = {\"p\": torch.tensor(np.array([p]), dtype=torch.float32), \"a\":torch.tensor(np.array([a]), dtype=torch.float32), \"name\": \"test\"}\n",
    "    tick = time.time()\n",
    "    output = problem(datapoints)\n",
    "    tock = time.time()\n",
    "    x = output[\"test_x_rnd\"]\n",
    "    # get values\n",
    "    for ind in model.x:\n",
    "        model.x[ind].value = x[0, ind].item()\n",
    "    xval, objval = model.getVal()\n",
    "    sols.append(xval.values())\n",
    "    objvals.append(objval)\n",
    "    conviols.append(sum(model.calViolation()))\n",
    "    elapseds.append(tock - tick)\n",
    "df = pd.DataFrame({\"Sol\":sols, \"Obj Val\": objvals, \"Constraints Viol\": conviols, \"Elapsed Time\": elapseds})\n",
    "time.sleep(1)\n",
    "print(df.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5ae6e5c-983f-4689-88e2-4ef26fe8e910",
   "metadata": {},
   "source": [
    "## Learnable Threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "545cae68-65b7-4d0d-b87d-f021dfb0a2a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from model.layer import netFC\n",
    "from model.threshold import roundThresholdModel\n",
    "# round x\n",
    "layers_rnd = netFC(input_dim=num_vars*2, hidden_dims=[80]*4, output_dim=num_vars)\n",
    "round_func = roundThresholdModel(layers=layers_rnd, param_keys=[\"p\", \"a\"], var_keys=[\"x_bar\"], output_keys=[\"x_rnd\"],\n",
    "                                 int_ind={\"x_bar\":model.intInd}, name=\"round\")\n",
    "_, problem = getNMProb(round_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "18744735-a740-489c-bd05-49988544dfac",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0  train_loss: 59.47038650512695\n",
      "epoch: 1  train_loss: 46.17607879638672\n",
      "epoch: 2  train_loss: 42.86919021606445\n",
      "epoch: 3  train_loss: 44.91936111450195\n",
      "epoch: 4  train_loss: 40.279144287109375\n",
      "epoch: 5  train_loss: 37.545692443847656\n",
      "epoch: 6  train_loss: 43.543426513671875\n",
      "epoch: 7  train_loss: 42.39023208618164\n",
      "epoch: 8  train_loss: 46.024139404296875\n",
      "epoch: 9  train_loss: 59.60663604736328\n",
      "epoch: 10  train_loss: 59.56098937988281\n",
      "epoch: 11  train_loss: 49.43817138671875\n",
      "epoch: 12  train_loss: 62.17286682128906\n",
      "epoch: 13  train_loss: 72.67794036865234\n",
      "epoch: 14  train_loss: 59.97739028930664\n",
      "epoch: 15  train_loss: 63.3626823425293\n",
      "epoch: 16  train_loss: 58.953956604003906\n",
      "epoch: 17  train_loss: 67.74063873291016\n",
      "epoch: 18  train_loss: 83.29310607910156\n",
      "epoch: 19  train_loss: 69.541015625\n",
      "epoch: 20  train_loss: 65.42090606689453\n",
      "epoch: 21  train_loss: 72.77654266357422\n",
      "epoch: 22  train_loss: 75.35345458984375\n",
      "epoch: 23  train_loss: 80.69808959960938\n",
      "epoch: 24  train_loss: 72.94364929199219\n",
      "epoch: 25  train_loss: 99.64214324951172\n",
      "epoch: 26  train_loss: 79.31028747558594\n",
      "epoch: 27  train_loss: 74.37252044677734\n",
      "epoch: 28  train_loss: 88.41320037841797\n",
      "epoch: 29  train_loss: 95.14671325683594\n",
      "epoch: 30  train_loss: 105.89622497558594\n",
      "epoch: 31  train_loss: 104.83903503417969\n",
      "epoch: 32  train_loss: 87.27291870117188\n",
      "epoch: 33  train_loss: 100.85918426513672\n",
      "epoch: 34  train_loss: 90.3879623413086\n",
      "epoch: 35  train_loss: 94.09661865234375\n",
      "epoch: 36  train_loss: 108.68936920166016\n",
      "epoch: 37  train_loss: 83.03824615478516\n",
      "epoch: 38  train_loss: 103.3767318725586\n",
      "epoch: 39  train_loss: 98.54875183105469\n",
      "epoch: 40  train_loss: 96.60873413085938\n",
      "epoch: 41  train_loss: 91.36627197265625\n",
      "epoch: 42  train_loss: 84.4214096069336\n",
      "epoch: 43  train_loss: 82.52657318115234\n",
      "epoch: 44  train_loss: 90.41411590576172\n",
      "epoch: 45  train_loss: 84.51293182373047\n",
      "epoch: 46  train_loss: 86.4743423461914\n",
      "epoch: 47  train_loss: 92.90785217285156\n",
      "epoch: 48  train_loss: 84.09566497802734\n",
      "epoch: 49  train_loss: 92.94988250732422\n",
      "epoch: 50  train_loss: 87.40996551513672\n",
      "epoch: 51  train_loss: 88.03833770751953\n",
      "epoch: 52  train_loss: 94.54875183105469\n",
      "epoch: 53  train_loss: 103.9119644165039\n",
      "epoch: 54  train_loss: 100.61336517333984\n",
      "epoch: 55  train_loss: 107.74758911132812\n",
      "epoch: 56  train_loss: 96.26746368408203\n",
      "epoch: 57  train_loss: 109.73877716064453\n",
      "epoch: 58  train_loss: 97.20502471923828\n",
      "epoch: 59  train_loss: 94.64266967773438\n",
      "epoch: 60  train_loss: 104.83226776123047\n",
      "epoch: 61  train_loss: 99.13941192626953\n",
      "epoch: 62  train_loss: 84.91490173339844\n",
      "epoch: 63  train_loss: 98.18247985839844\n",
      "epoch: 64  train_loss: 84.30056762695312\n",
      "epoch: 65  train_loss: 94.98148345947266\n",
      "epoch: 66  train_loss: 94.00623321533203\n",
      "epoch: 67  train_loss: 94.90825653076172\n",
      "epoch: 68  train_loss: 96.81916046142578\n",
      "epoch: 69  train_loss: 102.65791320800781\n",
      "epoch: 70  train_loss: 96.99761962890625\n",
      "epoch: 71  train_loss: 93.89099884033203\n",
      "epoch: 72  train_loss: 93.17521667480469\n",
      "epoch: 73  train_loss: 90.81783294677734\n",
      "epoch: 74  train_loss: 99.7073974609375\n",
      "epoch: 75  train_loss: 94.96542358398438\n",
      "epoch: 76  train_loss: 90.8712387084961\n",
      "epoch: 77  train_loss: 99.99750518798828\n",
      "epoch: 78  train_loss: 92.68315887451172\n",
      "epoch: 79  train_loss: 74.3183822631836\n",
      "epoch: 80  train_loss: 85.657470703125\n",
      "epoch: 81  train_loss: 90.50626373291016\n",
      "epoch: 82  train_loss: 84.75431060791016\n",
      "epoch: 83  train_loss: 97.55787658691406\n",
      "epoch: 84  train_loss: 91.40289306640625\n",
      "epoch: 85  train_loss: 102.89146423339844\n",
      "epoch: 86  train_loss: 105.9323501586914\n",
      "epoch: 87  train_loss: 95.43250274658203\n",
      "epoch: 88  train_loss: 91.80546569824219\n",
      "epoch: 89  train_loss: 82.98829650878906\n",
      "epoch: 90  train_loss: 93.17025756835938\n",
      "epoch: 91  train_loss: 89.88304901123047\n",
      "epoch: 92  train_loss: 95.42591857910156\n",
      "epoch: 93  train_loss: 102.99871063232422\n",
      "epoch: 94  train_loss: 84.44349670410156\n",
      "epoch: 95  train_loss: 101.4213638305664\n",
      "epoch: 96  train_loss: 95.25020599365234\n",
      "epoch: 97  train_loss: 89.9576416015625\n",
      "epoch: 98  train_loss: 83.07429504394531\n",
      "epoch: 99  train_loss: 92.86572265625\n",
      "epoch: 100  train_loss: 100.3185043334961\n",
      "epoch: 101  train_loss: 91.88630676269531\n",
      "Early stopping!!!\n"
     ]
    }
   ],
   "source": [
    "# training\n",
    "lr = 0.001    # step size for gradient descent\n",
    "epochs = 400  # number of training epochs\n",
    "warmup = 50   # number of epochs to wait before enacting early stopping policy\n",
    "patience = 50 # number of epochs with no improvement in eval metric to allow before early stopping\n",
    "# set adamW as optimizer\n",
    "optimizer = torch.optim.AdamW(problem.parameters(), lr=lr)\n",
    "# define trainer\n",
    "trainer = nm.trainer.Trainer(problem, loader_train, loader_dev, loader_test,\n",
    "                             optimizer, epochs=epochs, patience=patience, warmup=warmup)\n",
    "best_model = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "234958b5-a9bd-4a1a-b56a-edd8f2cbd6f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:05<00:00, 187.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           Obj Val  Constraints Viol  Elapsed Time\n",
      "count  1000.000000       1000.000000   1000.000000\n",
      "mean      6.831584          1.355442      0.004681\n",
      "std       1.731548          0.497593      0.001181\n",
      "min       3.246814          0.762442      0.002547\n",
      "25%       7.262399          0.762442      0.003999\n",
      "50%       7.262399          1.762442      0.004524\n",
      "75%       8.015585          1.762442      0.005275\n",
      "max       8.015585          2.762442      0.018537\n"
     ]
    }
   ],
   "source": [
    "sols, objvals, conviols, elapseds = [], [], [], []\n",
    "for p, a in tqdm(list(zip(p_test, a_test))):\n",
    "    datapoints = {\"p\": torch.tensor(np.array([p]), dtype=torch.float32), \"a\":torch.tensor(np.array([a]), dtype=torch.float32), \"name\": \"test\"}\n",
    "    tick = time.time()\n",
    "    output = problem(datapoints)\n",
    "    tock = time.time()\n",
    "    x = output[\"test_x_rnd\"]\n",
    "    # get values\n",
    "    for ind in model.x:\n",
    "        model.x[ind].value = x[0, ind].item()\n",
    "    xval, objval = model.getVal()\n",
    "    sols.append(xval.values())\n",
    "    objvals.append(objval)\n",
    "    conviols.append(sum(model.calViolation()))\n",
    "    elapseds.append(tock - tick)\n",
    "df = pd.DataFrame({\"Sol\":sols, \"Obj Val\": objvals, \"Constraints Viol\": conviols, \"Elapsed Time\": elapseds})\n",
    "time.sleep(1)\n",
    "print(df.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6c777ad-d5a3-4065-a137-123aa84c0df5",
   "metadata": {},
   "source": [
    "## Learning to Round with Fixed Solution Mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "64737e09-1735-4893-a0d6-c8ca73a91fc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from model.layer import netFC\n",
    "from model.round import roundModel\n",
    "# round x\n",
    "layers_rnd = netFC(input_dim=num_vars*2, hidden_dims=[80]*4, output_dim=num_vars)\n",
    "round_func = roundModel(layers=layers_rnd, param_keys=[\"p\", \"a\"], var_keys=[\"x_bar\"], output_keys=[\"x_rnd\"],\n",
    "                        int_ind={\"x_bar\":model.intInd}, name=\"round\")\n",
    "problem_rel, problem_rnd = getNMProb(round_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8b39c4a7-2b21-4384-9d85-2e479ce85965",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0  train_loss: 33.30405044555664\n",
      "epoch: 1  train_loss: 0.7042540907859802\n",
      "epoch: 2  train_loss: 0.44580766558647156\n",
      "epoch: 3  train_loss: 0.41395464539527893\n",
      "epoch: 4  train_loss: 0.4325869083404541\n",
      "epoch: 5  train_loss: 0.43818360567092896\n",
      "epoch: 6  train_loss: 0.4464978277683258\n",
      "epoch: 7  train_loss: 0.42415156960487366\n",
      "epoch: 8  train_loss: 0.3839186429977417\n",
      "epoch: 9  train_loss: 0.37431758642196655\n",
      "epoch: 10  train_loss: 0.4232294261455536\n",
      "epoch: 11  train_loss: 0.38242509961128235\n",
      "epoch: 12  train_loss: 0.3845195770263672\n",
      "epoch: 13  train_loss: 0.3933694660663605\n",
      "epoch: 14  train_loss: 0.3860446810722351\n",
      "epoch: 15  train_loss: 0.3651963174343109\n",
      "epoch: 16  train_loss: 0.37582266330718994\n",
      "epoch: 17  train_loss: 0.40678274631500244\n",
      "epoch: 18  train_loss: 0.4135703146457672\n",
      "epoch: 19  train_loss: 0.356105238199234\n",
      "epoch: 20  train_loss: 0.40241971611976624\n",
      "epoch: 21  train_loss: 0.368284672498703\n",
      "epoch: 22  train_loss: 0.37881582975387573\n",
      "epoch: 23  train_loss: 0.36577582359313965\n",
      "epoch: 24  train_loss: 0.37399938702583313\n",
      "epoch: 25  train_loss: 0.38141196966171265\n",
      "epoch: 26  train_loss: 0.4076487421989441\n",
      "epoch: 27  train_loss: 0.40572258830070496\n",
      "epoch: 28  train_loss: 0.3443998396396637\n",
      "epoch: 29  train_loss: 0.3805054724216461\n",
      "epoch: 30  train_loss: 0.36654359102249146\n",
      "epoch: 31  train_loss: 0.37982895970344543\n",
      "epoch: 32  train_loss: 0.38952094316482544\n",
      "epoch: 33  train_loss: 0.34745150804519653\n",
      "epoch: 34  train_loss: 0.3595829904079437\n",
      "epoch: 35  train_loss: 0.36305564641952515\n",
      "epoch: 36  train_loss: 0.3755895793437958\n",
      "epoch: 37  train_loss: 0.3412068486213684\n",
      "epoch: 38  train_loss: 0.3968510627746582\n",
      "epoch: 39  train_loss: 0.3377126157283783\n",
      "epoch: 40  train_loss: 0.33770889043807983\n",
      "epoch: 41  train_loss: 0.4207320511341095\n",
      "epoch: 42  train_loss: 0.3296019434928894\n",
      "epoch: 43  train_loss: 0.35450586676597595\n",
      "epoch: 44  train_loss: 0.35844042897224426\n",
      "epoch: 45  train_loss: 0.3576827347278595\n",
      "epoch: 46  train_loss: 0.3640114665031433\n",
      "epoch: 47  train_loss: 0.3760688900947571\n",
      "epoch: 48  train_loss: 0.3873136043548584\n",
      "epoch: 49  train_loss: 0.34587782621383667\n",
      "epoch: 50  train_loss: 0.34957048296928406\n",
      "epoch: 51  train_loss: 0.3532441258430481\n",
      "epoch: 52  train_loss: 0.3279098570346832\n",
      "epoch: 53  train_loss: 0.37137511372566223\n",
      "epoch: 54  train_loss: 0.33086100220680237\n",
      "epoch: 55  train_loss: 0.3419454097747803\n",
      "epoch: 56  train_loss: 0.3724788427352905\n",
      "epoch: 57  train_loss: 0.366763710975647\n",
      "epoch: 58  train_loss: 0.3672874867916107\n",
      "epoch: 59  train_loss: 0.27885758876800537\n",
      "epoch: 60  train_loss: 0.3638383150100708\n",
      "epoch: 61  train_loss: 0.33623504638671875\n",
      "epoch: 62  train_loss: 0.3103228211402893\n",
      "epoch: 63  train_loss: 0.3146247863769531\n",
      "epoch: 64  train_loss: 0.36700257658958435\n",
      "epoch: 65  train_loss: 0.2968943417072296\n",
      "epoch: 66  train_loss: 0.2885860800743103\n",
      "epoch: 67  train_loss: 0.33762025833129883\n",
      "epoch: 68  train_loss: 0.32745954394340515\n",
      "epoch: 69  train_loss: 0.3196850121021271\n",
      "epoch: 70  train_loss: 0.34725871682167053\n",
      "epoch: 71  train_loss: 0.3638620674610138\n",
      "epoch: 72  train_loss: 0.348724365234375\n",
      "epoch: 73  train_loss: 0.34612351655960083\n",
      "epoch: 74  train_loss: 0.34180620312690735\n",
      "epoch: 75  train_loss: 0.32125401496887207\n",
      "epoch: 76  train_loss: 0.29556939005851746\n",
      "epoch: 77  train_loss: 0.3974245488643646\n",
      "epoch: 78  train_loss: 0.34673160314559937\n",
      "epoch: 79  train_loss: 0.3286948800086975\n",
      "epoch: 80  train_loss: 0.33834773302078247\n",
      "epoch: 81  train_loss: 0.327443927526474\n",
      "epoch: 82  train_loss: 0.3498993217945099\n",
      "epoch: 83  train_loss: 0.31032660603523254\n",
      "epoch: 84  train_loss: 0.30507680773735046\n",
      "epoch: 85  train_loss: 0.36056530475616455\n",
      "epoch: 86  train_loss: 0.35339030623435974\n",
      "epoch: 87  train_loss: 0.3028313219547272\n",
      "epoch: 88  train_loss: 0.3110070824623108\n",
      "epoch: 89  train_loss: 0.31528693437576294\n",
      "epoch: 90  train_loss: 0.3611133396625519\n",
      "epoch: 91  train_loss: 0.3196319043636322\n",
      "epoch: 92  train_loss: 0.33163538575172424\n",
      "epoch: 93  train_loss: 0.30689308047294617\n",
      "epoch: 94  train_loss: 0.30407387018203735\n",
      "epoch: 95  train_loss: 0.29616403579711914\n",
      "epoch: 96  train_loss: 0.34880560636520386\n",
      "epoch: 97  train_loss: 0.3178708553314209\n",
      "epoch: 98  train_loss: 0.35702651739120483\n",
      "epoch: 99  train_loss: 0.3104220926761627\n",
      "epoch: 100  train_loss: 0.2659268379211426\n",
      "epoch: 101  train_loss: 0.3127511739730835\n",
      "epoch: 102  train_loss: 0.3444176912307739\n",
      "epoch: 103  train_loss: 0.31571105122566223\n",
      "epoch: 104  train_loss: 0.26460689306259155\n",
      "epoch: 105  train_loss: 0.31179046630859375\n",
      "epoch: 106  train_loss: 0.3414301872253418\n",
      "epoch: 107  train_loss: 0.38422471284866333\n",
      "epoch: 108  train_loss: 0.331871896982193\n",
      "epoch: 109  train_loss: 0.3532848060131073\n",
      "epoch: 110  train_loss: 0.3251911401748657\n",
      "epoch: 111  train_loss: 0.31559550762176514\n",
      "epoch: 112  train_loss: 0.3512815535068512\n",
      "epoch: 113  train_loss: 0.3747304677963257\n",
      "epoch: 114  train_loss: 0.27448704838752747\n",
      "epoch: 115  train_loss: 0.3258499503135681\n",
      "epoch: 116  train_loss: 0.31781378388404846\n",
      "epoch: 117  train_loss: 0.299923837184906\n",
      "epoch: 118  train_loss: 0.3248983919620514\n",
      "epoch: 119  train_loss: 0.28784650564193726\n",
      "epoch: 120  train_loss: 0.27633556723594666\n",
      "epoch: 121  train_loss: 0.289131760597229\n",
      "epoch: 122  train_loss: 0.3381190299987793\n",
      "epoch: 123  train_loss: 0.3080330193042755\n",
      "epoch: 124  train_loss: 0.37251016497612\n",
      "epoch: 125  train_loss: 0.3802145719528198\n",
      "epoch: 126  train_loss: 0.34688299894332886\n",
      "epoch: 127  train_loss: 0.29945307970046997\n",
      "epoch: 128  train_loss: 0.31524571776390076\n",
      "epoch: 129  train_loss: 0.41206830739974976\n",
      "epoch: 130  train_loss: 0.2992297112941742\n",
      "epoch: 131  train_loss: 0.2794994115829468\n",
      "epoch: 132  train_loss: 0.3119822144508362\n",
      "epoch: 133  train_loss: 0.3047511875629425\n",
      "epoch: 134  train_loss: 0.2969796061515808\n",
      "epoch: 135  train_loss: 0.2976459264755249\n",
      "epoch: 136  train_loss: 0.2971164286136627\n",
      "epoch: 137  train_loss: 0.32006460428237915\n",
      "epoch: 138  train_loss: 0.34169870615005493\n",
      "epoch: 139  train_loss: 0.3082294464111328\n",
      "epoch: 140  train_loss: 0.2726895809173584\n",
      "epoch: 141  train_loss: 0.31122130155563354\n",
      "epoch: 142  train_loss: 0.28717875480651855\n",
      "epoch: 143  train_loss: 0.32863542437553406\n",
      "epoch: 144  train_loss: 0.29524582624435425\n",
      "Early stopping!!!\n"
     ]
    }
   ],
   "source": [
    "# training for mapping\n",
    "lr = 0.001    # step size for gradient descent\n",
    "epochs = 400  # number of training epochs\n",
    "warmup = 50   # number of epochs to wait before enacting early stopping policy\n",
    "patience = 50 # number of epochs with no improvement in eval metric to allow before early stopping\n",
    "# set adamW as optimizer\n",
    "optimizer = torch.optim.AdamW(problem_rel.parameters(), lr=lr)\n",
    "# define trainer\n",
    "trainer = nm.trainer.Trainer(problem_rel, loader_train, loader_dev, loader_test,\n",
    "                             optimizer, epochs=epochs, patience=patience, warmup=warmup)\n",
    "best_model = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a1efad3d-dd09-4aef-81dd-3ed3c31ffb31",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0  train_loss: 131.53704833984375\n",
      "epoch: 1  train_loss: 129.52284240722656\n",
      "epoch: 2  train_loss: 133.23451232910156\n",
      "epoch: 3  train_loss: 133.18605041503906\n",
      "epoch: 4  train_loss: 130.60865783691406\n",
      "epoch: 5  train_loss: 133.2777557373047\n",
      "epoch: 6  train_loss: 133.48606872558594\n",
      "epoch: 7  train_loss: 129.01168823242188\n",
      "epoch: 8  train_loss: 126.90194702148438\n",
      "epoch: 9  train_loss: 129.93064880371094\n",
      "epoch: 10  train_loss: 131.24891662597656\n",
      "epoch: 11  train_loss: 129.44924926757812\n",
      "epoch: 12  train_loss: 133.0521697998047\n",
      "epoch: 13  train_loss: 131.92398071289062\n",
      "epoch: 14  train_loss: 133.05322265625\n",
      "epoch: 15  train_loss: 128.50523376464844\n",
      "epoch: 16  train_loss: 132.8893585205078\n",
      "epoch: 17  train_loss: 131.83924865722656\n",
      "epoch: 18  train_loss: 131.30224609375\n",
      "epoch: 19  train_loss: 130.4916229248047\n",
      "epoch: 20  train_loss: 132.34283447265625\n",
      "epoch: 21  train_loss: 132.94374084472656\n",
      "epoch: 22  train_loss: 129.9585723876953\n",
      "epoch: 23  train_loss: 135.364990234375\n",
      "epoch: 24  train_loss: 132.50030517578125\n",
      "epoch: 25  train_loss: 132.30697631835938\n",
      "epoch: 26  train_loss: 130.60520935058594\n",
      "epoch: 27  train_loss: 132.7358856201172\n",
      "epoch: 28  train_loss: 131.76968383789062\n",
      "epoch: 29  train_loss: 128.1695098876953\n",
      "epoch: 30  train_loss: 132.67459106445312\n",
      "epoch: 31  train_loss: 131.5423583984375\n",
      "epoch: 32  train_loss: 132.3212432861328\n",
      "epoch: 33  train_loss: 129.5757598876953\n",
      "epoch: 34  train_loss: 131.82736206054688\n",
      "epoch: 35  train_loss: 134.14718627929688\n",
      "epoch: 36  train_loss: 127.43548583984375\n",
      "epoch: 37  train_loss: 129.88255310058594\n",
      "epoch: 38  train_loss: 132.43226623535156\n",
      "epoch: 39  train_loss: 132.4693145751953\n",
      "epoch: 40  train_loss: 129.4850616455078\n",
      "epoch: 41  train_loss: 131.23008728027344\n",
      "epoch: 42  train_loss: 133.03106689453125\n",
      "epoch: 43  train_loss: 135.7313232421875\n",
      "epoch: 44  train_loss: 129.74774169921875\n",
      "epoch: 45  train_loss: 131.9124755859375\n",
      "epoch: 46  train_loss: 131.5132293701172\n",
      "epoch: 47  train_loss: 131.25167846679688\n",
      "epoch: 48  train_loss: 129.93963623046875\n",
      "epoch: 49  train_loss: 132.5907745361328\n",
      "epoch: 50  train_loss: 133.8511505126953\n",
      "epoch: 51  train_loss: 130.3143310546875\n",
      "epoch: 52  train_loss: 130.35740661621094\n",
      "epoch: 53  train_loss: 129.8577117919922\n",
      "epoch: 54  train_loss: 133.48020935058594\n",
      "epoch: 55  train_loss: 129.93377685546875\n",
      "epoch: 56  train_loss: 133.55165100097656\n",
      "epoch: 57  train_loss: 132.373779296875\n",
      "epoch: 58  train_loss: 134.23558044433594\n",
      "epoch: 59  train_loss: 130.7842254638672\n",
      "epoch: 60  train_loss: 133.11489868164062\n",
      "epoch: 61  train_loss: 134.76541137695312\n",
      "epoch: 62  train_loss: 130.4833526611328\n",
      "epoch: 63  train_loss: 132.29119873046875\n",
      "epoch: 64  train_loss: 132.63760375976562\n",
      "epoch: 65  train_loss: 134.52767944335938\n",
      "epoch: 66  train_loss: 131.04220581054688\n",
      "epoch: 67  train_loss: 134.50167846679688\n",
      "epoch: 68  train_loss: 131.03195190429688\n",
      "epoch: 69  train_loss: 130.8970489501953\n",
      "epoch: 70  train_loss: 129.94085693359375\n",
      "epoch: 71  train_loss: 131.37310791015625\n",
      "epoch: 72  train_loss: 132.39874267578125\n",
      "epoch: 73  train_loss: 133.06759643554688\n",
      "epoch: 74  train_loss: 132.66427612304688\n",
      "epoch: 75  train_loss: 131.43783569335938\n",
      "epoch: 76  train_loss: 131.30137634277344\n",
      "epoch: 77  train_loss: 133.22296142578125\n",
      "epoch: 78  train_loss: 128.12254333496094\n",
      "epoch: 79  train_loss: 131.83851623535156\n",
      "epoch: 80  train_loss: 127.63799285888672\n",
      "epoch: 81  train_loss: 131.34747314453125\n",
      "epoch: 82  train_loss: 131.89991760253906\n",
      "epoch: 83  train_loss: 129.42572021484375\n",
      "epoch: 84  train_loss: 133.88966369628906\n",
      "epoch: 85  train_loss: 134.81658935546875\n",
      "epoch: 86  train_loss: 133.717041015625\n",
      "epoch: 87  train_loss: 132.4062957763672\n",
      "epoch: 88  train_loss: 129.685302734375\n",
      "epoch: 89  train_loss: 130.1820068359375\n",
      "epoch: 90  train_loss: 132.4051971435547\n",
      "epoch: 91  train_loss: 131.72579956054688\n",
      "epoch: 92  train_loss: 127.98101806640625\n",
      "epoch: 93  train_loss: 132.50411987304688\n",
      "epoch: 94  train_loss: 132.7340087890625\n",
      "epoch: 95  train_loss: 132.04624938964844\n",
      "epoch: 96  train_loss: 132.18020629882812\n",
      "epoch: 97  train_loss: 130.28916931152344\n",
      "epoch: 98  train_loss: 137.15394592285156\n",
      "epoch: 99  train_loss: 132.86990356445312\n",
      "epoch: 100  train_loss: 134.01864624023438\n",
      "epoch: 101  train_loss: 130.64569091796875\n",
      "epoch: 102  train_loss: 129.0524139404297\n",
      "epoch: 103  train_loss: 134.63812255859375\n",
      "epoch: 104  train_loss: 127.85164642333984\n",
      "epoch: 105  train_loss: 131.15550231933594\n",
      "epoch: 106  train_loss: 130.83584594726562\n",
      "epoch: 107  train_loss: 129.97386169433594\n",
      "epoch: 108  train_loss: 130.89219665527344\n",
      "epoch: 109  train_loss: 131.65170288085938\n",
      "epoch: 110  train_loss: 129.86874389648438\n",
      "epoch: 111  train_loss: 130.85171508789062\n",
      "epoch: 112  train_loss: 128.57644653320312\n",
      "epoch: 113  train_loss: 128.31695556640625\n",
      "epoch: 114  train_loss: 130.48745727539062\n",
      "epoch: 115  train_loss: 134.1252899169922\n",
      "epoch: 116  train_loss: 133.3147735595703\n",
      "epoch: 117  train_loss: 131.77474975585938\n",
      "epoch: 118  train_loss: 132.1663360595703\n",
      "epoch: 119  train_loss: 135.25747680664062\n",
      "epoch: 120  train_loss: 134.03431701660156\n",
      "epoch: 121  train_loss: 134.35470581054688\n",
      "epoch: 122  train_loss: 131.28250122070312\n",
      "epoch: 123  train_loss: 134.40928649902344\n",
      "epoch: 124  train_loss: 136.44906616210938\n",
      "epoch: 125  train_loss: 129.82470703125\n",
      "epoch: 126  train_loss: 129.8663330078125\n",
      "epoch: 127  train_loss: 132.01141357421875\n",
      "epoch: 128  train_loss: 134.58633422851562\n",
      "epoch: 129  train_loss: 132.6869659423828\n",
      "epoch: 130  train_loss: 130.5024871826172\n",
      "epoch: 131  train_loss: 129.6752471923828\n",
      "epoch: 132  train_loss: 130.22756958007812\n",
      "epoch: 133  train_loss: 130.114501953125\n",
      "epoch: 134  train_loss: 132.40560913085938\n",
      "epoch: 135  train_loss: 131.12258911132812\n",
      "epoch: 136  train_loss: 129.8789520263672\n",
      "epoch: 137  train_loss: 128.11550903320312\n",
      "epoch: 138  train_loss: 130.70938110351562\n",
      "epoch: 139  train_loss: 130.9164276123047\n",
      "epoch: 140  train_loss: 129.6259307861328\n",
      "epoch: 141  train_loss: 131.77110290527344\n",
      "epoch: 142  train_loss: 134.34671020507812\n",
      "epoch: 143  train_loss: 130.2841796875\n",
      "epoch: 144  train_loss: 136.4046630859375\n",
      "epoch: 145  train_loss: 128.40257263183594\n",
      "epoch: 146  train_loss: 132.1670684814453\n",
      "epoch: 147  train_loss: 130.67918395996094\n",
      "epoch: 148  train_loss: 134.85513305664062\n",
      "epoch: 149  train_loss: 134.50682067871094\n",
      "epoch: 150  train_loss: 132.02088928222656\n",
      "epoch: 151  train_loss: 131.18212890625\n",
      "epoch: 152  train_loss: 126.6300277709961\n",
      "epoch: 153  train_loss: 130.86541748046875\n",
      "epoch: 154  train_loss: 133.20887756347656\n",
      "epoch: 155  train_loss: 130.20265197753906\n",
      "epoch: 156  train_loss: 128.56387329101562\n",
      "epoch: 157  train_loss: 131.34527587890625\n",
      "epoch: 158  train_loss: 131.30523681640625\n",
      "epoch: 159  train_loss: 130.075927734375\n",
      "epoch: 160  train_loss: 133.7855682373047\n",
      "epoch: 161  train_loss: 129.03758239746094\n",
      "Early stopping!!!\n"
     ]
    }
   ],
   "source": [
    "# freeze sol mapping\n",
    "#problem_rel.freeze()\n",
    "# training for rounding\n",
    "lr = 0.001    # step size for gradient descent\n",
    "epochs = 400  # number of training epochs\n",
    "warmup = 50   # number of epochs to wait before enacting early stopping policy\n",
    "patience = 50 # number of epochs with no improvement in eval metric to allow before early stopping\n",
    "# set adamW as optimizer\n",
    "optimizer = torch.optim.AdamW(problem.parameters(), lr=lr)\n",
    "# define trainer\n",
    "trainer = nm.trainer.Trainer(problem_rnd, loader_train, loader_dev, loader_test,\n",
    "                             optimizer, epochs=epochs, patience=patience, warmup=warmup)\n",
    "best_model = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7858c207-11d3-4cfc-8e7a-13d831ee7c1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:05<00:00, 184.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           Obj Val  Constraints Viol  Elapsed Time\n",
      "count  1000.000000       1000.000000   1000.000000\n",
      "mean      4.132830          1.526321      0.004759\n",
      "std       2.375297          1.165372      0.001110\n",
      "min       0.000000          0.000000      0.002683\n",
      "25%       3.262399          0.762442      0.004000\n",
      "50%       4.000000          1.475116      0.004543\n",
      "75%       4.147018          2.762442      0.005506\n",
      "max      16.390409          8.475116      0.009948\n"
     ]
    }
   ],
   "source": [
    "sols, objvals, conviols, elapseds = [], [], [], []\n",
    "for p, a in tqdm(list(zip(p_test, a_test))):\n",
    "    datapoints = {\"p\": torch.tensor(np.array([p]), dtype=torch.float32), \"a\":torch.tensor(np.array([a]), dtype=torch.float32), \"name\": \"test\"}\n",
    "    tick = time.time()\n",
    "    output = problem_rnd(datapoints)\n",
    "    tock = time.time()\n",
    "    x = output[\"test_x_rnd\"]\n",
    "    # get values\n",
    "    for ind in model.x:\n",
    "        model.x[ind].value = x[0, ind].item()\n",
    "    xval, objval = model.getVal()\n",
    "    sols.append(xval.values())\n",
    "    objvals.append(objval)\n",
    "    conviols.append(sum(model.calViolation()))\n",
    "    elapseds.append(tock - tick)\n",
    "df = pd.DataFrame({\"Sol\":sols, \"Obj Val\": objvals, \"Constraints Viol\": conviols, \"Elapsed Time\": elapseds})\n",
    "time.sleep(1)\n",
    "print(df.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f140ecf-274f-4dcb-b26e-e76c46ca53e7",
   "metadata": {},
   "source": [
    "### Learning to Feasibility Pump"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ddfd3db5-b61c-46dc-92ae-138ce002fc11",
   "metadata": {},
   "outputs": [],
   "source": [
    "from model.layer import netFC\n",
    "# define neural architecture for the solution mapping\n",
    "sol_func = nm.modules.blocks.MLP(insize=num_vars, outsize=num_vars, bias=True,\n",
    "                                 linear_map=nm.slim.maps[\"linear\"], nonlin=nn.ReLU,\n",
    "                                 hsizes=[80]*4)\n",
    "# define neural architecture for rounding\n",
    "rnd_layer = netFC(input_dim=num_vars*2, hidden_dims=[80]*4, output_dim=num_vars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e9397410-cf75-46de-83b9-d975605b2022",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to get nm optimization model\n",
    "from problem.neural import probRosenbrock\n",
    "def getProb(x, p, a):\n",
    "    return probRosenbrock(x, p, a, num_vars=num_vars, alpha=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2903e087-27af-4738-b1f5-3c30d717ba17",
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import feasibilityPumpModel\n",
    "# feasibility pump model\n",
    "problem_rel, problem_fp = feasibilityPumpModel([\"p\", \"a\"], getProb, sol_func, rnd_layer, int_ind=model.intInd, num_iters=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "57c24ef8-218a-40f0-a448-f0b38851c3f9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0  train_loss: 31.091703414916992\n",
      "epoch: 1  train_loss: 0.5559331774711609\n",
      "epoch: 2  train_loss: 0.4705972969532013\n",
      "epoch: 3  train_loss: 0.45412054657936096\n",
      "epoch: 4  train_loss: 0.4110414981842041\n",
      "epoch: 5  train_loss: 0.40120071172714233\n",
      "epoch: 6  train_loss: 0.42559176683425903\n",
      "epoch: 7  train_loss: 0.42336714267730713\n",
      "epoch: 8  train_loss: 0.3654492199420929\n",
      "epoch: 9  train_loss: 0.36057934165000916\n",
      "epoch: 10  train_loss: 0.3620113134384155\n",
      "epoch: 11  train_loss: 0.4111559987068176\n",
      "epoch: 12  train_loss: 0.42911890149116516\n",
      "epoch: 13  train_loss: 0.4039905369281769\n",
      "epoch: 14  train_loss: 0.391169011592865\n",
      "epoch: 15  train_loss: 0.38851964473724365\n",
      "epoch: 16  train_loss: 0.4309215247631073\n",
      "epoch: 17  train_loss: 0.36927974224090576\n",
      "epoch: 18  train_loss: 0.33142414689064026\n",
      "epoch: 19  train_loss: 0.41081157326698303\n",
      "epoch: 20  train_loss: 0.38458412885665894\n",
      "epoch: 21  train_loss: 0.40109512209892273\n",
      "epoch: 22  train_loss: 0.3996732532978058\n",
      "epoch: 23  train_loss: 0.39662161469459534\n",
      "epoch: 24  train_loss: 0.3038835823535919\n",
      "epoch: 25  train_loss: 0.39601391553878784\n",
      "epoch: 26  train_loss: 0.33228200674057007\n",
      "epoch: 27  train_loss: 0.3561042547225952\n",
      "epoch: 28  train_loss: 0.37729790806770325\n",
      "epoch: 29  train_loss: 0.282764732837677\n",
      "epoch: 30  train_loss: 0.43433985114097595\n",
      "epoch: 31  train_loss: 0.2971831262111664\n",
      "epoch: 32  train_loss: 0.4013814330101013\n",
      "epoch: 33  train_loss: 0.4036645293235779\n",
      "epoch: 34  train_loss: 0.2900491952896118\n",
      "epoch: 35  train_loss: 0.3728424906730652\n",
      "epoch: 36  train_loss: 0.409386545419693\n",
      "epoch: 37  train_loss: 0.354422390460968\n",
      "epoch: 38  train_loss: 0.2849787175655365\n",
      "epoch: 39  train_loss: 0.36679133772850037\n",
      "epoch: 40  train_loss: 0.35248833894729614\n",
      "epoch: 41  train_loss: 0.31790465116500854\n",
      "epoch: 42  train_loss: 0.4188787043094635\n",
      "epoch: 43  train_loss: 0.3420420289039612\n",
      "epoch: 44  train_loss: 0.321027010679245\n",
      "epoch: 45  train_loss: 0.3585531711578369\n",
      "epoch: 46  train_loss: 0.37403085827827454\n",
      "epoch: 47  train_loss: 0.33097437024116516\n",
      "epoch: 48  train_loss: 0.36007726192474365\n",
      "epoch: 49  train_loss: 0.3999781906604767\n",
      "epoch: 50  train_loss: 0.31953540444374084\n",
      "epoch: 51  train_loss: 0.3189969062805176\n",
      "epoch: 52  train_loss: 0.36967065930366516\n",
      "epoch: 53  train_loss: 0.3085063397884369\n",
      "epoch: 54  train_loss: 0.31791582703590393\n",
      "epoch: 55  train_loss: 0.29225221276283264\n",
      "epoch: 56  train_loss: 0.2825200855731964\n",
      "epoch: 57  train_loss: 0.4164673686027527\n",
      "epoch: 58  train_loss: 0.37081313133239746\n",
      "epoch: 59  train_loss: 0.3133235573768616\n",
      "epoch: 60  train_loss: 0.2921094596385956\n",
      "epoch: 61  train_loss: 0.3390296399593353\n",
      "epoch: 62  train_loss: 0.3575192987918854\n",
      "epoch: 63  train_loss: 0.28468427062034607\n",
      "epoch: 64  train_loss: 0.31441348791122437\n",
      "epoch: 65  train_loss: 0.32844507694244385\n",
      "epoch: 66  train_loss: 0.2987695336341858\n",
      "epoch: 67  train_loss: 0.3252376317977905\n",
      "epoch: 68  train_loss: 0.357532262802124\n",
      "epoch: 69  train_loss: 0.3264230489730835\n",
      "epoch: 70  train_loss: 0.308185875415802\n",
      "epoch: 71  train_loss: 0.30968180298805237\n",
      "epoch: 72  train_loss: 0.35756343603134155\n",
      "epoch: 73  train_loss: 0.2885982394218445\n",
      "epoch: 74  train_loss: 0.3578420877456665\n",
      "epoch: 75  train_loss: 0.3149089515209198\n",
      "epoch: 76  train_loss: 0.3424539268016815\n",
      "epoch: 77  train_loss: 0.39669954776763916\n",
      "epoch: 78  train_loss: 0.35115692019462585\n",
      "epoch: 79  train_loss: 0.40430036187171936\n",
      "epoch: 80  train_loss: 0.3812295198440552\n",
      "epoch: 81  train_loss: 0.3720134496688843\n",
      "epoch: 82  train_loss: 0.35705041885375977\n",
      "epoch: 83  train_loss: 0.34207549691200256\n",
      "epoch: 84  train_loss: 0.3343566656112671\n",
      "epoch: 85  train_loss: 0.31037378311157227\n",
      "epoch: 86  train_loss: 0.31073546409606934\n",
      "epoch: 87  train_loss: 0.3265116810798645\n",
      "epoch: 88  train_loss: 0.30699339509010315\n",
      "epoch: 89  train_loss: 0.3107094466686249\n",
      "epoch: 90  train_loss: 0.3380729556083679\n",
      "epoch: 91  train_loss: 0.3081430196762085\n",
      "epoch: 92  train_loss: 0.28657659888267517\n",
      "epoch: 93  train_loss: 0.29053667187690735\n",
      "epoch: 94  train_loss: 0.30031198263168335\n",
      "epoch: 95  train_loss: 0.2817413806915283\n",
      "epoch: 96  train_loss: 0.3420430123806\n",
      "epoch: 97  train_loss: 0.2538430988788605\n",
      "epoch: 98  train_loss: 0.31306153535842896\n",
      "epoch: 99  train_loss: 0.2940772473812103\n",
      "epoch: 100  train_loss: 0.33505481481552124\n",
      "epoch: 101  train_loss: 0.2670643925666809\n",
      "epoch: 102  train_loss: 0.31834128499031067\n",
      "epoch: 103  train_loss: 0.2630831003189087\n",
      "epoch: 104  train_loss: 0.31942206621170044\n",
      "epoch: 105  train_loss: 0.306376188993454\n",
      "epoch: 106  train_loss: 0.31157445907592773\n",
      "epoch: 107  train_loss: 0.32197847962379456\n",
      "epoch: 108  train_loss: 0.32718536257743835\n",
      "epoch: 109  train_loss: 0.3023863136768341\n",
      "epoch: 110  train_loss: 0.32515403628349304\n",
      "epoch: 111  train_loss: 0.31930482387542725\n",
      "epoch: 112  train_loss: 0.33793261647224426\n",
      "epoch: 113  train_loss: 0.3089761734008789\n",
      "epoch: 114  train_loss: 0.3438062071800232\n",
      "epoch: 115  train_loss: 0.30733346939086914\n",
      "epoch: 116  train_loss: 0.30865752696990967\n",
      "epoch: 117  train_loss: 0.3198002278804779\n",
      "epoch: 118  train_loss: 0.35026082396507263\n",
      "epoch: 119  train_loss: 0.2605551779270172\n",
      "epoch: 120  train_loss: 0.3137769103050232\n",
      "epoch: 121  train_loss: 0.3222474455833435\n",
      "epoch: 122  train_loss: 0.35107386112213135\n",
      "epoch: 123  train_loss: 0.3185516595840454\n",
      "epoch: 124  train_loss: 0.3036849796772003\n",
      "epoch: 125  train_loss: 0.2535697817802429\n",
      "epoch: 126  train_loss: 0.2992688715457916\n",
      "epoch: 127  train_loss: 0.3057636618614197\n",
      "epoch: 128  train_loss: 0.3534332513809204\n",
      "epoch: 129  train_loss: 0.3156837522983551\n",
      "epoch: 130  train_loss: 0.2594122886657715\n",
      "epoch: 131  train_loss: 0.29683342576026917\n",
      "epoch: 132  train_loss: 0.3138168752193451\n",
      "epoch: 133  train_loss: 0.2868668735027313\n",
      "epoch: 134  train_loss: 0.30332323908805847\n",
      "epoch: 135  train_loss: 0.2518898844718933\n",
      "epoch: 136  train_loss: 0.3123752772808075\n",
      "epoch: 137  train_loss: 0.2858598828315735\n",
      "epoch: 138  train_loss: 0.2791282534599304\n",
      "epoch: 139  train_loss: 0.3166896402835846\n",
      "epoch: 140  train_loss: 0.30960726737976074\n",
      "epoch: 141  train_loss: 0.29755592346191406\n",
      "epoch: 142  train_loss: 0.3083866238594055\n",
      "epoch: 143  train_loss: 0.3076830804347992\n",
      "epoch: 144  train_loss: 0.29967519640922546\n",
      "Early stopping!!!\n"
     ]
    }
   ],
   "source": [
    "# training for mapping\n",
    "lr = 0.001    # step size for gradient descent\n",
    "epochs = 400  # number of training epochs\n",
    "warmup = 50   # number of epochs to wait before enacting early stopping policy\n",
    "patience = 50 # number of epochs with no improvement in eval metric to allow before early stopping\n",
    "# set adamW as optimizer\n",
    "optimizer = torch.optim.AdamW(problem_rel.parameters(), lr=lr)\n",
    "# define trainer\n",
    "trainer = nm.trainer.Trainer(problem_rel, loader_train, loader_dev, loader_test,\n",
    "                             optimizer, epochs=epochs, patience=patience, warmup=warmup)\n",
    "best_model = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "917856c4-f7db-4537-a59c-1499d0d27557",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0  train_loss: 1209.739990234375\n",
      "epoch: 1  train_loss: 1228.4407958984375\n",
      "epoch: 2  train_loss: 1232.14013671875\n",
      "epoch: 3  train_loss: 1232.2135009765625\n",
      "epoch: 4  train_loss: 1245.966552734375\n",
      "epoch: 5  train_loss: 1247.1795654296875\n",
      "epoch: 6  train_loss: 1243.58154296875\n",
      "epoch: 7  train_loss: 1229.1455078125\n",
      "epoch: 8  train_loss: 1245.62939453125\n",
      "epoch: 9  train_loss: 1240.62353515625\n",
      "epoch: 10  train_loss: 1243.5201416015625\n",
      "epoch: 11  train_loss: 1233.32275390625\n",
      "epoch: 12  train_loss: 1256.79052734375\n",
      "epoch: 13  train_loss: 1238.919189453125\n",
      "epoch: 14  train_loss: 1233.199951171875\n",
      "epoch: 15  train_loss: 1242.6917724609375\n",
      "epoch: 16  train_loss: 1235.55419921875\n",
      "epoch: 17  train_loss: 1234.8634033203125\n",
      "epoch: 18  train_loss: 1246.98388671875\n",
      "epoch: 19  train_loss: 1227.5555419921875\n",
      "epoch: 20  train_loss: 1229.7686767578125\n",
      "epoch: 21  train_loss: 1232.7969970703125\n",
      "epoch: 22  train_loss: 1232.2926025390625\n",
      "epoch: 23  train_loss: 1253.1356201171875\n",
      "epoch: 24  train_loss: 1231.621826171875\n",
      "epoch: 25  train_loss: 1241.338134765625\n",
      "epoch: 26  train_loss: 1248.0069580078125\n",
      "epoch: 27  train_loss: 1253.6492919921875\n",
      "epoch: 28  train_loss: 1232.10693359375\n",
      "epoch: 29  train_loss: 1235.4808349609375\n",
      "epoch: 30  train_loss: 1227.90869140625\n",
      "epoch: 31  train_loss: 1251.1910400390625\n",
      "epoch: 32  train_loss: 1231.345458984375\n",
      "epoch: 33  train_loss: 1236.43603515625\n",
      "epoch: 34  train_loss: 1232.270751953125\n",
      "epoch: 35  train_loss: 1248.0980224609375\n",
      "epoch: 36  train_loss: 1229.802734375\n",
      "epoch: 37  train_loss: 1245.33349609375\n",
      "epoch: 38  train_loss: 1243.283447265625\n",
      "epoch: 39  train_loss: 1225.942138671875\n",
      "epoch: 40  train_loss: 1233.5106201171875\n",
      "epoch: 41  train_loss: 1249.3426513671875\n",
      "epoch: 42  train_loss: 1233.326171875\n",
      "epoch: 43  train_loss: 1249.1419677734375\n",
      "epoch: 44  train_loss: 1228.0247802734375\n",
      "epoch: 45  train_loss: 1239.0428466796875\n",
      "epoch: 46  train_loss: 1225.608642578125\n",
      "epoch: 47  train_loss: 1233.0946044921875\n",
      "epoch: 48  train_loss: 1240.2274169921875\n",
      "epoch: 49  train_loss: 1232.417236328125\n",
      "epoch: 50  train_loss: 1243.8607177734375\n",
      "epoch: 51  train_loss: 1230.9183349609375\n",
      "epoch: 52  train_loss: 1241.907470703125\n",
      "epoch: 53  train_loss: 1245.413818359375\n",
      "epoch: 54  train_loss: 1230.7008056640625\n",
      "epoch: 55  train_loss: 1234.7843017578125\n",
      "epoch: 56  train_loss: 1225.215087890625\n",
      "epoch: 57  train_loss: 1246.4068603515625\n",
      "epoch: 58  train_loss: 1235.96337890625\n",
      "epoch: 59  train_loss: 1240.19384765625\n",
      "epoch: 60  train_loss: 1244.615966796875\n",
      "epoch: 61  train_loss: 1222.0865478515625\n",
      "epoch: 62  train_loss: 1233.93115234375\n",
      "epoch: 63  train_loss: 1259.099365234375\n",
      "epoch: 64  train_loss: 1223.1214599609375\n",
      "epoch: 65  train_loss: 1244.2490234375\n",
      "epoch: 66  train_loss: 1233.017578125\n",
      "epoch: 67  train_loss: 1241.7708740234375\n",
      "epoch: 68  train_loss: 1232.4091796875\n",
      "epoch: 69  train_loss: 1225.886474609375\n",
      "epoch: 70  train_loss: 1257.849609375\n",
      "epoch: 71  train_loss: 1245.7581787109375\n",
      "epoch: 72  train_loss: 1251.4942626953125\n",
      "epoch: 73  train_loss: 1234.2117919921875\n",
      "epoch: 74  train_loss: 1240.9464111328125\n",
      "epoch: 75  train_loss: 1225.041015625\n",
      "epoch: 76  train_loss: 1231.4842529296875\n",
      "epoch: 77  train_loss: 1246.3876953125\n",
      "epoch: 78  train_loss: 1233.467041015625\n",
      "epoch: 79  train_loss: 1234.615478515625\n",
      "epoch: 80  train_loss: 1244.80322265625\n",
      "epoch: 81  train_loss: 1235.705810546875\n",
      "epoch: 82  train_loss: 1235.9189453125\n",
      "epoch: 83  train_loss: 1247.3477783203125\n",
      "epoch: 84  train_loss: 1240.6334228515625\n",
      "epoch: 85  train_loss: 1242.7637939453125\n",
      "epoch: 86  train_loss: 1235.4056396484375\n",
      "epoch: 87  train_loss: 1249.2069091796875\n",
      "epoch: 88  train_loss: 1242.548583984375\n",
      "epoch: 89  train_loss: 1242.263916015625\n",
      "epoch: 90  train_loss: 1235.8466796875\n",
      "epoch: 91  train_loss: 1225.3160400390625\n",
      "epoch: 92  train_loss: 1247.771240234375\n",
      "epoch: 93  train_loss: 1237.4573974609375\n",
      "epoch: 94  train_loss: 1233.3248291015625\n",
      "epoch: 95  train_loss: 1245.8773193359375\n",
      "epoch: 96  train_loss: 1230.2188720703125\n",
      "epoch: 97  train_loss: 1226.645263671875\n",
      "epoch: 98  train_loss: 1223.134521484375\n",
      "epoch: 99  train_loss: 1230.439208984375\n",
      "epoch: 100  train_loss: 1230.5516357421875\n",
      "epoch: 101  train_loss: 1237.78759765625\n",
      "Early stopping!!!\n"
     ]
    }
   ],
   "source": [
    "# freeze sol mapping\n",
    "#problem_rel.freeze()\n",
    "# training for feasibility pump\n",
    "lr = 0.001    # step size for gradient descent\n",
    "epochs = 400  # number of training epochs\n",
    "warmup = 50   # number of epochs to wait before enacting early stopping policy\n",
    "patience = 50 # number of epochs with no improvement in eval metric to allow before early stopping\n",
    "# set adamW as optimizer\n",
    "optimizer = torch.optim.AdamW(problem.parameters(), lr=lr)\n",
    "# define trainer\n",
    "trainer = nm.trainer.Trainer(problem_fp, loader_train, loader_dev, loader_test,\n",
    "                             optimizer, epochs=epochs, patience=patience, warmup=warmup)\n",
    "best_model = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "cb8b7b8b-96db-47cc-8140-91f75aa2f717",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:05<00:00, 179.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           Obj Val  Constraints Viol  Elapsed Time\n",
      "count  1000.000000       1000.000000   1000.000000\n",
      "mean      6.831584          1.355442      0.004911\n",
      "std       1.731548          0.497593      0.001176\n",
      "min       3.246814          0.762442      0.002025\n",
      "25%       7.262399          0.762442      0.004014\n",
      "50%       7.262399          1.762442      0.004664\n",
      "75%       8.015585          1.762442      0.005669\n",
      "max       8.015585          2.762442      0.010804\n"
     ]
    }
   ],
   "source": [
    "sols, objvals, conviols, elapseds = [], [], [], []\n",
    "for p, a in tqdm(list(zip(p_test, a_test))):\n",
    "    datapoints = {\"p\": torch.tensor(np.array([p]), dtype=torch.float32), \"a\":torch.tensor(np.array([a]), dtype=torch.float32), \"name\": \"test\"}\n",
    "    tick = time.time()\n",
    "    output = problem(datapoints)\n",
    "    tock = time.time()\n",
    "    x = output[\"test_x_rnd\"]\n",
    "    # get values\n",
    "    for ind in model.x:\n",
    "        model.x[ind].value = x[0, ind].item()\n",
    "    xval, objval = model.getVal()\n",
    "    sols.append(xval.values())\n",
    "    objvals.append(objval)\n",
    "    conviols.append(sum(model.calViolation()))\n",
    "    elapseds.append(tock - tick)\n",
    "df = pd.DataFrame({\"Sol\":sols, \"Obj Val\": objvals, \"Constraints Viol\": conviols, \"Elapsed Time\": elapseds})\n",
    "time.sleep(1)\n",
    "print(df.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d761fd9-ab80-4f5a-80ec-cfafb9539b39",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
