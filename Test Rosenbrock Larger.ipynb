{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "be61b914-e1af-43b9-9766-5092593f92dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch import nn\n",
    "from tqdm import tqdm\n",
    "\n",
    "# random seed\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "torch.cuda.manual_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "35fe4527-9dbf-48f0-bfb3-62cda59092aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# turn off warning\n",
    "import logging\n",
    "logging.getLogger('pyomo.core').setLevel(logging.ERROR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c72036c-cee6-49fe-8aae-ae979edfe232",
   "metadata": {},
   "source": [
    "## 5 Blocks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08e77546-552e-4802-b9d5-4655ff4b4842",
   "metadata": {},
   "source": [
    "### Problem Setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cf6d3b59-8894-4e2f-befa-d27b012f0138",
   "metadata": {},
   "outputs": [],
   "source": [
    "# init\n",
    "steepness = 50    # steepness factor\n",
    "num_blocks = 5    # number of expression blocks\n",
    "num_data = 5000   # number of data\n",
    "test_size = 1000  # number of test size\n",
    "val_size = 1000   # number of validation size\n",
    "train_size = num_data - test_size - val_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3386c29d-e160-4244-b844-120317242b63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters as input data\n",
    "p_low, p_high = 1.0, 8.0\n",
    "a_low, a_high = 0.5, 4.5\n",
    "p_train = np.random.uniform(p_low, p_high, (train_size, 1)).astype(np.float32)\n",
    "p_test  = np.random.uniform(p_low, p_high, (test_size, 1)).astype(np.float32)\n",
    "p_dev   = np.random.uniform(p_low, p_high, (val_size, 1)).astype(np.float32)\n",
    "a_train = np.random.uniform(a_low, a_high, (train_size, num_blocks)).astype(np.float32)\n",
    "a_test  = np.random.uniform(a_low, a_high, (test_size, num_blocks)).astype(np.float32)\n",
    "a_dev   = np.random.uniform(a_low, a_high, (val_size, num_blocks)).astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d0aa9659-0234-47e7-aa41-681c7dc12af2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nm datasets\n",
    "from neuromancer.dataset import DictDataset\n",
    "data_train = DictDataset({\"p\":p_train, \"a\":a_train}, name=\"train\")\n",
    "data_test = DictDataset({\"p\":p_test, \"a\":a_test}, name=\"test\")\n",
    "data_dev = DictDataset({\"p\":p_dev, \"a\":a_dev}, name=\"dev\")\n",
    "# torch dataloaders\n",
    "from torch.utils.data import DataLoader\n",
    "loader_train = DataLoader(data_train, batch_size=32, num_workers=0, collate_fn=data_train.collate_fn, shuffle=True)\n",
    "loader_test = DataLoader(data_test, batch_size=32, num_workers=0, collate_fn=data_test.collate_fn, shuffle=False)\n",
    "loader_dev = DataLoader(data_dev, batch_size=32, num_workers=0, collate_fn=data_dev.collate_fn, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91b8d9df-8379-4875-9bb9-734993571ac5",
   "metadata": {},
   "source": [
    "### Exact Solver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "700bd06c-7b68-49c8-8ecf-5e9c19b2909d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# exact Solver\n",
    "from src.problem import msRosenbrock\n",
    "model = msRosenbrock(steepness, num_blocks, timelimit=60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f606e66-d1a5-4192-a355-5fb78ba24858",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6b5b9a41-56cd-4971-9b8c-bf127e9a9921",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "penalty_weight = 50   # weight of constraint violation penealty\n",
    "hlayers_sol = 4       # number of hidden layers for solution mapping\n",
    "hlayers_rnd = 4       # number of hidden layers for solution mapping\n",
    "hsize = 64            # width of hidden layers for solution mapping\n",
    "lr = 1e-3             # learning rate\n",
    "batch_size = 64       # batch size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4eb51375-06f4-4b7e-8127-1bc172a91c3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set problem\n",
    "import neuromancer as nm\n",
    "from src.problem import nmRosenbrock\n",
    "from src.func.layer import netFC\n",
    "from src.func import roundGumbelModel\n",
    "# define Rosenbrock objective functions and constraints for both problem types\n",
    "obj_rel, constrs_rel = nmRosenbrock([\"x\"], [\"p\", \"a\"], steepness, num_blocks, penalty_weight=penalty_weight)\n",
    "obj_rnd, constrs_rnd = nmRosenbrock([\"x_rnd\"], [\"p\", \"a\"], steepness, num_blocks, penalty_weight=penalty_weight)\n",
    "# build neural architecture for the solution map\n",
    "func = nm.modules.blocks.MLP(insize=num_blocks+1, outsize=2*num_blocks, bias=True,\n",
    "                             linear_map=nm.slim.maps[\"linear\"],\n",
    "                             nonlin=nn.ReLU, hsizes=[hsize]*hlayers_sol)\n",
    "smap = nm.system.Node(func, [\"p\", \"a\"], [\"x\"], name=\"smap\")\n",
    "# define rounding model\n",
    "layers_rnd = netFC(input_dim=3*num_blocks+1, hidden_dims=[hsize]*hlayers_rnd, output_dim=2*num_blocks)\n",
    "rnd = roundGumbelModel(layers=layers_rnd, param_keys=[\"p\", \"a\"], var_keys=[\"x\"],  output_keys=[\"x_rnd\"], \n",
    "                       int_ind=model.int_ind, continuous_update=True, name=\"round\")\n",
    "# build neuromancer problem for rounding\n",
    "components = [smap, rnd]\n",
    "loss = nm.loss.PenaltyLoss(obj_rnd, constrs_rnd)\n",
    "problem = nm.problem.Problem(components, loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e02aae6e-0bad-4ef0-8364-7cfc9739db99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0  train_loss: 553.1668090820312\n",
      "epoch: 1  train_loss: 260.5215759277344\n",
      "epoch: 2  train_loss: 147.50978088378906\n",
      "epoch: 3  train_loss: 116.77005767822266\n",
      "epoch: 4  train_loss: 101.89723205566406\n",
      "epoch: 5  train_loss: 94.35205078125\n",
      "epoch: 6  train_loss: 83.77298736572266\n",
      "epoch: 7  train_loss: 78.71440124511719\n",
      "epoch: 8  train_loss: 70.5364990234375\n",
      "epoch: 9  train_loss: 65.93121337890625\n",
      "epoch: 10  train_loss: 60.96925354003906\n",
      "epoch: 11  train_loss: 59.60026550292969\n",
      "epoch: 12  train_loss: 56.82996368408203\n",
      "epoch: 13  train_loss: 52.19192123413086\n",
      "epoch: 14  train_loss: 49.57029724121094\n",
      "epoch: 15  train_loss: 48.0003776550293\n",
      "epoch: 16  train_loss: 44.070594787597656\n",
      "epoch: 17  train_loss: 43.590675354003906\n",
      "epoch: 18  train_loss: 45.1284065246582\n",
      "epoch: 19  train_loss: 40.563358306884766\n",
      "epoch: 20  train_loss: 41.99279022216797\n",
      "epoch: 21  train_loss: 40.12728500366211\n",
      "epoch: 22  train_loss: 38.03464126586914\n",
      "epoch: 23  train_loss: 38.42106246948242\n",
      "epoch: 24  train_loss: 37.61738586425781\n",
      "epoch: 25  train_loss: 38.11042404174805\n",
      "epoch: 26  train_loss: 37.887977600097656\n",
      "epoch: 27  train_loss: 39.56560516357422\n",
      "epoch: 28  train_loss: 38.35813903808594\n",
      "epoch: 29  train_loss: 37.327491760253906\n",
      "epoch: 30  train_loss: 38.149658203125\n",
      "epoch: 31  train_loss: 36.33699035644531\n",
      "epoch: 32  train_loss: 37.12718200683594\n",
      "epoch: 33  train_loss: 39.38103485107422\n",
      "epoch: 34  train_loss: 37.955631256103516\n",
      "epoch: 35  train_loss: 38.47527313232422\n",
      "epoch: 36  train_loss: 38.29032897949219\n",
      "epoch: 37  train_loss: 37.997318267822266\n",
      "epoch: 38  train_loss: 35.9854621887207\n",
      "epoch: 39  train_loss: 38.96800994873047\n",
      "epoch: 40  train_loss: 36.8239860534668\n",
      "epoch: 41  train_loss: 36.877384185791016\n",
      "epoch: 42  train_loss: 37.43653869628906\n",
      "epoch: 43  train_loss: 36.07297897338867\n",
      "epoch: 44  train_loss: 37.290611267089844\n",
      "epoch: 45  train_loss: 35.718833923339844\n",
      "epoch: 46  train_loss: 36.310970306396484\n",
      "epoch: 47  train_loss: 36.940792083740234\n",
      "epoch: 48  train_loss: 34.99641036987305\n",
      "Early stopping!!!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# training\n",
    "epochs = 200                    # number of training epochs\n",
    "warmup = 20                     # number of epochs to wait before enacting early stopping policy\n",
    "patience = 20                   # number of epochs with no improvement in eval metric to allow before early stopping\n",
    "optimizer = torch.optim.Adam(problem.parameters(), lr=lr)\n",
    "# create a trainer for the problem\n",
    "trainer = nm.trainer.Trainer(problem, loader_train, loader_dev, loader_test, optimizer, \n",
    "                            epochs=epochs, patience=patience, warmup=warmup)\n",
    "# training for the rounding problem\n",
    "best_model = trainer.train()\n",
    "# load best model dict\n",
    "problem.load_state_dict(best_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6af916ea-8c69-4643-861e-005800740486",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:05<00:00, 198.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           Obj Val  Constraints Viol  Elapsed Time\n",
      "count  1000.000000       1000.000000   1000.000000\n",
      "mean     28.751036          0.001681      0.004243\n",
      "std      13.638642          0.026606      0.001844\n",
      "min       5.071291          0.000000      0.001999\n",
      "25%      19.060152          0.000000      0.003049\n",
      "50%      27.050896          0.000000      0.004000\n",
      "75%      34.797640          0.000000      0.005022\n",
      "max     134.879930          0.708861      0.044728\n",
      "Number of infeasible solution: 8\n"
     ]
    }
   ],
   "source": [
    "params, sols, objvals, conviols, elapseds = [], [], [], [], []\n",
    "for p, a in tqdm(list(zip(p_test,a_test))):\n",
    "    # data point as tensor\n",
    "    datapoints = {\"p\": torch.tensor(np.array([p]), dtype=torch.float32), \n",
    "                  \"a\": torch.tensor(np.array([a]), dtype=torch.float32),\n",
    "                  \"name\": \"test\"}\n",
    "    # infer\n",
    "    tick = time.time()\n",
    "    output = problem(datapoints)\n",
    "    tock = time.time()\n",
    "    # assign params\n",
    "    model.set_param_val({\"p\":p, \"a\":a})\n",
    "    # assign vars\n",
    "    x = output[\"test_x_rnd\"]\n",
    "    for i in range(2*num_blocks):\n",
    "        model.vars[\"x\"][i].value = x[0,i].item()\n",
    "    # get solutions\n",
    "    xval, objval = model.get_val()    \n",
    "    params.append(list(p)+list(a))\n",
    "    sols.append(list(list(xval.values())[0].values()))\n",
    "    objvals.append(objval)\n",
    "    conviols.append(sum(model.cal_violation()))\n",
    "    elapseds.append(tock - tick)\n",
    "df = pd.DataFrame({\"Param\":params, \"Sol\":sols, \"Obj Val\": objvals, \"Constraints Viol\": conviols, \"Elapsed Time\": elapseds})\n",
    "time.sleep(1)\n",
    "print(df.describe())\n",
    "print(\"Number of infeasible solution: {}\".format(np.sum(df[\"Constraints Viol\"] > 0)))\n",
    "df.to_csv(\"result/rb_nm_50-5.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "043018ee-1684-4401-8e4f-19a829d03bde",
   "metadata": {},
   "source": [
    "## 10 Blocks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5560447-da9d-4cbe-aab4-538aa11c8ee8",
   "metadata": {},
   "source": [
    "### Problem Setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e64d99f3-f37b-44a9-928d-3e31665671a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# init\n",
    "steepness = 50    # steepness factor\n",
    "num_blocks = 10   # number of expression blocks\n",
    "num_data = 5000   # number of data\n",
    "test_size = 1000  # number of test size\n",
    "val_size = 1000   # number of validation size\n",
    "train_size = num_data - test_size - val_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8192ba9a-e365-406b-be6a-9b03e2824113",
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters as input data\n",
    "p_low, p_high = 1.0, 8.0\n",
    "a_low, a_high = 0.5, 4.5\n",
    "p_train = np.random.uniform(p_low, p_high, (train_size, 1)).astype(np.float32)\n",
    "p_test  = np.random.uniform(p_low, p_high, (test_size, 1)).astype(np.float32)\n",
    "p_dev   = np.random.uniform(p_low, p_high, (val_size, 1)).astype(np.float32)\n",
    "a_train = np.random.uniform(a_low, a_high, (train_size, num_blocks)).astype(np.float32)\n",
    "a_test  = np.random.uniform(a_low, a_high, (test_size, num_blocks)).astype(np.float32)\n",
    "a_dev   = np.random.uniform(a_low, a_high, (val_size, num_blocks)).astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5221c7b5-ce11-43bc-9f57-7b8f03874926",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nm datasets\n",
    "from neuromancer.dataset import DictDataset\n",
    "data_train = DictDataset({\"p\":p_train, \"a\":a_train}, name=\"train\")\n",
    "data_test = DictDataset({\"p\":p_test, \"a\":a_test}, name=\"test\")\n",
    "data_dev = DictDataset({\"p\":p_dev, \"a\":a_dev}, name=\"dev\")\n",
    "# torch dataloaders\n",
    "from torch.utils.data import DataLoader\n",
    "loader_train = DataLoader(data_train, batch_size=32, num_workers=0, collate_fn=data_train.collate_fn, shuffle=True)\n",
    "loader_test = DataLoader(data_test, batch_size=32, num_workers=0, collate_fn=data_test.collate_fn, shuffle=False)\n",
    "loader_dev = DataLoader(data_dev, batch_size=32, num_workers=0, collate_fn=data_dev.collate_fn, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64313e5f-9f65-4f55-8df6-5c8e800d1cc5",
   "metadata": {},
   "source": [
    "### Exact Solver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3c691f57-0d66-45d2-b190-8066ed8d30e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# exact Solver\n",
    "from src.problem import msRosenbrock\n",
    "model = msRosenbrock(steepness, num_blocks, timelimit=60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6cc85b2-bfb7-49b9-8a29-3ea3864082b9",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "82e38a9b-b0d3-4780-b555-0dd489c93c89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "penalty_weight = 50   # weight of constraint violation penealty\n",
    "hlayers_sol = 4       # number of hidden layers for solution mapping\n",
    "hlayers_rnd = 4       # number of hidden layers for solution mapping\n",
    "hsize = 64            # width of hidden layers for solution mapping\n",
    "lr = 1e-3             # learning rate\n",
    "batch_size = 64       # batch size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "21808f1d-2d4a-4078-8b3e-25fba4db11de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set problem\n",
    "import neuromancer as nm\n",
    "from src.problem import nmRosenbrock\n",
    "from src.func.layer import netFC\n",
    "from src.func import roundGumbelModel\n",
    "# define Rosenbrock objective functions and constraints for both problem types\n",
    "obj_rel, constrs_rel = nmRosenbrock([\"x\"], [\"p\", \"a\"], steepness, num_blocks, penalty_weight=penalty_weight)\n",
    "obj_rnd, constrs_rnd = nmRosenbrock([\"x_rnd\"], [\"p\", \"a\"], steepness, num_blocks, penalty_weight=penalty_weight)\n",
    "# build neural architecture for the solution map\n",
    "func = nm.modules.blocks.MLP(insize=num_blocks+1, outsize=2*num_blocks, bias=True,\n",
    "                             linear_map=nm.slim.maps[\"linear\"],\n",
    "                             nonlin=nn.ReLU, hsizes=[hsize]*hlayers_sol)\n",
    "smap = nm.system.Node(func, [\"p\", \"a\"], [\"x\"], name=\"smap\")\n",
    "# define rounding model\n",
    "layers_rnd = netFC(input_dim=3*num_blocks+1, hidden_dims=[hsize]*hlayers_rnd, output_dim=2*num_blocks)\n",
    "rnd = roundGumbelModel(layers=layers_rnd, param_keys=[\"p\", \"a\"], var_keys=[\"x\"],  output_keys=[\"x_rnd\"], \n",
    "                       int_ind=model.int_ind, continuous_update=True, name=\"round\")\n",
    "# build neuromancer problem for rounding\n",
    "components = [smap, rnd]\n",
    "loss = nm.loss.PenaltyLoss(obj_rnd, constrs_rnd)\n",
    "problem = nm.problem.Problem(components, loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "08dc94f6-e0c0-40a0-aae1-b533039d1adc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0  train_loss: 1144.11572265625\n",
      "epoch: 1  train_loss: 573.7198486328125\n",
      "epoch: 2  train_loss: 340.5039978027344\n",
      "epoch: 3  train_loss: 279.9085693359375\n",
      "epoch: 4  train_loss: 255.43014526367188\n",
      "epoch: 5  train_loss: 227.49249267578125\n",
      "epoch: 6  train_loss: 211.65391540527344\n",
      "epoch: 7  train_loss: 196.94802856445312\n",
      "epoch: 8  train_loss: 183.49827575683594\n",
      "epoch: 9  train_loss: 169.12628173828125\n",
      "epoch: 10  train_loss: 147.68006896972656\n",
      "epoch: 11  train_loss: 133.78614807128906\n",
      "epoch: 12  train_loss: 125.62577056884766\n",
      "epoch: 13  train_loss: 114.93440246582031\n",
      "epoch: 14  train_loss: 103.63280487060547\n",
      "epoch: 15  train_loss: 101.2866439819336\n",
      "epoch: 16  train_loss: 99.28314971923828\n",
      "epoch: 17  train_loss: 90.1810073852539\n",
      "epoch: 18  train_loss: 91.22042083740234\n",
      "epoch: 19  train_loss: 89.95957946777344\n",
      "epoch: 20  train_loss: 82.39407348632812\n",
      "epoch: 21  train_loss: 81.68199920654297\n",
      "epoch: 22  train_loss: 79.2601318359375\n",
      "epoch: 23  train_loss: 78.49829864501953\n",
      "epoch: 24  train_loss: 75.84291076660156\n",
      "epoch: 25  train_loss: 77.51225280761719\n",
      "epoch: 26  train_loss: 77.76033782958984\n",
      "epoch: 27  train_loss: 70.8369369506836\n",
      "epoch: 28  train_loss: 74.06941223144531\n",
      "epoch: 29  train_loss: 77.26971435546875\n",
      "epoch: 30  train_loss: 74.5498275756836\n",
      "epoch: 31  train_loss: 72.72711181640625\n",
      "epoch: 32  train_loss: 74.89034271240234\n",
      "epoch: 33  train_loss: 74.46322631835938\n",
      "epoch: 34  train_loss: 70.6489486694336\n",
      "epoch: 35  train_loss: 71.37190246582031\n",
      "epoch: 36  train_loss: 72.25714874267578\n",
      "epoch: 37  train_loss: 70.69880676269531\n",
      "epoch: 38  train_loss: 69.38774871826172\n",
      "epoch: 39  train_loss: 68.68807983398438\n",
      "epoch: 40  train_loss: 69.16293334960938\n",
      "epoch: 41  train_loss: 69.85272979736328\n",
      "epoch: 42  train_loss: 67.49368286132812\n",
      "epoch: 43  train_loss: 72.42424011230469\n",
      "epoch: 44  train_loss: 68.2743911743164\n",
      "epoch: 45  train_loss: 67.76718139648438\n",
      "epoch: 46  train_loss: 69.86750030517578\n",
      "epoch: 47  train_loss: 70.7736587524414\n",
      "epoch: 48  train_loss: 71.53392028808594\n",
      "epoch: 49  train_loss: 66.46250915527344\n",
      "epoch: 50  train_loss: 72.09133911132812\n",
      "epoch: 51  train_loss: 78.05604553222656\n",
      "epoch: 52  train_loss: 70.22085571289062\n",
      "epoch: 53  train_loss: 73.59925079345703\n",
      "epoch: 54  train_loss: 71.38330078125\n",
      "epoch: 55  train_loss: 64.22868347167969\n",
      "epoch: 56  train_loss: 66.08055877685547\n",
      "epoch: 57  train_loss: 73.28367614746094\n",
      "epoch: 58  train_loss: 67.74271392822266\n",
      "epoch: 59  train_loss: 66.0115966796875\n",
      "epoch: 60  train_loss: 75.99320220947266\n",
      "epoch: 61  train_loss: 72.86568450927734\n",
      "epoch: 62  train_loss: 73.53193664550781\n",
      "epoch: 63  train_loss: 68.9621353149414\n",
      "epoch: 64  train_loss: 73.75480651855469\n",
      "epoch: 65  train_loss: 69.88123321533203\n",
      "epoch: 66  train_loss: 69.06725311279297\n",
      "epoch: 67  train_loss: 67.01768493652344\n",
      "epoch: 68  train_loss: 69.08192443847656\n",
      "epoch: 69  train_loss: 73.55159759521484\n",
      "epoch: 70  train_loss: 75.05864715576172\n",
      "epoch: 71  train_loss: 69.63080596923828\n",
      "epoch: 72  train_loss: 72.92523193359375\n",
      "epoch: 73  train_loss: 69.72484588623047\n",
      "epoch: 74  train_loss: 68.09953308105469\n",
      "epoch: 75  train_loss: 70.38656616210938\n",
      "epoch: 76  train_loss: 67.8567123413086\n",
      "Early stopping!!!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# training\n",
    "epochs = 200                    # number of training epochs\n",
    "warmup = 20                     # number of epochs to wait before enacting early stopping policy\n",
    "patience = 20                   # number of epochs with no improvement in eval metric to allow before early stopping\n",
    "optimizer = torch.optim.Adam(problem.parameters(), lr=lr)\n",
    "# create a trainer for the problem\n",
    "trainer = nm.trainer.Trainer(problem, loader_train, loader_dev, loader_test, optimizer, \n",
    "                            epochs=epochs, patience=patience, warmup=warmup)\n",
    "# training for the rounding problem\n",
    "best_model = trainer.train()\n",
    "# load best model dict\n",
    "problem.load_state_dict(best_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9da1f078-28b4-4c57-b362-182fcb147f01",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:06<00:00, 150.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           Obj Val  Constraints Viol  Elapsed Time\n",
      "count  1000.000000       1000.000000   1000.000000\n",
      "mean     49.837315          0.002738      0.005646\n",
      "std      23.470743          0.054902      0.001954\n",
      "min       9.111755          0.000000      0.003000\n",
      "25%      34.851719          0.000000      0.004504\n",
      "50%      44.339315          0.000000      0.005000\n",
      "75%      58.335393          0.000000      0.006293\n",
      "max     183.667921          1.492963      0.012596\n",
      "Number of infeasible solution: 4\n"
     ]
    }
   ],
   "source": [
    "params, sols, objvals, conviols, elapseds = [], [], [], [], []\n",
    "for p, a in tqdm(list(zip(p_test,a_test))):\n",
    "    # data point as tensor\n",
    "    datapoints = {\"p\": torch.tensor(np.array([p]), dtype=torch.float32), \n",
    "                  \"a\": torch.tensor(np.array([a]), dtype=torch.float32),\n",
    "                  \"name\": \"test\"}\n",
    "    # infer\n",
    "    tick = time.time()\n",
    "    output = problem(datapoints)\n",
    "    tock = time.time()\n",
    "    # assign params\n",
    "    model.set_param_val({\"p\":p, \"a\":a})\n",
    "    # assign vars\n",
    "    x = output[\"test_x_rnd\"]\n",
    "    for i in range(2*num_blocks):\n",
    "        model.vars[\"x\"][i].value = x[0,i].item()\n",
    "    # get solutions\n",
    "    xval, objval = model.get_val()    \n",
    "    params.append(list(p))\n",
    "    sols.append(list(list(xval.values())[0].values()))\n",
    "    objvals.append(objval)\n",
    "    conviols.append(sum(model.cal_violation()))\n",
    "    elapseds.append(tock - tick)\n",
    "df = pd.DataFrame({\"Param\":params, \"Sol\":sols, \"Obj Val\": objvals, \"Constraints Viol\": conviols, \"Elapsed Time\": elapseds})\n",
    "time.sleep(1)\n",
    "print(df.describe())\n",
    "print(\"Number of infeasible solution: {}\".format(np.sum(df[\"Constraints Viol\"] > 0)))\n",
    "df.to_csv(\"result/rb_nm_50-10.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bc68f09-170f-4229-852b-4fd8a7708a05",
   "metadata": {},
   "source": [
    "## 15 Blocks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1108d97f-f031-4406-b000-9e8fc95ab5b6",
   "metadata": {},
   "source": [
    "### Problem Setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0f93ed99-f7f9-4c71-be60-1b187e9ac693",
   "metadata": {},
   "outputs": [],
   "source": [
    "# init\n",
    "steepness = 50    # steepness factor\n",
    "num_blocks = 15    # number of expression blocks\n",
    "num_data = 5000   # number of data\n",
    "test_size = 1000  # number of test size\n",
    "val_size = 1000   # number of validation size\n",
    "train_size = num_data - test_size - val_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "30e47cad-7360-47f1-9dde-5498cf524b38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters as input data\n",
    "p_low, p_high = 1.0, 8.0\n",
    "a_low, a_high = 0.5, 4.5\n",
    "p_train = np.random.uniform(p_low, p_high, (train_size, 1)).astype(np.float32)\n",
    "p_test  = np.random.uniform(p_low, p_high, (test_size, 1)).astype(np.float32)\n",
    "p_dev   = np.random.uniform(p_low, p_high, (val_size, 1)).astype(np.float32)\n",
    "a_train = np.random.uniform(a_low, a_high, (train_size, num_blocks)).astype(np.float32)\n",
    "a_test  = np.random.uniform(a_low, a_high, (test_size, num_blocks)).astype(np.float32)\n",
    "a_dev   = np.random.uniform(a_low, a_high, (val_size, num_blocks)).astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "20c5282b-9c00-4dc7-a2ef-553ef0cbe17a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nm datasets\n",
    "from neuromancer.dataset import DictDataset\n",
    "data_train = DictDataset({\"p\":p_train, \"a\":a_train}, name=\"train\")\n",
    "data_test = DictDataset({\"p\":p_test, \"a\":a_test}, name=\"test\")\n",
    "data_dev = DictDataset({\"p\":p_dev, \"a\":a_dev}, name=\"dev\")\n",
    "# torch dataloaders\n",
    "from torch.utils.data import DataLoader\n",
    "loader_train = DataLoader(data_train, batch_size=32, num_workers=0, collate_fn=data_train.collate_fn, shuffle=True)\n",
    "loader_test = DataLoader(data_test, batch_size=32, num_workers=0, collate_fn=data_test.collate_fn, shuffle=False)\n",
    "loader_dev = DataLoader(data_dev, batch_size=32, num_workers=0, collate_fn=data_dev.collate_fn, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0797544-b044-4169-adfc-10e6759def01",
   "metadata": {},
   "source": [
    "### Exact Solver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8f4d5c84-990f-4161-9809-7451951b9cf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# exact Solver\n",
    "from src.problem import msRosenbrock\n",
    "model = msRosenbrock(steepness, num_blocks, timelimit=60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5403f385-6fc7-4f84-baf5-160c2390b6a3",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a145cc58-6382-4ce0-9c50-33f80a6bd88d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "penalty_weight = 50   # weight of constraint violation penealty\n",
    "hlayers_sol = 4       # number of hidden layers for solution mapping\n",
    "hlayers_rnd = 4       # number of hidden layers for solution mapping\n",
    "hsize = 64            # width of hidden layers for solution mapping\n",
    "lr = 1e-3             # learning rate\n",
    "batch_size = 64       # batch size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1261f74e-12c9-4dd3-b250-8df9bcf9fabf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set problem\n",
    "import neuromancer as nm\n",
    "from src.problem import nmRosenbrock\n",
    "from src.func.layer import netFC\n",
    "from src.func import roundGumbelModel\n",
    "# define Rosenbrock objective functions and constraints for both problem types\n",
    "obj_rel, constrs_rel = nmRosenbrock([\"x\"], [\"p\", \"a\"], steepness, num_blocks, penalty_weight=penalty_weight)\n",
    "obj_rnd, constrs_rnd = nmRosenbrock([\"x_rnd\"], [\"p\", \"a\"], steepness, num_blocks, penalty_weight=penalty_weight)\n",
    "# build neural architecture for the solution map\n",
    "func = nm.modules.blocks.MLP(insize=num_blocks+1, outsize=2*num_blocks, bias=True,\n",
    "                             linear_map=nm.slim.maps[\"linear\"],\n",
    "                             nonlin=nn.ReLU, hsizes=[hsize]*hlayers_sol)\n",
    "smap = nm.system.Node(func, [\"p\", \"a\"], [\"x\"], name=\"smap\")\n",
    "# define rounding model\n",
    "layers_rnd = netFC(input_dim=3*num_blocks+1, hidden_dims=[hsize]*hlayers_rnd, output_dim=2*num_blocks)\n",
    "rnd = roundGumbelModel(layers=layers_rnd, param_keys=[\"p\", \"a\"], var_keys=[\"x\"],  output_keys=[\"x_rnd\"], \n",
    "                       int_ind=model.int_ind, continuous_update=True, name=\"round\")\n",
    "# build neuromancer problem for rounding\n",
    "components = [smap, rnd]\n",
    "loss = nm.loss.PenaltyLoss(obj_rnd, constrs_rnd)\n",
    "problem = nm.problem.Problem(components, loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c092fb3-225d-4708-be98-d4e55aa5f27b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training\n",
    "epochs = 200                    # number of training epochs\n",
    "warmup = 20                     # number of epochs to wait before enacting early stopping policy\n",
    "patience = 20                   # number of epochs with no improvement in eval metric to allow before early stopping\n",
    "optimizer = torch.optim.Adam(problem.parameters(), lr=lr)\n",
    "# create a trainer for the problem\n",
    "trainer = nm.trainer.Trainer(problem, loader_train, loader_dev, loader_test, optimizer, \n",
    "                            epochs=epochs, patience=patience, warmup=warmup)\n",
    "# training for the rounding problem\n",
    "best_model = trainer.train()\n",
    "# load best model dict\n",
    "problem.load_state_dict(best_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11703028-ddc4-4e25-83b3-56aa0a754e10",
   "metadata": {},
   "outputs": [],
   "source": [
    "params, sols, objvals, conviols, elapseds = [], [], [], [], []\n",
    "for p, a in tqdm(list(zip(p_test,a_test))):\n",
    "    # data point as tensor\n",
    "    datapoints = {\"p\": torch.tensor(np.array([p]), dtype=torch.float32), \n",
    "                  \"a\": torch.tensor(np.array([a]), dtype=torch.float32),\n",
    "                  \"name\": \"test\"}\n",
    "    # infer\n",
    "    tick = time.time()\n",
    "    output = problem(datapoints)\n",
    "    tock = time.time()\n",
    "    # assign params\n",
    "    model.set_param_val({\"p\":p, \"a\":a})\n",
    "    # assign vars\n",
    "    x = output[\"test_x_rnd\"]\n",
    "    for i in range(2*num_blocks):\n",
    "        model.vars[\"x\"][i].value = x[0,i].item()\n",
    "    # get solutions\n",
    "    xval, objval = model.get_val()    \n",
    "    params.append(list(p))\n",
    "    sols.append(list(list(xval.values())[0].values()))\n",
    "    objvals.append(objval)\n",
    "    conviols.append(sum(model.cal_violation()))\n",
    "    elapseds.append(tock - tick)\n",
    "df = pd.DataFrame({\"Param\":params, \"Sol\":sols, \"Obj Val\": objvals, \"Constraints Viol\": conviols, \"Elapsed Time\": elapseds})\n",
    "time.sleep(1)\n",
    "print(df.describe())\n",
    "print(\"Number of infeasible solution: {}\".format(np.sum(df[\"Constraints Viol\"] > 0)))\n",
    "df.to_csv(\"result/rb_nm_50-15.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c2e4faa-a711-41d8-afa3-88d22636e738",
   "metadata": {},
   "source": [
    "## 20 Blocks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a8602ac-2a20-45f8-b1a0-ca7f3f460936",
   "metadata": {},
   "source": [
    "### Problem Setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52363c72-0d20-4c08-bd00-0f62aa73686c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# init\n",
    "steepness = 50    # steepness factor\n",
    "num_blocks = 20   # number of expression blocks\n",
    "num_data = 5000   # number of data\n",
    "test_size = 1000  # number of test size\n",
    "val_size = 1000   # number of validation size\n",
    "train_size = num_data - test_size - val_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21938c00-d73f-4255-bc7c-bce6e8034cf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters as input data\n",
    "p_low, p_high = 1.0, 8.0\n",
    "a_low, a_high = 0.5, 4.5\n",
    "p_train = np.random.uniform(p_low, p_high, (train_size, 1)).astype(np.float32)\n",
    "p_test  = np.random.uniform(p_low, p_high, (test_size, 1)).astype(np.float32)\n",
    "p_dev   = np.random.uniform(p_low, p_high, (val_size, 1)).astype(np.float32)\n",
    "a_train = np.random.uniform(a_low, a_high, (train_size, num_blocks)).astype(np.float32)\n",
    "a_test  = np.random.uniform(a_low, a_high, (test_size, num_blocks)).astype(np.float32)\n",
    "a_dev   = np.random.uniform(a_low, a_high, (val_size, num_blocks)).astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ecf81ff-627d-444d-8d90-121c7e20c217",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nm datasets\n",
    "from neuromancer.dataset import DictDataset\n",
    "data_train = DictDataset({\"p\":p_train, \"a\":a_train}, name=\"train\")\n",
    "data_test = DictDataset({\"p\":p_test, \"a\":a_test}, name=\"test\")\n",
    "data_dev = DictDataset({\"p\":p_dev, \"a\":a_dev}, name=\"dev\")\n",
    "# torch dataloaders\n",
    "from torch.utils.data import DataLoader\n",
    "loader_train = DataLoader(data_train, batch_size=32, num_workers=0, collate_fn=data_train.collate_fn, shuffle=True)\n",
    "loader_test = DataLoader(data_test, batch_size=32, num_workers=0, collate_fn=data_test.collate_fn, shuffle=False)\n",
    "loader_dev = DataLoader(data_dev, batch_size=32, num_workers=0, collate_fn=data_dev.collate_fn, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2b99751-3a6e-46dc-9e33-fdf68d813034",
   "metadata": {},
   "source": [
    "### Exact Solver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2e7f07c-6a97-44d6-aca7-56f395e0c964",
   "metadata": {},
   "outputs": [],
   "source": [
    "# exact Solver\n",
    "from src.problem import msRosenbrock\n",
    "model = msRosenbrock(steepness, num_blocks, timelimit=60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1d8ca29-1f3f-4bc0-af4a-fc24868337fe",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "536d2d58-67ba-408a-b72a-7c5bd54a5ff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "penalty_weight = 50   # weight of constraint violation penealty\n",
    "hlayers_sol = 4       # number of hidden layers for solution mapping\n",
    "hlayers_rnd = 4       # number of hidden layers for solution mapping\n",
    "hsize = 64            # width of hidden layers for solution mapping\n",
    "lr = 1e-3             # learning rate\n",
    "batch_size = 64       # batch size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fee1241e-e53a-4cbb-927a-edcdb3537303",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set problem\n",
    "import neuromancer as nm\n",
    "from src.problem import nmRosenbrock\n",
    "from src.func.layer import netFC\n",
    "from src.func import roundGumbelModel\n",
    "# define Rosenbrock objective functions and constraints for both problem types\n",
    "obj_rel, constrs_rel = nmRosenbrock([\"x\"], [\"p\", \"a\"], steepness, num_blocks, penalty_weight=penalty_weight)\n",
    "obj_rnd, constrs_rnd = nmRosenbrock([\"x_rnd\"], [\"p\", \"a\"], steepness, num_blocks, penalty_weight=penalty_weight)\n",
    "# build neural architecture for the solution map\n",
    "func = nm.modules.blocks.MLP(insize=num_blocks+1, outsize=2*num_blocks, bias=True,\n",
    "                             linear_map=nm.slim.maps[\"linear\"],\n",
    "                             nonlin=nn.ReLU, hsizes=[hsize]*hlayers_sol)\n",
    "smap = nm.system.Node(func, [\"p\", \"a\"], [\"x\"], name=\"smap\")\n",
    "# define rounding model\n",
    "layers_rnd = netFC(input_dim=3*num_blocks+1, hidden_dims=[hsize]*hlayers_rnd, output_dim=2*num_blocks)\n",
    "rnd = roundGumbelModel(layers=layers_rnd, param_keys=[\"p\", \"a\"], var_keys=[\"x\"],  output_keys=[\"x_rnd\"], \n",
    "                       int_ind=model.int_ind, continuous_update=True, name=\"round\")\n",
    "# build neuromancer problem for rounding\n",
    "components = [smap, rnd]\n",
    "loss = nm.loss.PenaltyLoss(obj_rnd, constrs_rnd)\n",
    "problem = nm.problem.Problem(components, loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4f10263-5f69-4288-a4e8-8be83929c8d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training\n",
    "epochs = 200                    # number of training epochs\n",
    "warmup = 20                     # number of epochs to wait before enacting early stopping policy\n",
    "patience = 20                   # number of epochs with no improvement in eval metric to allow before early stopping\n",
    "optimizer = torch.optim.Adam(problem.parameters(), lr=lr)\n",
    "# create a trainer for the problem\n",
    "trainer = nm.trainer.Trainer(problem, loader_train, loader_dev, loader_test, optimizer, \n",
    "                            epochs=epochs, patience=patience, warmup=warmup)\n",
    "# training for the rounding problem\n",
    "best_model = trainer.train()\n",
    "# load best model dict\n",
    "problem.load_state_dict(best_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e835084-eb75-45d9-bcbd-5fb4c613fb41",
   "metadata": {},
   "outputs": [],
   "source": [
    "params, sols, objvals, conviols, elapseds = [], [], [], [], []\n",
    "for p, a in tqdm(list(zip(p_test,a_test))):\n",
    "    # data point as tensor\n",
    "    datapoints = {\"p\": torch.tensor(np.array([p]), dtype=torch.float32), \n",
    "                  \"a\": torch.tensor(np.array([a]), dtype=torch.float32),\n",
    "                  \"name\": \"test\"}\n",
    "    # infer\n",
    "    tick = time.time()\n",
    "    output = problem(datapoints)\n",
    "    tock = time.time()\n",
    "    # assign params\n",
    "    model.set_param_val({\"p\":p, \"a\":a})\n",
    "    # assign vars\n",
    "    x = output[\"test_x_rnd\"]\n",
    "    for i in range(2*num_blocks):\n",
    "        model.vars[\"x\"][i].value = x[0,i].item()\n",
    "    # get solutions\n",
    "    xval, objval = model.get_val()    \n",
    "    params.append(list(p))\n",
    "    sols.append(list(list(xval.values())[0].values()))\n",
    "    objvals.append(objval)\n",
    "    conviols.append(sum(model.cal_violation()))\n",
    "    elapseds.append(tock - tick)\n",
    "df = pd.DataFrame({\"Param\":params, \"Sol\":sols, \"Obj Val\": objvals, \"Constraints Viol\": conviols, \"Elapsed Time\": elapseds})\n",
    "time.sleep(1)\n",
    "print(df.describe())\n",
    "print(\"Number of infeasible solution: {}\".format(np.sum(df[\"Constraints Viol\"] > 0)))\n",
    "df.to_csv(\"result/rb_nm_50-20.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ade88317-6131-4f77-8df0-c45ac6c1d6ad",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
